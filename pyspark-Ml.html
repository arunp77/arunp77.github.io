<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <title>Pyspark</title>
    <meta content="" name="description">
    <meta content="" name="keywords">
    <!-- Favicons -->
    <link href="assets/img/Favicon-1.png" rel="icon">
    <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

    <!-- Vendor CSS Files -->
    <link href="assets/vendor/aos/aos.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
    <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
    <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
    <!-- Creating a python code section-->
    <link rel="stylesheet" href="assets/css/prism.css">
    <script src="assets/js/prism.js"></script>

    <!-- Template Main CSS File -->
    <link href="assets/css/style.css" rel="stylesheet">

    <!-- To set the icon, visit https://fontawesome.com/account-->
    <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
    <!-- end of icon-->

    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!-- =======================================================
    * Template Name: iPortfolio
    * Updated: Sep 18 2023 with Bootstrap v5.3.2
    * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
    * Author: BootstrapMade.com
    * License: https://bootstrapmade.com/license/
    ======================================================== -->
</head>

<body>

    <!-- ======= Mobile nav toggle button ======= -->
    <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

    <!-- ======= Header ======= -->
    <header id="header">
    <div class="d-flex flex-column">
        <div class="profile">
            <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
            <h1 class="text-light"><a href="index.html">Arun</a></h1>
            <div class="social-links mt-3 text-center">
                <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
                <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
                <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
                <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
            </div>
        </div>

        <nav id="navbar" class="nav-menu navbar">
            <ul>
                <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
                <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
                <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
                <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
                <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
                <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
                <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
                <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
                <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
                <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
            </ul>
        </nav><!-- .nav-menu -->
    </div>
    </header><!-- End Header -->

    <main id="main">
        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs"> 
        <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
            <h2>Data Engineering</h2>
            <ol>
                <li><a href="Data-engineering.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
            </ol>
            </div>
    
        </div>
        </section><!-- End Breadcrumbs -->

        <!------  right dropdown menue ------->
        <div class="right-side-list">
            <div class="dropdown">
                <button class="dropbtn"><strong>Shortcuts:</strong></button>
                <div class="dropdown-content">
                    <ul>
                        <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                        <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                        <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                        <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                        <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                        <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                        <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                        <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                        <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                        <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                        <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquery</a></li>
                        <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                        <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                            <!-- Add more subsections as needed -->
                    </ul>
                </div>
            </div>
        </div>

        <!-- ======= Portfolio Details Section ======= -->
        <section id="portfolio-details" class="portfolio-details">
            <div class="container">
                <div class="row gy-4">
                    <h1>Big data processing with Pyspark DataFrames</h1>
                    <div class="col-lg-8">
                        <div class="portfolio-details-slider swiper">
                            <div class="swiper-wrapper align-items-center"> 
                                <figure>
                                <img src="assets/img/data-engineering/Apache_Spark_logo.svg.png" alt="" style="max-width: 50%; max-height: auto;">
                                <figcaption></figcaption>
                                </figure>
                            </div>
                            <div class="swiper-pagination"></div>
                        </div>
                    </div>

                    <div class="col-lg-4 grey-box">
                        <div class="section-title">
                            <h3>Table of Contents</h3>
                            <ol>
                                <li><a href="#introduction">Introduction to Apache Spark</a></li>
                                <li><a href="#how-it-works">How does Spark work?</a></li>
                                <ul>
                                    <li><a href="#difference">Difference between Hadoop and Apache Spark</a></li>
                                    <li><a href="#key-concept">Some key concepts of Apache Spark</a></li>
                                    <li><a href="#key-features">Key Features</a></li>
                                </ul>
                                <li><a href="#Components">Components of Apache Spark</a></li>
                                <li><a href="#reference">Reference</a></li>
                            </ol>
                        </div>
                    </div>
                </div>

                <section>
                    <h3 id="rdd">RDD's - Resilient Distributed Datasets</h3>
                    The RDD structure is the elementary structure of Spark. It is flexible and optimal in performance for any linear operation. However, this structure has limited performance when it comes to non-linear operations, which is why we will introduce the DataFrame structure in the following exercise.
                    <div class="grey-box">
                        A good reference for the pyspark can be found at <a href="https://www.tutorialspoint.com/pyspark/index.htm">Link</a>. Another link with some 
                        good examples can be found at <a href="https://sparkbyexamples.com/" target="_blank">Link</a>.
                    </div>

                    <hr>
                    <!-------------- example ------------------->
                    <h3>Example</h3>
                    Let's do a example here. First we will create a Sparkcontext via sparksession and then load a datafile and in the end do some Machine learning analysis.
                    <ul>
                        <li><strong>Step-1: </strong>Build the SparkSession</li>
                        <pre class="language-python"><code>
                            # Importing Spark Session and SparkContext
                            from pyspark.sql import SparkSession
                            from pyspark import SparkContext
                            
                            # Definition of a SparkContext
                            SparkContext.getOrCreate() 
                            
                            # Definition of a SparkSession
                            spark = SparkSession \
                                .builder \
                                .master("local") \
                                .appName("Introduction to DataFrame") \
                                .getOrCreate()
                                
                            spark
                        </code></pre>
                        Here it will create a spark context and sparksession both. We will focus on SparkCOntext only here as it allows to work with RDD. So we create a SparkCOntext:
                        <pre class="language-python"><code>
                            # Creating a shortcut to the SparkContext already created
                            sc = SparkContext.getOrCreate()
                            sc                            
                        </code></pre>
                        <li><strong>Step-2</strong>Here we will load the textfile in .csv format.  Next we create a rdd from the database. A <code>rdd_row</code> will also be created using the <code>map</code>, applying on each line the structure Row with the explanatory variables <code>year</code>, <code>month</code>, <code>day</code> and <code>flightNum</code>. 
                        The data file is available at <a href="https://raw.githubusercontent.com/nraychaudhuri/ibm-spark-examples/master/data/airline-flights/alaska-airlines/2008.csv" target="_blank">Github repo</a>.</li>
                        <pre class="language-python"><code>
                        # Importing Row from the pyspark.sql package
                        from pyspark.sql import Row
                        
                        # Loading the file '2008_raw.csv'
                        rdd = sc.textFile('2008_raw.csv').map(lambda line: line.split(","))
                        
                        # Creating a new rdd by selecting the explanatory variables
                        rdd_row = rdd.map(lambda line: Row(year = line[0],
                                                            month = line[1],
                                                            day = line[2],
                                                            flightNum = line[5]))
                        
                        # Creating a data frame from an rdd
                        df = spark.createDataFrame(rdd_row)
                        df.show(5) # to show the top 5 rows.
                        </code></pre>
                        <li><strong>Step-3: </strong>Creating the DataFrame from a CSV using the SparkSession. </li>
                        <pre class="language-python"><code>raw_df = spark.read.csv('2008.csv', header=True)</code></pre>
                        To print the schema, we can use <pre class="language-python"><code>raw_df.printSchema()</code></pre>.
                        <li><strong>Step-4: </strong> Exploring and manipulating a DataFrame. Now we create <code>flights1</code> dataframe, containing only the variables: 'year', 'month', 'day', 'flightNum', 'origin', 'dest', 'distance', 'canceled', 'cancellationCode'. and then displaying the first 20 lines:</li>
                        <pre class="language-python"><code>
                            # Creating a data frame containing only the explanatory variables
                            flights1 = raw_df.select('year', 'month', 'day', 'flightNum', 'origin', 'dest', 'distance', 'canceled', 'cancellationCode', 'carrierDelay')
                            
                            # Display of 20 first lines
                            flights1.show() # 'show' displays 20 lines by default
                        </code></pre>
                        Now we can also create the dataframe with a specific datatype in each column using:
                        <pre class="language-python"><code>
                            flights = raw_df.select(raw_df.year.cast("int"),
                            raw_df.month.cast("int"),
                            raw_df.day.cast("int"),
                            raw_df.flightNum.cast("int"),
                            raw_df.origin.cast("string"),
                            raw_df.dest.cast("string"),
                            raw_df.distance.cast("int"),
                            raw_df.canceled.cast("boolean"),
                            raw_df.cancellationCode.cast("string"),
                            raw_df.carrierDelay.cast("int"))
                            
                            # Display of 20 first lines
                            flights.show()
                        </code></pre>
                        <li><strong>Step-5</strong> Some manipulation:</li>
                        To count the  number of flights that were delayed, you can use the following code snippet:
                        <pre class="language-python"><code>flights.select('flightNum').distinct().count()</code></pre>
                        Just like Pandas dataframe, we can have similar function <code>describe</code> to obtain a summary of the dataframe.
                        <pre class="language-python"><code>flights.describe().toPandas()</code></pre>
                        This will create  a pandas DataFrame that contains the statistical summary for numerical columns (e.g., min, max etc).
                        <p>A groupby just like python can be used to do some more analysis.</p>
                        <pre class="language-python"><code>flights.groupBy('cancellationCode', 'canceled').count().show()</code></pre>
                        Summarize the variable 'cancellationCode' and 'canceled' by displaying the number of observations of each value
                        <p>A <code>filter</code> can be used to  select certain rows based on conditions.</p>
                        <pre class="language-python"><code>flights.filter(flights.cancellationCode == 'C').show()</code></pre>
                        This will show the cancelled flights for the reason 'C'.
                        <p>The <code>withColumn</code> methods allows us to create a new column.</p>
                        <pre class="language-python"><code>flights.withColumn('isLongFlight', flights.distance > 1000 ).show(10)</code></pre>
                        This create a new Boolean variable <code>isLongFlight</code> that will be true if the flight is more than 1000 miles long and then displays first 10 line.
                        <p></p>
                        <li><strong>Step-6: </strong>Handling missing values. Missing values appear as null in the database. There are functions such as <code>dropna</code> or <code>fillna</code>, as in module Pandas. </li>
                        <pre class="language-python"><code>df.fillna( newValue, 'columnName') </code></pre>
                        This is the syntax to handle the missing value in the column 'columnName'.
                        <pre class="language-python"><code>flights.fillna(0, 'carrierDelay').show(6)</code></pre>
                        <p>To replace a value with some other value, we can use <code>df.replace(oldValue, newValue)</code>. To replace on a specific column, we can use <code> df.replace(oldValue, newValue, 'columnName'</code>
                        To replace sevral values to be replaced: <code>df.replace([oldValue1, oldValue2], [newValue1, newValue2], 'columnName')</code></p>
                        <li><strong>Step-7: </strong>Order by operation can be used to order the dataframe according to the values of one of its variables. <code>df.orderBy(df.age)</code> or <code>df.orderBy(df.age.desc())</code> 
                            to order the dataframe on the basis of variable age in a decreasing order.</li>
                        <pre class="language-python"><code>flights.orderBy(flights.flightNum.desc()).show()</code></pre>
                        <li><strong>Step-8: </strong>Spark SQL also allows you to use the SQL language. It is possible to run Pyspark using the method sql. The first step is to create a SQL view, referenced in the SQL code using the <code>createOrReplaceTempView</code>.</li>
                        <pre class="language-python"><code>
                            # Creating an SQL view
                            flights.createOrReplaceTempView("flightsView")
                            
                            # Creating a data frame containing only the variable "carrierDelay"
                            sqlDF = spark.sql("SELECT carrierDelay FROM flightsView")
                            
                            # Display of the first 10 lines
                            sqlDF.show(10)
                        </code></pre>
                        This create a SQL view of flights that will be called <code>flightsView</code> and then we create a DataFrame called <Code>sqlDF</Code> containing only the <code>carrierDelay</code> variable using a sql query. In the end we display the first line.
                        <li><strong>Step-9: Sample and displaying tips</strong>The disadvantage of the method show is that it has a bad rendering when a database contains a large number of variables. It is possible to use the method toPandas to fix this problem. 
                            This method only works on a small database. To do this, the sample method returns an extract from the data, essentially taking 3 arguments:
                            <ul>
                                <li><code>withRemplacement</code>: a Boolean to specify <code>False</code> if you don't want to overwrite the DataFrame</li>
                                <li><code>fraction</code>: the fraction of data to be kept</li>
                                <li><code>seed</code> : an integer that allows the results to be reproduced: for the same seed, a function, although random, will always give the same results</li>
                            </ul>
                            <code>flights.sample(False, .0001, seed = 222).toPandas()</code>
                            This display around ten lines of the database in an elegant way (just like pandas dataframe).
                        </li>
                        
                    </ul>
                    <pre class="language-python"><code></code></pre>

                    <br>
                    <h3 id="ml-pyspark">Machine learning with Pyspark Regression with PySpark </h3>
                    Spark ML is a very recent module, developed in parallel by Databricks and UC Berkeley AMPLab and launched at the end of 2015. Spark ML allows to run the majority of the 
                    algorithms of Machine Learning in a distributed way with a very big gain in performance.
                    In this part, we will study the case of a simple regression in order to understand how to prepare the data and deal with a Machine Learning problem using Spark ML. 
                    More advanced algorithms are the objective of the following exercise. Let's break this again:
                    <ul>
                        <li>Run the cell below to build a SparkSession for our exercise.</li>
                        <pre class="language-python"><code>
                            # Importing SparkSession and SparkContext
                            from pyspark.sql import SparkSession
                            from pyspark import SparkContext
                            
                            # Defining a SparkContext locally
                            sc = SparkContext.getOrCreate()
                            
                            # Building a Spark Session
                            spark = SparkSession \
                                .builder \
                                .appName("Introduction to Spark ML") \
                                .getOrCreate()
                                
                            spark
                        </code></pre>
                        <li>In this case, we use following dataset available at <a href="https://archive.ics.uci.edu/dataset/203/yearpredictionmsd" target="_blank">link</a>. This dataset contains audio 
                            audio characteristics of 515345 songs released between 1922 and 2011. These songs are essentially Western commercial hits. This database contains 91 columns:
                            <ul>
                                <li>A variable containing the year of the song</li>
                                <li>12 variables containing a 12-dimensional projection of the song's audio timbre</li>
                                <li>78 variables containing audio timbre covariance information</li>
                            </ul>
                            The objective is then to estimate the year of release of a song according to its audio characteristics. The goal is to perform a simple linear regression on the timbre information to predict the release year.
                        </li>
                        <li><strong>Loading the dataset: </strong>Next we load the datafile: <code>YearPredictionMSD.txt</code> into a dataframe <code>df_raw</code>.</li>
                        <pre class="language-python"><code>
                            df_raw = spark.read.csv('YearPredictionMSD.txt')

                            # First display method
                            df_raw.show(2, truncate = 4)
                            # By modifying the values of 'truncate', this method does not allow a good visualization of the data
                            # by the number of variables
                            
                            # Second display method
                            df_raw.sample(False, .00001, seed = 222).toPandas()
                        </code></pre>
                        <figure>
                            <img src="assets/img/data-engineering/pyspark-ml.png" alt="" style="max-width: 90%; max-height: auto;">
                            <figcaption></figcaption>
                        </figure>
                        <div class="grey-box">
                            Displaying a database with the method <code>show()</code> is faster. However, the result can sometimes be incomprehensible when there are too many variables.
                            We can then select a few variables and truncate the display to make them clean or privilege the succession of methods <code>sample()</code> and <code>toPandas()</code> 
                            taking care to choose a reasonable number of lines to display. It should be kept in mind that even if the sample method is relatively fast, it necessarily involves a 
                            counting of lines, a simple but slow operation in Spark. The DataSchema can be checked using <code>df_raw.printSchema()</code>. To change the type of each variable, it would be necessary to change its type as follows :
                            <pre><code>
                                df_raw.select(df_raw._c0.cast("double"),
                                df_raw._c1.cast("double"),
                                df_raw._c2.cast("double"),
                                df_raw._c3.cast("double"),
                                ...
                                )
                            </code></pre>
                            Such a task becomes more and more tedious when the number of variables becomes large. This approach can be automated using the <code>col</code> function from the sub-module <code>pyspark.sql.functions</code>. 
                            The <code>col</code> function allows you to directly name a column and automate this type of approach within a loop. The next two rows then allow you to change all the columns to dual' in a new DataFrame <code>df</code>:
                            <pre><code>
                    exprs = [col(c).cast("double") for c in df_raw.columns]
                    df = df_raw.select(*exprs)
                            </code></pre>
                        </div>
                        <li>We create dataframe <code>df</code> from <code>df_raw</code> by changing the types of columns related to the timbre to <code>double</code> and the year to <code>int</code>.</li>
                        <pre class="language-python"><code>
                            from pyspark.sql.functions import col

                            # Convert columns related to timbre to double and year to int
                            exprs = [col(c).cast("double") for c in df_raw.columns[1:91]]
                            df = df_raw.select(df_raw._c0.cast('int'), *exprs)
                            
                            # display of the variable schema
                            df.printSchema()
                        </code></pre>
                        <figure>
                            <img src="assets/img/data-engineering/pyspark-ml-schema.png" alt="" style="max-width: 90%; max-height: auto;">
                            <figcaption></figcaption>
                        </figure>
                        To view the descriptive summary of the database <code>df</code> using <code>df.describe().toPandas()</code>.
                        <li><strong>Formatting the database in <code>svmlib</code> format:</strong> To be used by the Machine Learning algorithms of Spark ML, the database must be a DataFrame containing 2 columns:
                            <ul>
                                <li><code>label</code>: containing the variable to be predicted</li>
                                <li><code>features</code>: containing the explanatory variables.</li>
                            </ul>
                            The <code>DenseVector()</code> function, from the package <code>pyspark.ml.linalg</code>, allows you to group several variables into one.
                            <div class="grey-box"><strong>Note: </strong>To be able to use the function <code>DenseVector</code>, we have to use the method <code>map</code> after transforming the <code>DataFrame</code> to <code>rdd</code>.
                            <pre><code>
                            rdd_ml = df.rdd.map(lambda x: (x[0], 
                                DenseVector(x[1:])))
                            # assuming that the variable to be explained is 
                            # in the first position
                            </code></pre>
                            and then
                            <pre>df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])</pre>
                            </div>
                        </li>
                        <li>Now in the present we first import <code>DenseVector</code>, then create <code>rdd_ml</code> separating the variable to be explained from the features (to be put in the form <code>DenseVector</code>). df_ml is created containing the database under the two variables: 'labels' and 'features'. </li>
                        <pre class="language-python"><code>
                            from pyspark.ml.linalg import DenseVector

                            # Creating an rdd by separating the variable to be explained from the features
                            rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))
                            
                            # Creation of a data frame composed of two variables: label and features
                            df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])
                            
                            # Display of the first 10 lines of the data frame
                            df_ml.show(10)
                        </code></pre>
                        <figure>
                            <img src="assets/img/data-engineering/pyspark-ml-features.png" alt="" style="max-width: 90%; max-height: auto;">
                            <figcaption></figcaption>
                        </figure>
                        <li>For the regression model, we create train and test datasets using <code>train, test = df.randomSplit([.7, .3], seed= 222)</code>.</li>
                        <pre class="language-python"><code>train, test = df_ml.randomSplit([.8, .2], seed= 1234)</code></pre>
                        <li><strong>Linear Regression:</strong> Spark ML contains many Machine Learning functions. Let's start here with the most basic one: linear regression. It is present under the name <code>LinearRegression</code> 
                            in the module <code>pyspark.ml.regression</code>. This function allows you to perform a regression in a distributed way and performs calculations on the different clusters predefined in the SparkSession, 
                            regardless of their number or the size of the database. To use it, we must proceed with the two usual steps:
                            <ul>
                                <li>Create the function with context-specific parameters,</li>
                                <li>Use the <code>fit</code> method to apply it to the data.</li>
                            </ul>
                            <div class="grey-box">For more details on this method, we can go to following <a href="https://spark.apache.org/docs/latest/ml-classification-regression.html" target="_blank">link</a>.</div>
                            We now import the <code>LinearRegression</code>, create a distributed linear regression function <code>lr</code> to be applied to the set <code>train</code>. We then create <code>linearModel</code>,
                            the model from <code>lr</code> applied to <code>train</code>.
                        </li> 
                        <pre class="language-python"><code>
                            from pyspark.ml.regression import LinearRegression

                            # Creating a linear regression function
                            lr = LinearRegression(labelCol='label', featuresCol= 'features')
                            
                            # Fitting of training data "train".
                            linearModel = lr.fit(train)
                        </code></pre>
                        <li></li>
                        <pre class="language-python"><code></code></pre>
                        <li></li>
                        <pre class="language-python"><code></code></pre>
                        <li></li>
                        <pre class="language-python"><code></code></pre>
                        <li></li>
                        <pre class="language-python"><code></code></pre>
                    </ul>


                    <pre class="language-python"><code></code></pre>

                    

                    



                </section>

                <!-------Reference ------->
                <section id="reference">
                    <h2>References</h2>
                    <ol>
                        <li><a href="https://spark.apache.org/documentation.html" target="_blank"> Official Documentation</a></li>
                        <li><a href="https://www.databricks.com/learn/training/login" target="_blank">Databricks Learning Academy</a></li>
                        <li><a href="https://sparkbyexamples.com/" target="_blank">Spark by Examples</a></li>
                        <li><a href="https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark" target="_blank">Datacamp tutorial</a>.</li>
                        <li>For databricks, you can look at tutorial videos on youtube at <a href="https://www.youtube.com/watch?v=ChISx0-cMpU" target="_blank">youtube video by Bryan Cafferky</a>, 
                            writer of the book "Master Azure Databricks". A great playlist for someone who just want to learn about the big data analytics at Databricks Azure cloud platform.</li>
                        <li>See the video for <a href="https://www.youtube.com/watch?v=_C8kWso4ne4" target="_blank">pyspark basics by Krish Naik</a>. Great video for starter.</li>
                        <li><a href="https://www.youtube.com/watch?v=QLGrLFOzMRw" target="_blank">Great youtube on Apache spark</a> one premise working.</li>
                    </ol> 
                </section>

                <hr>
            
                <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

                    <h3>Some other interesting things to know:</h3>
                    <ul style="list-style-type: disc; margin-left: 30px;">
                        <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
                        <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
                    </ul>
                </div>
                <p></p>

                <div class="navigation">
                    
                    <a href="index.html#portfolio" class="clickable-box">
                        <span class="arrow-left">Portfolio section</span>
                    </a>
                    
                    <a href="Data-engineering.html" class="clickable-box">
                        <span class="arrow-right">Content</span>
                    </a>

                </div>
            </div>
        </section><!-- End Portfolio Details Section -->
    </main><!-- End #main --

    <!-- ======= Footer ======= -->
    <footer id="footer">
    <div class="container">
        <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
        </div>
    </div>
    </footer><!-- End  Footer -->

    <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="assets/vendor/typed.js/typed.umd.js"></script>
    <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function () {
        hljs.initHighlightingOnLoad();
    });
    </script>

</body>

</html> 