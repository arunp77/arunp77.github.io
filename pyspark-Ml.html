<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <title>Pyspark</title>
    <meta content="" name="description">
    <meta content="" name="keywords">
    <!-- Favicons -->
    <link href="assets/img/Favicon-1.png" rel="icon">
    <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

    <!-- Vendor CSS Files -->
    <link href="assets/vendor/aos/aos.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
    <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
    <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
    <!-- Creating a python code section-->
    <link rel="stylesheet" href="assets/css/prism.css">
    <script src="assets/js/prism.js"></script>

    <!-- Template Main CSS File -->
    <link href="assets/css/style.css" rel="stylesheet">

    <!-- To set the icon, visit https://fontawesome.com/account-->
    <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
    <!-- end of icon-->

    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!-- =======================================================
    * Template Name: iPortfolio
    * Updated: Sep 18 2023 with Bootstrap v5.3.2
    * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
    * Author: BootstrapMade.com
    * License: https://bootstrapmade.com/license/
    ======================================================== -->
</head>

<body>

    <!-- ======= Mobile nav toggle button ======= -->
    <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

    <!-- ======= Header ======= -->
    <header id="header">
    <div class="d-flex flex-column">
        <div class="profile">
            <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
            <h1 class="text-light"><a href="index.html">Arun</a></h1>
            <div class="social-links mt-3 text-center">
                <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
                <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
                <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
                <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
            </div>
        </div>

        <nav id="navbar" class="nav-menu navbar">
            <ul>
                <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
                <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
                <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
                <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
                <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
                <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
                <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
                <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
                <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
                <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
            </ul>
        </nav><!-- .nav-menu -->
    </div>
    </header><!-- End Header -->

    <main id="main">
        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs"> 
        <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
            <h2>Data Engineering</h2>
            <ol>
                <li><a href="Data-engineering.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
            </ol>
            </div>
    
        </div>
        </section><!-- End Breadcrumbs -->

        <!------  right dropdown menue ------->
        <div class="right-side-list">
            <div class="dropdown">
                <button class="dropbtn"><strong>Shortcuts:</strong></button>
                <div class="dropdown-content">
                    <ul>
                        <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                        <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                        <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                        <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                        <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                        <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                        <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                        <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                        <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                        <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                        <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquery</a></li>
                        <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                        <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                            <!-- Add more subsections as needed -->
                    </ul>
                </div>
            </div>
        </div>

        <!-- ======= Portfolio Details Section ======= -->
        <section id="portfolio-details" class="portfolio-details">
            <div class="container">
                <div class="row gy-4">
                    <h1>Big data processing with Pyspark DataFrames</h1>
                    <div class="col-lg-8">
                        <div class="portfolio-details-slider swiper">
                            <div class="swiper-wrapper align-items-center"> 
                                <figure>
                                <img src="assets/img/data-engineering/Apache_Spark_logo.svg.png" alt="" style="max-width: 50%; max-height: auto;">
                                <figcaption></figcaption>
                                </figure>
                            </div>
                            <div class="swiper-pagination"></div>
                        </div>
                    </div>

                    <div class="col-lg-4 grey-box">
                        <div class="section-title">
                            <h3>Table of Contents</h3>
                            <ol>
                                <li><a href="#introduction">Introduction to Apache Spark</a></li>
                                <li><a href="#how-it-works">How does Spark work?</a></li>
                                <ul>
                                    <li><a href="#difference">Difference between Hadoop and Apache Spark</a></li>
                                    <li><a href="#key-concept">Some key concepts of Apache Spark</a></li>
                                    <li><a href="#key-features">Key Features</a></li>
                                </ul>
                                <li><a href="#Components">Components of Apache Spark</a></li>
                                <li><a href="#reference">Reference</a></li>
                            </ol>
                        </div>
                    </div>
                </div>

                <section>
                    <h3 id="summary-spark">Summary Apache Spark</h3>
                    <ul>
                        <li><strong>Definition: </strong>Apache Spark is an Open source analytical processing engine for large-scale powerful distributed data processing and machine learning applications.
                            Apache Spark 3.5 is a framework that is supported in Scala, Python, R Programming, and Java. Below are different implementations of Spark.
                            <ul>
                                <li>Spark - Default interface for Scala and Java</li>
                                <li>PySpark – Python interface for Spark</li>
                                <li>SparklyR – R interface for Spark.</li>
                            </ul>
                        </li>
                        <li><strong>Features of Apache Spark:</strong>
                            <ul>
                                <li>In-memory computation</li>
                                <li>Distributed processing using parallelize</li>
                                <li>Can be used with many cluster managers (Spark, Yarn, Mesos e.t.c)</li>
                                <li>Fault-tolerant</li>
                                <li>Immutable</li>
                                <li>Lazy evaluation</li>
                                <li>Cache & persistence</li>
                                <li>Inbuild-optimization when using DataFrames</li>
                                <li>Supports ANSI SQL</li>
                            </ul>
                        </li>
                        <li><strong>Advantages of Apache Spark:</strong>
                            <ul>
                                <li>Spark is a general-purpose, in-memory, fault-tolerant, distributed processing engine that allows you to process data efficiently in a distributed fashion.</li>
                                <li>Applications running on Spark are 100x faster than traditional systems.</li>
                                <li>There are so many benefits from using Spark for data ingestion pipelines.</li>
                                <li>Using Spark we can process data from Hadoop HDFS, AWS S3, Databricks DBFS, Azure Blob Storage, and many file systems.</li>
                                <li>Spark also is used to process real-time data using Streaming and Kafka.</li>
                                <li>Using Spark Streaming you can also stream files from the file system and also stream from the socket.</li>
                                <li>Spark natively has machine learning and graph libraries.</li>
                                <li>Provides connectors to store the data in NoSQL databases like MongoDB.</li>
                            </ul>
                        </li>
                        <li><strong>Apache Spark Architecture: </strong>
                            Spark works in a master-slave architecture where the master is called the “Driver” and slaves are called “Workers”. When you run a Spark application, Spark Driver creates a context that is an entry point to your application, and all operations (transformations and actions) are executed on worker nodes, and the resources are managed by Cluster Manager.
                            <figure>
                                <img src="assets/img/data-engineering/pyspark-summary.png" alt="" style="max-width: 50%; max-height: auto;">
                                <figcaption></figcaption>
                            </figure>
                        </li>
                        <li><strong>Cluster Manager Types: </strong>Standalone, Apache Mesos, Hadoop YARN, Kubernetes, local (for master() in order to run Spark on local computer).</li>
                        <li><strong>Spark Modules: </strong>Spark Core, Spark SQL, Spark Streaming, Spark MLlib, Spark GraphX</li>
                        <li><strong>Spark Core: </strong>Spark Core is the main base library of Spark which provides the abstraction of how distributed task dispatching, scheduling, basic I/O functionalities etc.</li>
                        <li><strong>SparkSession: </strong>It is an entry point to underlying Spark functionality in order to programmetically use Spark RDD, DataFrame, and Dataset. It's object <code>spark</code> is default available in spark-shell.
                            The initial step in a Spark program involving RDDs, DataFrames, or Datasets would be to create a SparkSession instance. The Sparksession will be created using <code>SparkSession.builder()</code>. 
                            <pre class="language-python"><code>
                        from pyspark.sql import SparkSession
                        # Create a SparkSession object
                        spark = SparkSession.builder \
                            .appName("YourAppName") \
                            .master("local[*]") \
                            .getOrCreate()
                    
                        spark # to call spark
                            </code></pre>                        
                        If you're using SparkSession, which is the entry point to Spark SQL, you can work with DataFrames and Datasets, which provide higher-level abstractions and optimizations compared to RDDs.
                        Here are some common tasks you can perform with SparkSession:
                        <ul>
                            <li><strong>Read and write data: </strong>SparkSession provides methods to read data from various sources such as Parquet, JSON, CSV, JDBC, Avro, ORC, and many more. Similarly, you can write data to different formats and locations.
                                For example:<code>df = spark.read.csv("file.csv")</code> and <code>df.write.parquet("output.parquet")</code></li>
                            <li><strong>Create DataFrames: </strong>You can create DataFrames from existing RDDs, lists, dictionaries, or by applying transformations on other DataFrames. For example:<code></code>
                                <pre class="language-python"><code>
                                    from pyspark.sql import Row

                                    data = [(1, "Alice"), (2, "Bob")]
                                    rdd = sc.parallelize(data)
                                    row_rdd = rdd.map(lambda x: Row(id=x[0], name=x[1]))
                                    df = spark.createDataFrame(row_rdd)
                                </code></pre>
                            </li>
                            <li><strong>SQL queries: </strong>SparkSession allows you to run SQL queries on DataFrames using the sql() method. For example:
                            <pre class="language-python"><code>
                                df.createOrReplaceTempView("people")
                                result = spark.sql("SELECT * FROM people WHERE age > 20")
                            </code></pre>
                            </li>
                            <li><strong>DataFrame operations: </strong>You can perform various operations on DataFrames like <code>select()</code>, <code>filter()</code>, <code>join()</code>, <code>orderBy()</code>, <code>agg()</code> etc. For example:
                                <pre class="language-python"><code>result = df.select("name").filter(df.age > 20).orderBy(df.name)</code></pre>
                            </li>
                            <li><strong>Window functions: </strong>You can use window functions for advanced analytics tasks like ranking, lead/lag analysis, etc. For example: 
                            <pre class="language-python"><code>
                            from pyspark.sql.window import Window
                            from pyspark.sql.functions import rank
                            
                            window = Window.partitionBy("department").orderBy("salary")
                            result = df.withColumn("rank", rank().over(window))
                            </code></pre>
                            </li>
                            <li><strong>Machine learning: </strong>We can use Spark MLlib, a scalable machine learning library, to train and apply machine learning models on DataFrames. For example:
                                <pre class="language-python"><code>
                                    from pyspark.ml.classification import LogisticRegression

                                    lr = LogisticRegression(featuresCol="features", labelCol="label")
                                    model = lr.fit(train_data)
                                    predictions = model.transform(test_data)
                                </code></pre>
                            </li>
                            <li><strong>Streaming data:</strong>We can process streaming data using Spark Structured Streaming, which provides high-level APIs for streaming processing. For example:
                                <pre class="language-python"><code>stream_df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "host:port").load()</code></pre>
                            </li>
                        </ul>
                        </li>
                        <li><strong>Spark Context: </strong>It is also a entry point to Spark and PySpark. Before SparkSession 2.0, it was the main entry point.
                            Creating SparkContext was the first step to the program with RDD and to connect to Spark Cluster. It’s object sc by default available in <code>spark-shell</code>.
                            Since Spark 2.0, when we create <code>SparkSession</code>, <code>SparkContext</code> object is by default created and it can be accessed using <code>spark.sparkContext</code>.
                        <pre class="language-python"><code>
                        from pyspark.sql import SparkSession

                        # Create a SparkSession
                        spark = SparkSession.builder \
                            .appName("YourAppName") \
                            .master("local[*]") \
                            .getOrCreate()
                        
                        # Access the SparkContext from the SparkSession
                        sc = spark.sparkContext
                        </code></pre>
                        Once the spark context is created, we can do following things:
                            <ul>
                                <li><strong>Parallelize data: </strong>We can parallelize an existing collection in your driver program (e.g., a list or array) using the <code>parallelize()</code> method. This will distribute the data across the nodes in your Spark cluster and create an RDD. <code>rdd  = sc.parallelize(data)</code></li>
                                <li><strong>Read data from external sources: </strong>We can read data from various external sources such as HDFS, S3, HBase, or any supported data source using the methods provided by SparkContext, such as <code>textFile()</code> for reading text files.<code>text_rdd = sc.textFile("hdfs://path/to/file.txt")</code></li>
                                <li><strong>Transformations: </strong>can perform various transformations on RDDs, such as <code>map()</code>, <code>filter()</code>, <code>flatMap()</code>, <code>reduceByKey()</code>, etc., to process and manipulate the data. For example: <code>squared_rdd = rdd.map(lambda x: x * x)</code></li>
                                <li><strong>Caching: </strong>We  can cache RDDs in memory to speed up iterative or interactive computations by using the <code>cache()</code> method. For example: <code>rdd.cache()</code></li>
                                <li><strong>Accumulators: </strong>We can can use accumulators to aggregate information across all tasks, such as counting occurrences of certain events. For example: <code>accumulator = sc.accumulator(0)</code>.</li>
                            </ul>
                        </li>
                        <div class="grey-box">
                            <h5>Difference between SparkContext and SparkSession</h5>
                            <ol>
                                <li><strong>SparkContext (sc): </strong>
                                    <ul>
                                        <li>SparkContext is the entry point to Spark functionality and represents the connection to a Spark cluster. It coordinates the execution of operations on the cluster.</li>
                                        <li>It provides access to the underlying Spark functionality, including RDDs (Resilient Distributed Datasets), which are the fundamental data abstraction in Spark.</li>
                                        <li>SparkContext is primarily used for low-level operations and interactions with RDDs, such as creating RDDs, performing transformations, and executing actions.</li>
                                    </ul>
                                </li>
                                <li><strong>SparkSession (spark): </strong>
                                    <ul>
                                        <li>SparkSession, introduced in Spark 2.x, is a higher-level abstraction on top of SparkContext and provides a unified entry point to Spark SQL, DataFrame, and Dataset APIs.</li>
                                        <li>It is the recommended way to interact with Spark in modern Spark applications, as it provides a more user-friendly and consistent interface for working with structured data.</li>
                                        <li>SparkSession encapsulates SparkContext and provides additional functionalities, including reading and writing structured data from various sources, running SQL queries, and performing DataFrame operations.</li>
                                        <li>SparkSession also manages the underlying SparkContext internally, so there's no need to create a SparkContext explicitly when using SparkSession.</li>
                                    </ul>
                                </li>
                            </ol>
                            In summary, while SparkContext is primarily focused on low-level distributed computing operations with RDDs, SparkSession provides a higher-level interface for working with structured data, including DataFrames and Datasets, as well as integrating with Spark SQL, MLlib, and other Spark components. It's generally recommended to use SparkSession for modern Spark applications unless you have specific requirements that necessitate working directly with RDDs and SparkContext.
                        </div>
                    </ul>



                    <!------------------------>
                    <h3 id="rdd">RDD's - Resilient Distributed Datasets</h3>
                    The RDD structure is the elementary structure of Spark. It is flexible and optimal in performance for any linear operation. However, this structure has limited performance when it comes to non-linear operations, which is why we will introduce the DataFrame structure in the following exercise.
                    <div class="grey-box">
                        A good reference for the pyspark can be found at <a href="https://www.tutorialspoint.com/pyspark/index.htm">Link</a>. Another link with some 
                        good examples can be found at <a href="https://sparkbyexamples.com/" target="_blank">Link</a>.
                    </div>
                    <p><strong>SparkContext</strong></p>






                    



                </section>

                <!-------Reference ------->
                <section id="reference">
                    <h2>References</h2>
                    <ol>
                        <li><a href="https://spark.apache.org/documentation.html" target="_blank"> Official Documentation</a></li>
                        <li><a href="https://www.databricks.com/learn/training/login" target="_blank">Databricks Learning Academy</a></li>
                        <li><a href="https://sparkbyexamples.com/" target="_blank">Spark by Examples</a></li>
                        <li><a href="https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark" target="_blank">Datacamp tutorial</a>.</li>
                        <li>For databricks, you can look at tutorial videos on youtube at <a href="https://www.youtube.com/watch?v=ChISx0-cMpU" target="_blank">youtube video by Bryan Cafferky</a>, 
                            writer of the book "Master Azure Databricks". A great playlist for someone who just want to learn about the big data analytics at Databricks Azure cloud platform.</li>
                        <li>See the video for <a href="https://www.youtube.com/watch?v=_C8kWso4ne4" target="_blank">pyspark basics by Krish Naik</a>. Great video for starter.</li>
                        <li><a href="https://www.youtube.com/watch?v=QLGrLFOzMRw" target="_blank">Great youtube on Apache spark</a> one premise working.</li>
                    </ol> 
                </section>

                <hr>
            
                <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

                    <h3>Some other interesting things to know:</h3>
                    <ul style="list-style-type: disc; margin-left: 30px;">
                        <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
                        <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
                    </ul>
                </div>
                <p></p>

                <div class="navigation">
                    
                    <a href="index.html#portfolio" class="clickable-box">
                        <span class="arrow-left">Portfolio section</span>
                    </a>
                    
                    <a href="Data-engineering.html" class="clickable-box">
                        <span class="arrow-right">Content</span>
                    </a>

                </div>
            </div>
        </section><!-- End Portfolio Details Section -->
    </main><!-- End #main --

    <!-- ======= Footer ======= -->
    <footer id="footer">
    <div class="container">
        <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
        </div>
    </div>
    </footer><!-- End  Footer -->

    <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="assets/vendor/typed.js/typed.umd.js"></script>
    <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function () {
        hljs.initHighlightingOnLoad();
    });
    </script>

</body>

</html> 