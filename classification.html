<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Classification models</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Arun</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
          <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
          <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
          <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
          <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
          <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
          <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
          <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
          <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
          <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
          <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
          <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
          <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
          <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

<main id="main">

        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs">
          <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
              <h2></h2>
              <ol>
                <li><a href="portfolio-details-1.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
              </ol>
            </div>
    
          </div>
        </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
      
      <div class="dropdown">
          <button class="dropbtn"><strong>Shortcuts:</strong></button>
          <div class="dropdown-content">
              <ul>
                  <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                  <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                  <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                  <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                  <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                  <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                  <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                  <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                  <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                  <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                  <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                  <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                  <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                    <!-- Add more subsections as needed -->
                </ul>
          </div>
        </div>
    </div>

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">
        <div class="row gy-4">
          <h1>Classification in Machine Learning: An Introduction</h1>
          <div class="col-lg-8">
            <div class="portfolio-details-slider swiper">
              <div class="swiper-wrapper align-items-center">
                <div class="swiper-slide">
                    <figure>
                      <img src="assets/img/data-engineering/classification.png" alt="" style="max-width: 50%; max-height: 50%;">
                      <figcaption></figcaption>
                    </figure>
                </div>
              </div>
          </div>
        </div>

        <div class="col-lg-4 grey-box">
          
          <div class="section-title">
            <h3>Content</h3>
            <ol>
              <li><a href="#introduction">Introduction</a></li>
              <li><a href="#Relationship-of-regression-lines">Relationship of regression lines</a></li>
              <li><a href="#Types-of-Linear-Regression">Types of Linear Regression</a></li>
              <li><a href="#Mathematical-1">Mathematical Explanation</a></li>
              <li><a href="#Assumption-of-LR">Assumptions of Linear Regression</a></li>
              <li><a href="#evaluation-metrics-for-LR">Evaluation Metrics for Linear Regression</a></li>
              <li><a href="#overfit-goodfit-underfit">Overfitting, Good Fit, and Underfitting in Machine Learning</a></li>
              <li><a href="#reference">Reference</a></li>
          </ol>
          </div>
        </div>
      </div>

      <section id="introdction">
        <h2>Introduction</h2>
        Classification is a supervised machine learning task. Classification is defined as the process of recognition, understanding, and grouping of objects and ideas into categories. With the help of these pre-categories training datasets, classification in machine learning programs 
        leverage a wide range of algorithms to classify future datasets into respective and relevant categories. Classification problems are an important category of problems 
        in analytics in which the outcome variable or response variable ('\(y\)') takes discrete values. Primary objective of a classification model is to predict 
        the probability of an observation belonging to a class, known as <strong>class probability</strong>.
        
        <p>
        </p>

        <p><strong>Example: </strong>For example, a spam filter can be used to classify emails as spam or not spam. A credit card fraud detection system can be used to classify transactions as legitimate or fraudulent.</p>
        
        <!--------------------------->
        <h3 id="learners">Learners in Classification Problems</h3>
        There are two types of learners:
        <ul>
          <li><strong>Lazy Learners: </strong>In lazy learning, the algorithm initially stores the training dataset and then awaits the arrival of the test dataset. Classification is conducted using the most relevant data from the training dataset. While this approach reduces training time, it requires more time for predictions. Examples of lazy learning algorithms include case-based reasoning and the k-Nearest Neighbors (KNN) algorithm.</li>
          <li><strong>Eager Learners: </strong>Eager learners construct a classification model using the training dataset before obtaining a test dataset. They invest more time in studying the data and less time in making predictions. Examples of eager learning algorithms include Artificial Neural Networks (ANN), naive Bayes, and Decision Trees.</li>
        </ul>

        <table>
          <tr>
              <th><strong>Differences</strong></th>
              <th><strong>Eager learners</strong></th>
              <th><strong>Lazy learners</strong></th>
          </tr>
          <tr>
            <td>Training Approach:</td>
            <td>Build the classification model using the entire training dataset upfront.</td>
            <td>Delay building the model until a test instance needs to be classified, using only local or on-demand learning.</td>
          </tr>
          <tr>
            <td>Time Allocation:</td>
            <td>Spend more time upfront during the training phase, analyzing and processing the entire dataset.</td>
            <td>Spend less time on initial training, as they postpone processing until specific instances require classification.</td>
          </tr>
          <tr>
            <td>Prediction Time:</td>
            <td>Typically faster at prediction time since the model is already built and ready to use.</td>
            <td>Can be slower at prediction time because they need to perform computations or comparisons on-demand.</td>
          </tr>
          <tr>
            <td>Resource Consumption:</td>
            <td>Require more memory and computational resources upfront due to processing the entire dataset during training.</td>
            <td>May consume fewer resources initially but may require more resources at prediction time for local processing.</td>
          </tr>
          <tr>
            <td>Examples:</td>
            <td>Examples include algorithms like Artificial Neural Networks (ANN), naive Bayes, and Decision Trees.</td>
            <td>Examples include case-based reasoning and the k-nearest neighbors (KNN) algorithm.</td>
          </tr>
        </table>


        <br>
        <h3 id="classification">Classification Predictive Modeling</h3>
        In machine learning, classification problems involve predicting a category for a given input. These problems are everywhere:
        <ul>
          <li>Think of sorting emails as either spam or not spam.</li>
          <li>Imagine recognizing handwritten characters, like letters or digits.</li>
          <li>Consider deciding if a user's behavior indicates they might stop using a service (churn).</li>
        </ul>
        A training dataset with numerous examples of inputs and outputs is necessary for classification from a modeling standpoint. 
        A model will determine the optimal way to map samples of input data to certain class labels using the training dataset. The training dataset 
        must therefore contain a large number of samples of each class label and be suitably representative of the problem.
        There are numerous varieties of algorithms for classification in modeling problems, including predictive modeling and classification.
        We can classify the classification algorithms in following 4 categories:
        <ol>
          <li><strong>Binary Classification:</strong> Sorting into two categories, like spam or not spam. Binary classification problems often require two classes, one representing the normal state and the other representing the 
            aberrant state. For instance, the normal condition is "not spam," while the abnormal state is "spam." Class label 0 is given to the class in the normal state, whereas class label 1 is given to the class in the abnormal condition.
            A model that forecasts a <strong><em>Bernoulli probability distribution</strong></em> for each case is frequently used to represent a binary classification task. 
            <div class="grey-box">Let \(X\) be a normal variable representing the input features, and \(y\) be a binary random variable representing the class label (0 or 1).
              The  Bernoulli distribution models the probability of success (or the positive outcome, often denoted as \(p\))
              in a single Bernoulli trial. The probability mass function (PMF) of the Bernoulli distribution is given by:
              $$P(y=1|X) = p$$
              $$P(y=0|X) = 1-p$$
              Here \(p\) represents the probability of the positive outcome given the input features \(X\).
            </div>
            The following are well-known binary classification algorithms:
            <ul>
              <li>Logistic Regression</li>
              <li>Support Vector Machines</li>
              <li>Simple Bayes</li>
              <li>Decision Trees</li>
            </ul>
          </li>
          <li><strong>Multi-Class Classification:</strong> Sorting into several categories, like different types of animals.
            The multi-class classification does not have the idea of normal and abnormal outcomes, in contrast to binary classification. Instead, instances are grouped into one of several well-known classes.
            In some cases, the number of class labels could be rather high. In a facial recognition system, for instance, a model might predict that a shot belongs to one of thousands or tens of thousands of faces.
            Text translation models and other problems involving word prediction could be categorized as a particular case of multi-class classification.
            Multiclass classification tasks are frequently modeled using a model that forecasts a <strong><em>Multinoulli probability distribution</em></strong> for each example.
            <div class="grey-box">
              A Multinoulli probability distribution, also known as a categorical distribution, is a generalization of the Bernoulli distribution to more than two categories. Instead of having just two possible outcomes (e.g., 0 and 1), a Multinoulli distribution accommodates multiple discrete outcomes, each with its own probability.
              Mathematically, let \(X\) be a random variable representing the input features, and \(y\) be a categorical random variable representing the class label among \(K\) possible categories (where \(K\) is greater than 2).
              The Multinoulli distribution models the probability of each category \(k\) in a single trial. The probability mass function (PMF) of the Multinoulli distribution is given by:
              $$P(y=k|X) = p_k$$
              where \(p_k\) represents the probability of category \(k\) given the input features \(X\), and \(\sum_{k=1}^K p_k = 1\) (since the probabilities must sum up to 1 for all possible categories).
            </div>
            For multi-class classification, many binary classification techniques are applicable. The following well-known algorithms can be used for multi-class classification:
              <ul>
                <li>Progressive Boosting</li>
                <li>Choice trees</li>
                <li>Nearest K Neighbors</li>
                <li>Rough Forest</li>
                <li>Simple Bayes</li>
              </ul>
              In multi-class classification tasks, the "one-vs-rest" (OvR) and "one-vs-one" (OvO) methods are commonly used.
              <ul>
                <li><strong>One-vs-Rest (OvR): </strong> Also known as "one model for each class," this method involves training multiple binary classification models, each classifying one class against all other classes.</li>
                <li><strong>One-vs-One (OvO): </strong> In this approach, a binary classifier is trained for each pair of classes. This results in \(\frac{N\times (N-1)}{2}\) classifiers for \(N\) classes.</li>
              </ul>
          </li>
          <li><strong>Multi-Label Classification:</strong> Assigning multiple labels to each example, like tagging a post with multiple topics.</li>
          <li><strong>Imbalanced Classification:</strong> Dealing with datasets where one category is much more common than the others.</li>
        </ol>





        



      

        <!----------------->
        <h3 id="Common-algorithms">Common classification algorithms</h3>
        Classification problems may have binary or multiple outcomes or classes. Binary outcomes are called 
        binary classification and multiple outcomes are called multinomial classification.
        There are several techniques used for solving classification problems such as: logistic regression classification trees i.e. decision tree learning discriminant analysis neural networks support vector machines.
        <ol>
          <li><strong>Logistic regression: </strong>Logistic regression is a statistical model that is used to predict the probability of a binary outcome (e.g., spam vs. not spam). It is a linear model, meaning that it assumes that 
            there is a linear relationship between the features of the data and the probability of the outcome.</li>
          <li><strong>Decision trees: </strong>Decision trees are a tree-like structure that represents a set of rules for making decisions. They are often used for classification tasks because they are easy to interpret and can 
            handle both numerical and categorical data.</li>
          <li><strong>K-nearest neighbors (KNN): </strong>KNN is a simple but effective classification algorithm that classifies new data points based on the majority class of their k nearest neighbors in the training data.</li>
          <li><strong>Support vector machines (SVMs): </strong>SVMs are a powerful classification algorithm that can handle both linear and nonlinear relationships between the features of the data and the outcome. They are 
            particularly useful for complex classification problems.</li>
          <li><strong>Naive Bayes: </strong>Naive Bayes is a probabilistic classifier that is based on Bayes' theorem. It is a simple and efficient algorithm that is often used for text classification tasks.</li>
        </ol>

        <!----------------------->
        <h3 id="Binary">Binary Logistic Regression</h3>
        Logistic regression is a statistical model in which the response variable takes a discrete value and the 
        explanatory variables can either be continuous or discrete. If the outcome variable takes only two values, 
        then the model is called binary logistic regression model. 

        <p>Logistic regression is statistical method used to model the probability of a outcome (i.e., outcome that can take on one of two values, such as 0 or 1, yes or no, etc.) based on one or more predictor variables. Mathematically, 
          logistic regression uses the logistic function, also known as the sigmoid function, to model the probability of the outcome.</p>

        <p>The logistic function is defined as:</p>

        $$P(Y=1 | X) = \frac{1}{1+e^{-z}}$$

        Here \(z\) is defined as:

        $$z = \theta_0 + \theta_1 x_1 + \theta_2 x_2+ ... \theta_n x_n = \theta^T x$$

        and
        <ul>
          <li>\(P(Y=1 | X)\) is the probability of the outcome,</li>
          <li>\(X = (x_1, x_2, ..., x_n)\) represents the input features variables.</li>
          <li>\(\theta_0\) is the intercept, \(\theta_1, \theta_2, ..., \theta_n\) are the coefficients for the  input features variables.</li>
          <li>\(z\) is the linear combination of the input features.</li>
          <li>\(\theta^T\) represents the transpose of the parameter vector.</li>
          <li>'\(e\)' is the base of the natural logarithm (approximately equal to 2.71828).</li>
        </ul>
        
        <figure>
          <img src="assets/img/machine-ln/logistic-fun.png" alt="" style="max-width: 70%; max-height: auto;">
          <figcaption></figcaption>
        </figure>
        
         

        <div class="box">
          Sometimes the logistic function is also represemnted in more compact form as:
          $$h_\theta(x) = \frac{1}{1+e^{-\theta^T x}}$$
          where:
          <ul>
            <li>\(h_\theta(x)\) is the predicted probability that \(y = 1\).</li>
            <li>\(\theta\) is the parameter vector</li>
            <li>\(x\) is the input feature vector</li>
          </ul>
        </div>

        The logistic function has an S-shaped curve (thus also known as Sigmoid function).
        The logistic function maps any real-valued number to a value between 0 and 1, which can be interpreted as a probability. The goal of logistic regression is to find the values of the coefficients that maximize the likelihood of 
        observing the data, given the model. This is typically done using maximum likelihood estimation. Once the coefficients have been estimated, the logistic regression model can be used to predict the probability of the outcome 
        for new observations. For example, if we have a new observation with predictor variables \(x_1=1, x_2=3\) and \(x_3=4\), and the estimated coefficients are \(\theta_0 = -1, \theta_1 =0.5, \theta_2 = 1\) and \(\theta_3 = 0.2\), 
        we can calculate the probability of the outcome as follows:
        
        $$z = -1 +0.52 +13 +0.2 \times 4 = 3.3$$

        and hence,

        $$P(Y = 1 | X) = \frac{1}{1+e^{-3.3}} = 0.96$$

        Therefore, the probability of the outcome for this new observation is 0.96. It's important to note that logistic regression assumes a linear relationship between the predictor variables and the log odds of the outcome. This means that the logistic 
        regression model assumes that the relationship between the predictor variables and the probability of the outcome can be modeled using a linear equation.
        
        <p>From above equation, we can re write following equations:</p>
        $$\text{ln}\left(\frac{P(Y=1 | X)}{1- P(Y=1 | X)}\right) = z = \theta_0 + \theta_1 x_1 + ... + \theta_n x_n . $$
 
        The right hand side of the equation is a linear function. Such models are called <strong>generalized linear models (GLM)</strong>. In GLM, the errors may not follow normal distribution and there exists a transformation function of the outcome variable that takes a linear 
        functional form.

        <!----------------->
        <h3>Cost function for Logistic Regression</h3>
        The cost function for the logistic regression is used to quantify the error between preducted and actua; values.
        The objective is to minimize this cost. The cost function is defined as:
        $$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] $$
        Here,
        <ul>
          <li>\(m\) is the number of training examples.</li>
          <li>\(y^{(i)}\) is the actual output for the i-th training example.</li>
          <li>\(h_\theta (x^{(i)})\) is the predicted output for the i-th training example using the logisict function.</li>
        </ul>
        The cost function penalizes large errors, and the logarithmic terms ensure that the cost is higher when the prediction is far from the actual value.

        In training, the goal is to find the parameter vector \(\theta\) that minimizes the cost function. This is often done using optimization algorithms like gradient descent.

        <!----------------->
        <h4>Gradient Descent Rule:</h4> 
        The gradient descent algorithm for the logistic regression aims to find the optimal parameters \(\theta\)
        that minimize the cost function. The update rule is derived from the partial derivatives of the cost function with respect to each parameter. 

        <p>The update rule for the gradient descent algorithm is as follows:</p>
        $$\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}$$

        where:
        <ul>
          <li>\(\theta_j\) is the j-th parameter (weight or coefficient).</li>
          <li>\(\alpha\) is the learning rate, a positive scalar determining the size of each step in the parameter  space.</li>
          <li>\(\frac{\partial J(\theta)}{\partial \theta_j}\) is the partial derivative of the cost function \(J(\theta)\) with respect to \(\theta_j\).</li>
        </ul>

        <h5>Algorith steps:</h5>
        <ol>
          <li>Initialize parameters: \(\theta_0, \theta_1, ..., \theta_n\).</li>
          <li>Repeat until convergence:
            <ul>
              <li>Update each parameter simultaneously using the update rule.</li>
              <li>Simultaneous update means calculating the new values of all parameters before applying them to the model.</li>
              <li>Repeat the update until the change in the cost function becomes small or a fixed number of iterations are reached.</li>
            </ul>
          </li>
        </ol>

        <!----------------->
        <h4>Model Interpretation</h4>
        <ul>
          <li>The logistic function outputs values between 0 and 1, representing probabilities.</li>
          <li>The decision boundary is where \(P(Y=1 | X) =0.5\). If \(P(Y=1 | X) > 0.5\), the instance is predicted to belong to class 1; otherwise, it belongs to class 0.</li>
        </ul>


        <!----------------->
        <div class="box">
          <h5><strong><a href="https://arunp77.github.io/Arun-Kumar-Pandey/Linear-reg.html#Maximum-likelihood-estimation" target="_blank">Maximum Likelihood Estimation (MLE)</a></strong></h5>
          Maximum Likelihood Estimation (MLE) is a statistical method used in the context of logistic regression (and more broadly in statistical modeling) to estimate the parameters of a model. In the case of logistic regression, MLE aims to find the set of parameters that maximizes the likelihood function, which measures how well the model explains the observed data.
          <p><strong>Mathematical Description:</strong>
          For logistic regression, let's denote the likelihood function as \(L(\theta)\), where \(\theta\) represents the parameters of the logistic regression model. The likelihood function is given by the product of the 
          probabilities of the observed outcomes under the current parameter values.</p>

          <p>For a binary classification problem (0 or 1), the likelihood function is often expressed as:</p>

          $$L(\theta) = \Pi_{i=1}^m P(y^{(i)}|x^{(i)}; \theta)^{y^{(i)}}\cdot \left(1-P(y^{(i)}|x^{(i)}; \theta)\right)^{1-y^{(i)}}.$$

          Here:

          <ul>
            <li>\(m\) is the number of training examples.</li>
            <li>\(y^{(i)}\) is the actual output for the i-th training example (0 or 1).</li>
            <li>\(x^{(i)}\) is the input feature vector for the i-th training example.</li>
            <li>\(P(y^{(i)|x^{(i); \theta}})\) is the predicted probabolity if the i-th example belonging to class 1.</li>
          </ul>

          In logistic regression, the predicted probability is given by the logistic function:

          $$P(y^{(i)} =1| x^{(i)}; \theta) = h_\theta(x^{(i)})$$

          $$P(y^{(i)} =0| x^{(i)}; \theta) = 1- h_\theta(x^{(i)})$$
          
          The goal of MLE is to find the values of \(\theta\) that maximize the likelihood function \(L(\theta)\). In practice, it 
          is often more convenient to maximize the log-likelihood function (logarithm of the likelihood function), denoted as \(l(\theta)\):

          $$l(\theta) = \sum_{i=1}^m \left[y^{(i)} \text{log}(h_\theta(x^{(i)}))- (1-y^{(i)})\text{log}(1-h_\theta(x^{(i)}))\right].$$

          Maximizing \(l(\theta)\) is equivalent to maximizing \(L(\theta)\), as the logrithm is a monotonically increasing function.

          <p>The logistic regression cost function \(J(\theta)\) that we discussed earlier is essentially the negative log-likelihood, with some scaling for convenience in optimization:</p>

          $$J(\theta) = -\frac{1}{m} l(\theta).$$

          So, in logistic regression, the optimization process, whether using gradient descent or another optimization algorithm, is essentially performing Maximum Likelihood Estimation to find the parameters that maximize the likelihood of observing the given set of training examples.


        </div>





        <!----------------->
        <h4>Training Objective</h4>
        The model is trained by finding the optimal values for \(\theta\) that minimize the prediction error. This is typically done using techniques like Maximum Likelihood Estimation (MLE).







      </section>

  
      <section id="Example">
        <ul>
            <li>You can go to <a href="https://github.com/arunp77/Machine-Learning/tree/main/Projects-ML" target="_blank">following project</a> for a reference for linear regression analysis. </li>
        </ul>
      </section>
     


      <!-------Reference ------->
      <section id="reference">
        <h2>References</h2>
        <ul>
          <li>My github Repositories on Remote sensing <a href="https://github.com/arunp77/Machine-Learning/" target="_blank">Machine learning</a></li>
          <li><a href="https://mlu-explain.github.io/linear-regression/" target="_blank">A Visual Introduction To Linear regression</a> (Best reference for theory and visualization).</li>
          <li>Book on Regression model: <a href="https://avehtari.github.io/ROS-Examples/" target="_blank">Regression and Other Stories</a></li>
          <li>Book on Statistics: <a href="https://hastie.su.domains/Papers/ESLII.pdf" target="_blank">The Elements of Statistical Learning</a></li>
          <li><a href="https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf">https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf</a></li>
        </ul>
      </section>

      <hr>
      
      <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

        <h3>Some other interesting things to know:</h3>
        <ul style="list-style-type: disc; margin-left: 30px;">
            <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
            <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
        </ul>
      </div>
      <p></p>

      <div class="navigation">
          <a href="index.html#portfolio" class="clickable-box">
              <span class="arrow-left">Portfolio section</span>
          </a>
          
          <a href="portfolio-details-1.html" class="clickable-box">
              <span class="arrow-right">Content</span>
          </a>
      </div>
  </div>
</div>
</section><!-- End Portfolio Details Section -->
</main><!-- End #main --

<!-- ======= Footer ======= -->
<footer id="footer">
  <div class="container">
    <div class="copyright">
      &copy; Copyright <strong><span>Arun</span></strong>
    </div>
  </div>
</footer><!-- End  Footer -->

<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/typed.js/typed.umd.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    hljs.initHighlightingOnLoad();
  });
</script>

</body>

</html>