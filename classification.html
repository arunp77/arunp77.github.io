<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Classification models</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Arun</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
          <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
          <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
          <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
          <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
          <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
          <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
          <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
          <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
          <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
          <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
          <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
          <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
          <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

<main id="main">

        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs">
          <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
              <h2></h2>
              <ol>
                <li><a href="machine-learning.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
              </ol>
            </div>
    
          </div>
        </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
      
      <div class="dropdown">
          <button class="dropbtn"><strong>Shortcuts:</strong></button>
          <div class="dropdown-content">
              <ul>
                  <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                  <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                  <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                  <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                  <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                  <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                  <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                  <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                  <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                  <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                  <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                  <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                  <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                    <!-- Add more subsections as needed -->
                </ul>
          </div>
        </div>
    </div>

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">
        <div class="row gy-4">
          <h1>Classification in Machine Learning: An Introduction</h1>
          <div class="col-lg-8">
            <div class="portfolio-details-slider swiper">
              <div class="swiper-wrapper align-items-center">
                <div class="swiper-slide">
                    <figure>
                      <img src="assets/img/data-engineering/classification.png" alt="" style="max-width: 50%; max-height: 50%;">
                      <figcaption></figcaption>
                    </figure>
                </div>
              </div>
          </div>
        </div>

        <div class="col-lg-4 grey-box">
          
          <div class="section-title">
            <h3>Content</h3>
            <ol>
              <li><a href="#introduction">Introduction</a></li>
              <li><a href="#learners">Learners in Classification Problems</a></li>
              <li><a href="#classification">Classification Predictive Modeling</a></li>
                <ul>
                  <li><a href="#binary">Binary Classification</a></li>
                  <li><a href="#multi-class">Multi-Class Classification</a></li>
                  <li><a href="#multi-label">Multi-Label Classification</a></li>
                  <li><a href="#Imbalanced">Imbalanced Classification</a></li>
                </ul>
              <li><a href="#Common-algorithms">Types of Classification Algorithms</a></li>
              <li><a href="#reference">Reference</a></li>
          </ol>
          </div>
        </div>
      </div>

      <section id="introdction">
        <h2>Introduction</h2>
        Classification is a supervised machine learning task. Classification is defined as the process of recognition, understanding, and grouping of objects and ideas into categories. With the help of these pre-categories training datasets, classification in machine learning programs 
        leverage a wide range of algorithms to classify future datasets into respective and relevant categories. Classification problems are an important category of problems 
        in analytics in which the outcome variable or response variable ('\(y\)') takes discrete values. Primary objective of a classification model is to predict 
        the probability of an observation belonging to a class, known as <strong>class probability</strong>.
        
        <p>
        </p>

        <p><strong>Example: </strong>For example, a spam filter can be used to classify emails as spam or not spam. A credit card fraud detection system can be used to classify transactions as legitimate or fraudulent.</p>
        
        <!--------------------------->
        <h3 id="learners">Learners in Classification Problems</h3>
        There are two types of learners:
        <ul>
          <li><strong>Lazy Learners: </strong>In lazy learning, the algorithm initially stores the training dataset and then awaits the arrival of the test dataset. Classification is conducted using the most relevant data from the training dataset. While this approach reduces training time, it requires more time for predictions. Examples of lazy learning algorithms include case-based reasoning and the k-Nearest Neighbors (KNN) algorithm.</li>
          <li><strong>Eager Learners: </strong>Eager learners construct a classification model using the training dataset before obtaining a test dataset. They invest more time in studying the data and less time in making predictions. Examples of eager learning algorithms include Artificial Neural Networks (ANN), naive Bayes, and Decision Trees.</li>
        </ul>

        <table>
          <tr>
              <th><strong>Differences</strong></th>
              <th><strong>Eager learners</strong></th>
              <th><strong>Lazy learners</strong></th>
          </tr>
          <tr>
            <td>Training Approach:</td>
            <td>Build the classification model using the entire training dataset upfront.</td>
            <td>Delay building the model until a test instance needs to be classified, using only local or on-demand learning.</td>
          </tr>
          <tr>
            <td>Time Allocation:</td>
            <td>Spend more time upfront during the training phase, analyzing and processing the entire dataset.</td>
            <td>Spend less time on initial training, as they postpone processing until specific instances require classification.</td>
          </tr>
          <tr>
            <td>Prediction Time:</td>
            <td>Typically faster at prediction time since the model is already built and ready to use.</td>
            <td>Can be slower at prediction time because they need to perform computations or comparisons on-demand.</td>
          </tr>
          <tr>
            <td>Resource Consumption:</td>
            <td>Require more memory and computational resources upfront due to processing the entire dataset during training.</td>
            <td>May consume fewer resources initially but may require more resources at prediction time for local processing.</td>
          </tr>
          <tr>
            <td>Examples:</td>
            <td>Examples include algorithms like Artificial Neural Networks (ANN), naive Bayes, and Decision Trees.</td>
            <td>Examples include case-based reasoning and the k-nearest neighbors (KNN) algorithm.</td>
          </tr>
        </table>


        <br>
        <h3 id="classification">Classification Predictive Modeling</h3>
        In machine learning, classification problems involve predicting a category for a given input. These problems are everywhere:
        <ul>
          <li>Think of sorting emails as either spam or not spam.</li>
          <li>Imagine recognizing handwritten characters, like letters or digits.</li>
          <li>Consider deciding if a user's behavior indicates they might stop using a service (churn).</li>
        </ul>
        A training dataset with numerous examples of inputs and outputs is necessary for classification from a modeling standpoint. 
        A model will determine the optimal way to map samples of input data to certain class labels using the training dataset. The training dataset 
        must therefore contain a large number of samples of each class label and be suitably representative of the problem.
        There are numerous varieties of algorithms for classification in modeling problems, including predictive modeling and classification.
        We can classify the classification algorithms in following 4 categories:
        <ol>
          <li id="binary"><strong>Binary Classification:</strong> Sorting into two categories, like spam or not spam. Binary classification problems often require two classes, one representing the normal state and the other representing the 
            aberrant state. For instance, the normal condition is "not spam," while the abnormal state is "spam." Class label 0 is given to the class in the normal state, whereas class label 1 is given to the class in the abnormal condition.
            A model that forecasts a <strong><em>Bernoulli probability distribution</strong></em> for each case is frequently used to represent a binary classification task. 
            <div class="grey-box">Let \(X\) be a normal variable representing the input features, and \(y\) be a binary random variable representing the class label (0 or 1).
              The  Bernoulli distribution models the probability of success (or the positive outcome, often denoted as \(p\))
              in a single Bernoulli trial. The probability mass function (PMF) of the Bernoulli distribution is given by:
              $$P(y=1|X) = p$$
              $$P(y=0|X) = 1-p$$
              Here \(p\) represents the probability of the positive outcome given the input features \(X\).
              (For more details, see <a href="Descriptive-statistics.html">Descriptive statistics</a>.)
            </div>
            The following are well-known binary classification algorithms:
            <ul>
              <li>Logistic Regression</li>
              <li>Support Vector Machines</li>
              <li>Simple Bayes</li>
              <li>Decision Trees</li>
            </ul>
          </li>
          <li id="multi-class"><strong>Multi-Class Classification:</strong> Sorting into several categories, like different types of animals.
            The multi-class classification does not have the idea of normal and abnormal outcomes, in contrast to binary classification. Instead, instances are grouped into one of several well-known classes.
            In some cases, the number of class labels could be rather high. In a facial recognition system, for instance, a model might predict that a shot belongs to one of thousands or tens of thousands of faces.
            Text translation models and other problems involving word prediction could be categorized as a particular case of multi-class classification.
            Multiclass classification tasks are frequently modeled using a model that forecasts a <strong><em>Multinoulli probability distribution</em></strong> for each example.
            <div class="grey-box">
              A Multinoulli probability distribution, also known as a categorical distribution, is a generalization of the Bernoulli distribution to more than two categories. Instead of having just two possible outcomes (e.g., 0 and 1), a Multinoulli distribution accommodates multiple discrete outcomes, each with its own probability.
              Mathematically, let \(X\) be a random variable representing the input features, and \(y\) be a categorical random variable representing the class label among \(K\) possible categories (where \(K\) is greater than 2).
              The Multinoulli distribution models the probability of each category \(k\) in a single trial. The probability mass function (PMF) of the Multinoulli distribution is given by:
              $$P(y=k|X) = p_k$$
              where \(p_k\) represents the probability of category \(k\) given the input features \(X\), and \(\sum_{k=1}^K p_k = 1\) (since the probabilities must sum up to 1 for all possible categories).
              (For more details, see <a href="Descriptive-statistics.html">Descriptive statistics</a>.)
            </div>
            For multi-class classification, many binary classification techniques are applicable. The following well-known algorithms can be used for multi-class classification:
              <ul>
                <li>Progressive Boosting</li>
                <li>Choice trees</li>
                <li>Nearest K Neighbors</li>
                <li>Rough Forest</li>
                <li>Simple Bayes</li>
              </ul>
              In multi-class classification tasks, the "one-vs-rest" (OvR) and "one-vs-one" (OvO) methods are commonly used.
              <ul>
                <li><strong>One-vs-Rest (OvR): </strong> Also known as "one model for each class," this method involves training multiple binary classification models, each classifying one class against all other classes.</li>
                <li><strong>One-vs-One (OvO): </strong> In this approach, a binary classifier is trained for each pair of classes. This results in \(\frac{N\times (N-1)}{2}\) classifiers for \(N\) classes.</li>
              </ul>
          </li>
          <li id="multi-label"><strong>Multi-Label Classification:</strong> Multi-label classification involves predicting two or more class labels for each example, unlike multi-class or binary classification, where only one label is predicted.
            For example: tagging a post with multiple topics, or  in photo classification, a model might predict the presence of multiple objects like "person," "apple," and "bicycle" in a single image, as opposed to just one label.
            <p>Multi-label classification problems are often addressed using models that predict multiple outcomes, treating each prediction as a binary classification. This means each label is predicted independently, similar to a 
              Bernoulli probability distribution.</p>
            Specialized versions of conventional classification algorithms, such as 
            <ul>
              <li>Multi-label Gradient Boosting,</li> 
              <li>Multi-label Random Forests, and</li> 
              <li>Multi-label Decision Trees</li>
            </ul>
            are used for multi-label classification tasks. 
          </li>
          <div class="grey-box">
            <p><strong>Difference between the Multi-class and Multi-Label classification models:</strong></p>
            <p>In multi-class classification, each example is assigned to one and only one class label from a set of two or more mutually exclusive classes. The goal is to predict the single most appropriate class label for each instance. 
              Examples include predicting the species of a flower from among several categories (e.g., iris, daisy, rose) or classifying emails into different folders (e.g., work, personal, promotions). Only one class label is assigned to 
              each instance.</p>
            <p>In multi-label classification, each example can be assigned to multiple class labels simultaneously. The model predicts the presence or absence of each label independently for every instance. This approach is suitable for 
              problems where instances may belong to multiple classes at the same time. For example, in image classification, a single image may contain multiple objects (e.g., person, dog, car), and the model needs to identify all of 
              them. Multiple class labels can be assigned to each instance.</p>
          </div>
          <li id="Imbalanced"><strong>Imbalanced Classification:</strong> Dealing with datasets where one category is much more common than the others. 
            Imbalanced classification refers to a scenario in machine learning where the distribution of class labels in the dataset is highly skewed, with one class significantly outnumbering the others. This can pose challenges for 
            predictive modeling, as the model may become biased towards the majority class and perform poorly on the minority class(es).
            Addressing imbalanced classification typically involves techniques such as:
            <ul>
              <li><strong>Resampling:</strong> This involves either oversampling the minority class, undersampling the majority class, or a combination of both to rebalance the dataset.</li>
              <li><strong>Algorithmic Approaches:</strong> Some algorithms are specifically designed to handle imbalanced datasets better, such as ensemble methods like Random Forests and Gradient Boosting, as well as various techniques like cost-sensitive learning and anomaly detection.</li>
              <li><strong>Evaluation Strategies:</strong> Choosing appropriate evaluation metrics that are sensitive to the imbalance, such as precision, recall, and F1-score, can provide a more nuanced understanding of the model's performance.</li>
            </ul>
          </li>
        </ol>

        <!----------------->
        <h3 id="Common-algorithms">Types of Classification Algorithms</h3>
        Classification problems may have binary or multiple outcomes or classes. Binary outcomes are called 
        binary classification and multiple outcomes are called multinomial classification.
        There are several techniques used for solving classification problems such as:        
        <ol>
          <li><strong><a href="logistic-regression.html">Logistic regression</a>: </strong>A linear model used for binary classification. It estimates the probability that an instance belongs to a particular class based on its features. Despite its name, logistic regression is a classification algorithm rather than a regression algorithm.</li>
          <li><strong>Decision trees: </strong>A tree-like model where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents a class label. Decision trees are easy to interpret and can handle both numerical and categorical data.</li>
          <li><strong><a href="knn.html">K-nearest neighbors (KNN)</a>: </strong>A non-parametric, instance-based learning algorithm used for both classification and regression tasks. KNN classifies instances based on their similarity to nearby instances in the feature space. It is simple but computationally expensive for large datasets.</li>
          <li><strong>Support vector machines (SVMs): </strong>A supervised learning algorithm that separates instances into different classes by finding the hyperplane that maximizes the margin between classes. SVMs are effective in high-dimensional spaces and are versatile due to their kernel trick, which allows them to handle nonlinear data.</li>
          <li><strong>Random Forest Algorithm: </strong>An ensemble learning method that constructs a multitude of decision trees during training and outputs the mode of the classes (classification) or the mean prediction (regression) of the individual trees. Random forests are robust against overfitting and can handle high-dimensional data.</li>
          <li><strong><a href="naive-byes.html">Naive Bayes</a>: </strong>A probabilistic classifier based on Bayes' theorem with the "naive" assumption of independence between features. It is simple and efficient, making it suitable for large datasets. Naive Bayes classifiers are often used for text classification tasks.</li>
        </ol>

      </section>

  
      <div class="grey-box">
        <ul>
          <li>For more details on Logistic regression, go to <a href="logistic-regression.html">page</a>.</li>
          <li>You can go to <a href="https://github.com/arunp77/Machine-Learning/tree/main/Projects-ML" target="_blank">following project</a> for a reference for linear regression analysis. </li>
        </ul>
      </div>


      <!-------Reference ------->
      <section id="reference">
        <h2>References</h2>
        <ul>
          <li>My github Repositories on Remote sensing <a href="https://github.com/arunp77/Machine-Learning/" target="_blank">Machine learning</a></li>
          <li><a href="https://mlu-explain.github.io/linear-regression/" target="_blank">A Visual Introduction To Linear regression</a> (Best reference for theory and visualization).</li>
          <li>Book on Regression model: <a href="https://avehtari.github.io/ROS-Examples/" target="_blank">Regression and Other Stories</a></li>
          <li>Book on Statistics: <a href="https://hastie.su.domains/Papers/ESLII.pdf" target="_blank">The Elements of Statistical Learning</a></li>
          <li><a href="https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf">https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf</a></li>
        </ul>
      </section>

      <hr>
      
      <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

        <h3>Some other interesting things to know:</h3>
        <ul style="list-style-type: disc; margin-left: 30px;">
            <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
            <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
        </ul>
      </div>
      <p></p>

      <div class="navigation">
          <a href="index.html#portfolio" class="clickable-box">
              <span class="arrow-left">Portfolio section</span>
          </a>
          
          <a href="machine-learning.html" class="clickable-box">
              <span class="arrow-right">Content</span>
          </a>
      </div>
  </div>
</div>
</section><!-- End Portfolio Details Section -->
</main><!-- End #main --

<!-- ======= Footer ======= -->
<footer id="footer">
  <div class="container">
    <div class="copyright">
      &copy; Copyright <strong><span>Arun</span></strong>
    </div>
  </div>
</footer><!-- End  Footer -->

<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/typed.js/typed.umd.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    hljs.initHighlightingOnLoad();
  });
</script>

</body>

</html>