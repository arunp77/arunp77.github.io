<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Pyspark</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

    <!-- ======= Header ======= -->
    <header id="header">
    <div class="d-flex flex-column">
        <div class="profile">
            <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
            <h1 class="text-light"><a href="index.html">Arun</a></h1>
            <div class="social-links mt-3 text-center">
                <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
                <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
                <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
                <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
            </div>
        </div>

        <nav id="navbar" class="nav-menu navbar">
            <ul>
                <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
                <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
                <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
                <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
                <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
                <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
                <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
                <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
                <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
                <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
            </ul>
        </nav><!-- .nav-menu -->
    </div>
    </header><!-- End Header -->

    <main id="main">
        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs"> 
          <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
              <h2>Data Engineering</h2>
              <ol>
                <li><a href="Data-engineering.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
              </ol>
            </div>
    
          </div>
        </section><!-- End Breadcrumbs -->

        <!------  right dropdown menue ------->
        <div class="right-side-list">
            <div class="dropdown">
                <button class="dropbtn"><strong>Shortcuts:</strong></button>
                <div class="dropdown-content">
                    <ul>
                        <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                        <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                        <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                        <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                        <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                        <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                        <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                        <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                        <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                        <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                        <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                        <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                        <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                            <!-- Add more subsections as needed -->
                    </ul>
                </div>
            </div>
        </div>

        <!-- ======= Portfolio Details Section ======= -->
        <section id="portfolio-details" class="portfolio-details">
        <div class="container">
            <div class="row gy-4">
                <h1>Apache Pyspark</h1>
                <div class="col-lg-8">
                    <div class="portfolio-details-slider swiper">
                        <div class="swiper-wrapper align-items-center"> 
                            <figure>
                            <img src="assets/img/data-engineering/Apache_Spark_logo.svg.png" alt="" style="max-width: 50%; max-height: auto;">
                            <figcaption></figcaption>
                            </figure>
                        </div>
                        <div class="swiper-pagination"></div>
                    </div>
                </div>

                <div class="col-lg-4 grey-box">
                    <div class="section-title">
                        <h3>Table of Contents</h3>
                        <ol>
                            <li><a href="#introduction">Introduction to Apache Spark</a></li>
                            <li><a href="#how-it-works">How does Spark work?</a></li>
                            <ul>
                                <li><a href="#difference">Difference between Hadoop and Apache Spark</a></li>
                                <li><a href="#key-concept">Some key concepts of Apache Spark</a></li>
                                <li><a href="#key-features">Key Features</a></li>
                            </ul>
                            <li><a href="#Components">Components of Apache Spark</a></li>
                            
                            <li><a href="#reference">Reference</a></li>
                        </ol>
                    </div>
                </div>
            </div>

            <section>
            <h3 id="introduction">Introduction to Apache pySpark</h3>
            PySpark is the Python API for Apache Spark, an open-source, distributed computing framework designed for large-scale data processing. It allows you to leverage the power of Spark from the familiar and approachable Python environment.
            It provides Python bindings that allow developers to create, test, and deploy applications using the Spark API. 

            <h3 id="summary-spark">Summary Apache Spark</h3>
            <ul>
                <li><strong>Definition: </strong>Apache Spark is an Open source analytical processing engine for large-scale powerful distributed data processing and machine learning applications.
                    Apache Spark 3.5 is a framework that is supported in Scala, Python, R Programming, and Java. Below are different implementations of Spark.
                    <ul>
                        <li>Spark - Default interface for Scala and Java</li>
                        <li>PySpark – Python interface for Spark</li>
                        <li>SparklyR – R interface for Spark.</li>
                    </ul>
                </li>
                <li><strong>Features of Apache Spark:</strong>
                    <ul>
                        <li>In-memory computation</li>
                        <li>Distributed processing using parallelize</li>
                        <li>Can be used with many cluster managers (Spark, Yarn, Mesos e.t.c)</li>
                        <li>Fault-tolerant</li>
                        <li>Immutable</li>
                        <li>Lazy evaluation</li>
                        <li>Cache & persistence</li>
                        <li>Inbuild-optimization when using DataFrames</li>
                        <li>Supports ANSI SQL</li>
                    </ul>
                </li>
                <li><strong>Advantages of Apache Spark:</strong>
                    <ul>
                        <li>Spark is a general-purpose, in-memory, fault-tolerant, distributed processing engine that allows you to process data efficiently in a distributed fashion.</li>
                        <li>Applications running on Spark are 100x faster than traditional systems.</li>
                        <li>There are so many benefits from using Spark for data ingestion pipelines.</li>
                        <li>Using Spark we can process data from Hadoop HDFS, AWS S3, Databricks DBFS, Azure Blob Storage, and many file systems.</li>
                        <li>Spark also is used to process real-time data using Streaming and Kafka.</li>
                        <li>Using Spark Streaming you can also stream files from the file system and also stream from the socket.</li>
                        <li>Spark natively has machine learning and graph libraries.</li>
                        <li>Provides connectors to store the data in NoSQL databases like MongoDB.</li>
                    </ul>
                </li>
                <li><strong>Apache Spark Architecture: </strong>
                    Spark works in a master-slave architecture where the master is called the “Driver” and slaves are called “Workers”. When you run a Spark application, Spark Driver creates a context that is an entry point to your application, and all operations (transformations and actions) are executed on worker nodes, and the resources are managed by Cluster Manager.
                    <figure>
                        <img src="assets/img/data-engineering/pyspark-summary.png" alt="" style="max-width: 50%; max-height: auto;">
                        <figcaption></figcaption>
                    </figure>
                </li>
                <li><strong>Cluster Manager Types: </strong>Standalone, Apache Mesos, Hadoop YARN, Kubernetes, local (for master() in order to run Spark on local computer).</li>
                <li><strong>Spark Modules: </strong>Spark Core, Spark SQL, Spark Streaming, Spark MLlib, Spark GraphX</li>
                <li><strong>Spark Core: </strong>Spark Core is the main base library of Spark which provides the abstraction of how distributed task dispatching, scheduling, basic I/O functionalities etc.</li>
                <li><strong>SparkSession: </strong>It is an entry point to underlying Spark functionality in order to programmetically use Spark RDD, DataFrame, and Dataset. It's object <code>spark</code> is default available in spark-shell.
                    The initial step in a Spark program involving RDDs, DataFrames, or Datasets would be to create a SparkSession instance. The Sparksession will be created using <code>SparkSession.builder()</code>. 
                    <pre class="language-python"><code>
                from pyspark.sql import SparkSession
                # Create a SparkSession object
                spark = SparkSession.builder \
                    .appName("YourAppName") \
                    .master("local[*]") \
                    .getOrCreate()
            
                spark # to call spark
                    </code></pre>                        
                If you're using SparkSession, which is the entry point to Spark SQL, you can work with DataFrames and Datasets, which provide higher-level abstractions and optimizations compared to RDDs.
                Here are some common tasks you can perform with SparkSession:
                <ul>
                    <li><strong>Read and write data: </strong>SparkSession provides methods to read data from various sources such as Parquet, JSON, CSV, JDBC, Avro, ORC, and many more. Similarly, you can write data to different formats and locations.
                        For example:<code>df = spark.read.csv("file.csv")</code> and <code>df.write.parquet("output.parquet")</code></li>
                    <li><strong>Create DataFrames: </strong>You can create DataFrames from existing RDDs, lists, dictionaries, or by applying transformations on other DataFrames. For example:<code></code>
                        <pre class="language-python"><code>
                            from pyspark.sql import Row

                            data = [(1, "Alice"), (2, "Bob")]
                            rdd = sc.parallelize(data)
                            row_rdd = rdd.map(lambda x: Row(id=x[0], name=x[1]))
                            df = spark.createDataFrame(row_rdd)
                        </code></pre>
                    </li>
                    <li><strong>SQL queries: </strong>SparkSession allows you to run SQL queries on DataFrames using the sql() method. For example:
                    <pre class="language-python"><code>
                        df.createOrReplaceTempView("people")
                        result = spark.sql("SELECT * FROM people WHERE age > 20")
                    </code></pre>
                    </li>
                    <li><strong>DataFrame operations: </strong>You can perform various operations on DataFrames like <code>select()</code>, <code>filter()</code>, <code>join()</code>, <code>orderBy()</code>, <code>agg()</code> etc. For example:
                        <pre class="language-python"><code>result = df.select("name").filter(df.age > 20).orderBy(df.name)</code></pre>
                    </li>
                    <li><strong>Window functions: </strong>You can use window functions for advanced analytics tasks like ranking, lead/lag analysis, etc. For example: 
                    <pre class="language-python"><code>
                    from pyspark.sql.window import Window
                    from pyspark.sql.functions import rank
                    
                    window = Window.partitionBy("department").orderBy("salary")
                    result = df.withColumn("rank", rank().over(window))
                    </code></pre>
                    </li>
                    <li><strong>Machine learning: </strong>We can use Spark MLlib, a scalable machine learning library, to train and apply machine learning models on DataFrames. For example:
                        <pre class="language-python"><code>
                            from pyspark.ml.classification import LogisticRegression

                            lr = LogisticRegression(featuresCol="features", labelCol="label")
                            model = lr.fit(train_data)
                            predictions = model.transform(test_data)
                        </code></pre>
                    </li>
                    <li><strong>Streaming data:</strong>We can process streaming data using Spark Structured Streaming, which provides high-level APIs for streaming processing. For example:
                        <pre class="language-python"><code>stream_df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "host:port").load()</code></pre>
                    </li>
                </ul>
                </li>
                <li><strong>Spark Context: </strong>It is also a entry point to Spark and PySpark. Before SparkSession 2.0, it was the main entry point.
                    Creating SparkContext was the first step to the program with RDD and to connect to Spark Cluster. It’s object sc by default available in <code>spark-shell</code>.
                    Since Spark 2.0, when we create <code>SparkSession</code>, <code>SparkContext</code> object is by default created and it can be accessed using <code>spark.sparkContext</code>.
                <pre class="language-python"><code>
                from pyspark.sql import SparkSession

                # Create a SparkSession
                spark = SparkSession.builder \
                    .appName("YourAppName") \
                    .master("local[*]") \
                    .getOrCreate()
                
                # Access the SparkContext from the SparkSession
                sc = spark.sparkContext
                </code></pre>
                Once the spark context is created, we can do following things:
                    <ul>
                        <li><strong>Parallelize data: </strong>We can parallelize an existing collection in your driver program (e.g., a list or array) using the <code>parallelize()</code> method. This will distribute the data across the nodes in your Spark cluster and create an RDD. <code>rdd  = sc.parallelize(data)</code></li>
                        <li><strong>Read data from external sources: </strong>We can read data from various external sources such as HDFS, S3, HBase, or any supported data source using the methods provided by SparkContext, such as <code>textFile()</code> for reading text files.<code>text_rdd = sc.textFile("hdfs://path/to/file.txt")</code></li>
                        <li><strong>Transformations: </strong>can perform various transformations on RDDs, such as <code>map()</code>, <code>filter()</code>, <code>flatMap()</code>, <code>reduceByKey()</code>, etc., to process and manipulate the data. For example: <code>squared_rdd = rdd.map(lambda x: x * x)</code></li>
                        <li><strong>Caching: </strong>We  can cache RDDs in memory to speed up iterative or interactive computations by using the <code>cache()</code> method. For example: <code>rdd.cache()</code></li>
                        <li><strong>Accumulators: </strong>We can can use accumulators to aggregate information across all tasks, such as counting occurrences of certain events. For example: <code>accumulator = sc.accumulator(0)</code>.</li>
                    </ul>
                </li>
                <div class="grey-box">
                    <h5>Difference between SparkContext and SparkSession</h5>
                    <ol>
                        <li><strong>SparkContext (sc): </strong>
                            <ul>
                                <li>SparkContext is the entry point to Spark functionality and represents the connection to a Spark cluster. It coordinates the execution of operations on the cluster.</li>
                                <li>It provides access to the underlying Spark functionality, including RDDs (Resilient Distributed Datasets), which are the fundamental data abstraction in Spark.</li>
                                <li>SparkContext is primarily used for low-level operations and interactions with RDDs, such as creating RDDs, performing transformations, and executing actions.</li>
                            </ul>
                        </li>
                        <li><strong>SparkSession (spark): </strong>
                            <ul>
                                <li>SparkSession, introduced in Spark 2.x, is a higher-level abstraction on top of SparkContext and provides a unified entry point to Spark SQL, DataFrame, and Dataset APIs.</li>
                                <li>It is the recommended way to interact with Spark in modern Spark applications, as it provides a more user-friendly and consistent interface for working with structured data.</li>
                                <li>SparkSession encapsulates SparkContext and provides additional functionalities, including reading and writing structured data from various sources, running SQL queries, and performing DataFrame operations.</li>
                                <li>SparkSession also manages the underlying SparkContext internally, so there's no need to create a SparkContext explicitly when using SparkSession.</li>
                            </ul>
                        </li>
                    </ol>
                    In summary, while SparkContext is primarily focused on low-level distributed computing operations with RDDs, SparkSession provides a higher-level interface for working with structured data, including DataFrames and Datasets, as well as integrating with Spark SQL, MLlib, and other Spark components. It's generally recommended to use SparkSession for modern Spark applications unless you have specific requirements that necessitate working directly with RDDs and SparkContext.
                </div>
            </ul>
            <hr>

            <h3>PySpark methods</h3>
            <table>
                <tr>
                    <th>Functionality</th>
                    <th>PySpark Method</th>
                </tr>
                <tr>
                    <td>SparkSession</td>
                    <td>
                        <ul>
                            <li><code>SparkSession.builder.appName()</code>: Set the application name.</li>
                            <li><code>SparkSession.builder.getOrCreate()</code>: Create a SparkSession or get an existing one.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Reading and Writing Data:</td>
                    <td>
                        <ul>
                            <li><code>spark.read.format().option().load()</code>: Read data from various sources.</li>
                            <li><code>DataFrame.write.format().mode().save()</code>: Write data to various storage systems.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>DataFrame Operations: </td>
                    <td>
                        <ul>
                            <li><code>DataFrame.show()</code>: Display the contents of a DataFrame.</li>
                            <li><code>DataFrame.select()</code>:  Select specific columns.</li>
                            <li><code>DataFrame.filter()</code>: Filter rows based on conditions.</li>
                            <li><code>DataFrame.groupBy()</code>:  Group data based on one or more columns.</li>
                            <li><code>DataFrame.join()</code>: Perform join operations between DataFrames.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Transformations:</td>
                    <td>
                        <ul>
                            <li><code>DataFrame.withColumn()</code>: Add or replace columns.</li>
                            <li><code>DataFrame.drop()</code>: Drop specified columns.</li>
                            <li><code>DataFrame.na.fill()</code>: DataFrame.na.fill()</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Actions</td>
                    <td>
                        <ul>
                            <li><code>DataFrame.count()</code>: Count the number of rows.</li>
                            <li><code>DataFrame.collect()</code>: Retrieve all data from the DataFrame.</li>
                            <li><code>DataFrame.take()</code>: Retrieve a specified number of rows.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>SQL Queries:</td>
                    <td>
                        <ul>
                            <li><code>DataFrame.createOrReplaceTempView()</code>: Register DataFrame as a temporary table.</li>
                            <li><code>spark.sql()</code>: Execute SQL queries on DataFrames.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Data Aggregation and Summary</td>
                    <td>
                        <ul>
                            <li><code>DataFrame.describe()</code>: Generates descriptive statistics</li>
                            <li><code>DataFrame.approxQuantile()</code>: Approximate quantiles of numerical columns.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Data Cleaning and Handling:</td>
                    <td>
                        <ul>
                            <li><code>DataFrame.dropDuplicates()</code>: Remove duplicate rows.</li>
                            <li><code>DataFrame.fillna()</code>:  Replace null values with specified values.</li>
                            <li><code>DataFrame.dropna()</code>: Remove rows with null values.</li>
                            <li><code>DataFrame.replace()</code>: Replace specified values.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Window Functions:</td>
                    <td>
                        <ul>
                            <li><code>pyspark.sql.functions.row_number()</code>: Assigns a unique number to each row within a partition.</li>
                            <li><code>pyspark.sql.functions.rank()</code>: Computes the rank of rows within a partition.</li>
                            <li><code>pyspark.sql.functions.lead()</code> and <code>pyspark.sql.functions.lag()</code>: Access data from subsequent or preceding rows.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Caching</td>
                    <td>
                        <ul>
                            <li><code>DataFrame.cache()</code>: Persist the DataFrame in memory for faster access.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Machine Learning:</td>
                    <td>
                        <ul>
                            <li>PySpark includes various MLlib functions for machine learning tasks.</li>
                            <li><code>pyspark.ml.Pipeline()</code>: Define a machine learning pipeline.</li>
                            <li>PySpark MLlib provides various algorithms for classification, regression, clustering, and collaborative filtering.</li>
                            <li>Methods like <code>pyspark.ml.classification.LogisticRegression()</code>, and <code>pyspark.ml.regression.LinearRegression()</code> etc.</li>
                        </ul>
                    </td>
                </tr>
                
                <tr>
                    <td>Statistical Functions:</td>
                    <td>
                        <ul>
                            <li><code>pyspark.sql.functions.corr()</code></li>
                            <li><code>pyspark.sql.functions.covar_pop()</code> and <code>pyspark.sql.functions.covar_samp()</code>: Compute population and sample covariance.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Time Series and Date Functions:</td>
                    <td>
                        <ul>
                            <li><code>pyspark.sql.functions.year()</code> and <code>pyspark.sql.functions.month()</code>: etc.: Extract components from a date.</li>
                            <li><code>pyspark.sql.functions.datediff()</code>: Calculate the difference between two dates</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Graph Processing (GraphFrames):</td>
                    <td>
                        <ul>
                            <li>GraphFrames is an extension of DataFrames for graph processing.</li>
                            <li>Methods like <code>g.vertices</code>, <code>g.edges</code>, <code>g.bfs()</code>, etc., for graph analysis.</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>User-Defined Functions (UDFs):</td>
                    <td>
                        <ul>
                            <li><code>pyspark.sql.functions.udf()</code>: Define a User-Defined Function.
                            </li>
                            <li><code>pyspark.sql.functions.when()</code>: Conditional expressions in a DataFrame.</li>
                        </ul>
                    </td>
                </tr>
            </table>

            <br>
            <div class="box">
                <h3>Codes on GitHub</h3>
                For the details on how pyspark can be used to do data analytics of a large dataset is available at my 
                <a href="https://github.com/arunp77/Database-datapipeline-ETL/tree/main/pyspark" target="_blank">GitHub repository</a>.
                In this repository, I have done analysis on some examples datasets using Pyspark Dataframe and RDD methods. 
            </div>

            <br>
            <!-------------->
            <h3>RDD's Resilient Distributed Datasets (RDDs)</h3>
            The RDD structure is the elementary structure of Spark. It is flexible and optimal in performance for any  linear operation. 
            However, this structure has limited performance when it comes to non-linear operations.

            <h5>SparkContext:</h5>
            <code>SparkContext</code> is a crucial entry point for any Spark functionality in Apache Spark applications. It coordinates the execution of Spark jobs 
            and manages the resources allocated for these jobs. In Spark versions before 2.0, it was the main entry point for Spark, but in later versions, 
            <code>SparkSession</code> is typically used, which internally creates a <code>SparkContext</code> object.
            <ul>
                <li><strong>Definition: </strong><code>SparkContext</code> is the  main entry point for Spark functionality in Spark applications.</li>
                <li><strong>Purpose: </strong> It coordinates the execution of Spark jobs and manages the resources allocated for these jobs.</li>
                <li><strong>Usage: </strong> In older versions of Spark, it was directly created by developers. In newer versions, it's often implicitly created as part of a <code>SparkSession</code>.</li>
            </ul>

            To start a <code>SparkContext</code>, we typically create an instance of it in the Spark application code. However, in newer versions of Spark (2.0 and above), it's recommended to use 
            <code>SparkSession</code> instead, which internally manages the <code>SparkCOntext</code>. 

            <ul>
                <li><strong>Using SparkContext</strong>:
                    <pre class="language-python"><code>
from pyspark import SparkContext

# Create a SparkContext object
sc = SparkContext("local", "MyApp")
                    </code></pre>
                    In this example:
                    <ul>
                        <li>"local" specifies that Spark should run in local mode using a single thread.</li>
                        <li>"MyApp" is a name for your Spark application.</li>
                    </ul>
                </li>
                <li><strong>Using SparkSession</strong>: For newer versions of Spark, you typically use SparkSession, which internally creates a SparkContext. Here's how you do it:
                    <pre class="language-python"><code>
from pyspark.sql import SparkSession

# Create a SparkSession object
spark = SparkSession \
    .builder \
    .master("local") \
    .appName("MyApp") \
    .getOrCreate()
spark
                    </code></pre>
                    In this case:
                    <ul>
                        <li>"MyApp" is also the name for your Spark application.</li>
                        <li>getOrCreate() method ensures that if a SparkSession already exists, it returns that session; otherwise, it creates a new one.</li>
                    </ul>
                    The <cod>SparkContext</cod> can also be created using SparkSession:
                    <pre class="language-python"><code>
# Creating SparkContext through SparkSession
from pyspark.sql import SparkSession

# Create a SparkContext object
spark = SparkSession.builder \
    .appName("MyApp") \
    .getOrCreate()

# Access SparkContext from sparksession 
sc = spark.SparkContext
                    </code></pre>
                </li>
            </ul>

            <h5>Example:</h5>
            The <code>textFile</code> method of <code>SparkContext</code> allows to load a CSV file into a RDD. 
            <pre class="language-python"><code>rdd = sc.textFile("path_to_file.ext")</code></pre>
            To display top 5 elements of the rdd, we can use <code>take(5)</code> function:
            <pre class="language-python"><code>rdd.take(5)</code></pre>
            To count the number of lines a rdd, <code>count()</code> can be used:
            <pre class="language-python"><code>count = rdd.count()</code></pre>
            <strong>Map & Reduce:</strong>
            Map and Reduce are fundamental operations in distributed computing, popularized by frameworks like Apache Hadoop and Apache Spark. They originated from functional 
            programming concepts and are widely used for processing large-scale datasets efficiently. Here's an explanation of each:

            <ol>
                <li><strong>Map:</strong></li> The map operation applies a function to each element in a dataset and produces a new dataset of the same size. It transforms each element 
                into another element based on the function provided. The map operation is typically used for tasks like data cleaning, transformation, or feature extraction.
                <pre class="language-python"><code>
# Applying a map operation to double each element in a list
numbers = [1, 2, 3, 4, 5]
doubled_numbers = list(map(lambda x: x * 2, numbers))
print(doubled_numbers)
                </code></pre>
                <pre>Output: [2, 4, 6, 8, 10]</pre>
                <li><strong>Reduce:</strong></li> The reduce operation combines elements of a dataset pairwise using a given function until a single result is obtained. It repeatedly 
                applies the function to pairs of elements until only one element remains. The reduce operation is commonly used for tasks like aggregation, summarization, or finding totals.
                <pre class="language-python"><code>
from functools import reduce

# Applying a reduce operation to find the sum of elements in a list
numbers = [1, 2, 3, 4, 5]
total_sum = reduce(lambda x, y: x + y, numbers)
print(total_sum)
                </code></pre>
                <pre>Output: 15</pre>
                The RDD works very well for "line-by-line" operations, which is why the <code>mao</code> and <code>reduce</code> methods are extremely effective for these calculations.
                Applying successively the map method and then the <code>reduceByKey</code>  method allows us to cleverly summarize our data.
                <pre class="language-python"><code>
rdd.map(lambda x: (x[7]:1)).reduceByKey(lambda x,y:x+y)
                </code></pre>
                This combines couples with the same key by the lambda function here to them, and pair (x,y) returns x+y. 
            </ol>
            The map operation is applied in parallel to different partitions of the dataset, and then the results are shuffled and aggregated using the reduce operation to produce the final output.

            <br><br>
            <div class="grey-box">
                <Strong>DataFrame for structured data:</Strong>
                The RDD structure is not optimized for column tasks or Machine learning. The <code>DataFrame</code> structure was created to meet this need.
                It uses the underlying bases of a RDD but it has been structured in columns and rows in a SQL structure in a form inspired by the <code>DataFrame</code> of the python pandas module.
                <p><Strong>Advantages:</Strong> The DataFrame structure has two main Advantages. First of all, this structure is similar to the pandas DataFrame and is therefore
                easy to learn. It is also efficient. A DataFrame om PySpark is as fast as a DataFrame in scala and it is the most optimized distributed structure in machine learning. </p>
                Let's take a example to understand the basics of this:
                <ul>
                    <li>Let's first build the spark session:
                        <pre class="language-python"><code>
# Importing Spark Session and SparkContext
from pyspark.sql import SparkSession
from pyspark import SparkContext

# Definition of a SparkContext
SparkContext.getOrCreate() 

# Definition of a SparkSession
spark = SparkSession \
    .builder \
    .master("local") \
    .appName("Introduction to DataFrame") \
    .getOrCreate()
    
spark
                        </code></pre>
                        <figure>
                            <img src="assets/img/data-engineering/spark-session-1.png" alt="" style="max-width: 50%; max-height: auto;">
                            <figcaption></figcaption>
                        </figure>
                    </li>
                <strong>For more details you can always go to my github repo: <a href="https://github.com/arunp77/Database-datapipeline-ETL/tree/main/pyspark" target="_blank">Pyspark repo</a>.</strong>
                </ul>
            </div>




            </section>

            <!-------Reference ------->
            <section id="reference">
                <h2>References</h2>
                <ol>
                    <li><a href="https://spark.apache.org/documentation.html" target="_blank"> Official Documentation</a></li>
                    <li><a href="https://www.databricks.com/learn/training/login" target="_blank">Databricks Learning Academy</a></li>
                    <li><a href="https://sparkbyexamples.com/" target="_blank">Spark by Examples</a></li>
                    <li><a href="https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark" target="_blank">Datacamp tutorial</a>.</li>
                    <li>For databricks, you can look at tutorial videos on youtube at <a href="https://www.youtube.com/watch?v=ChISx0-cMpU" target="_blank">youtube video by Bryan Cafferky</a>, 
                        writer of the book "Master Azure Databricks". A great playlist for someone who just want to learn about the big data analytics at Databricks Azure cloud platform.</li>
                    <li>See the video for <a href="https://www.youtube.com/watch?v=_C8kWso4ne4" target="_blank">pyspark basics by Krish Naik</a>. Great video for starter.</li>
                    <li><a href="https://www.youtube.com/watch?v=QLGrLFOzMRw" target="_blank">Great youtube on Apache spark</a> one premise working.</li>
                </ol> 
            </section>

            <hr>
        
            <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

                <h3>Some other interesting things to know:</h3>
                <ul style="list-style-type: disc; margin-left: 30px;">
                    <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
                    <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
                </ul>
            </div>
            <p></p>

            <div class="navigation">
                
                <a href="index.html#portfolio" class="clickable-box">
                    <span class="arrow-left">Portfolio section</span>
                </a>
                
                <a href="Data-engineering.html" class="clickable-box">
                    <span class="arrow-right">Content</span>
                </a>

            </div>
        </div>
        </section><!-- End Portfolio Details Section -->
    </main><!-- End #main --

    <!-- ======= Footer ======= -->
    <footer id="footer">
    <div class="container">
        <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
        </div>
    </div>
    </footer><!-- End  Footer -->

    <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="assets/vendor/typed.js/typed.umd.js"></script>
    <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function () {
        hljs.initHighlightingOnLoad();
    });
    </script>

</body>

</html>