<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>ETL-pipeline</title>
  <meta content="" name="description"> 
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- Include Prism styles -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css">

  <!-- Include Prism scripts -->
  <!----<script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script>-->

  <!-- Example for Prism.js -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css" integrity="sha384-uEbBqWkE4LEVKVKJBii7Pb3pDi8svYlmL8jq5vHEewoYW6hDyyMuwQX+FRQ0PK5c" crossorigin="anonymous">
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js" integrity="sha384-uEbBqWkE4LEVKVKJBii7Pb3pDi8svYlmL8jq5vHEewoYW6hDyyMuwQX+FRQ0PK5c" crossorigin="anonymous"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js" integrity="sha384-uEbBqWkE4LEVKVKJBii7Pb3pDi8svYlmL8jq5vHEewoYW6hDyyMuwQX+FRQ0PK5c" crossorigin="anonymous"></script>
  
  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Arun</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
          <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
          <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
          <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
          <a href="https://medium.com/@arunp77" class="medium"><i class="bx bxl-medium"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
            <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
            <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
            <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
            <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
            <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
            <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
            <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
            <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
            <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
            <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

  <main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Portfoio Details</h2>
          <ol>
            <li><a href="Data-engineering.html" class="clickable-box">Go to content</a></li>
          </ol>
        </div>

      </div>
    </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
        <div class="dropdown">
            <button class="dropbtn"><strong>Shorcuts:</strong></button>
            <div class="dropdown-content">
                <ul>
                    <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                    <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                    <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                    <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                    <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(14, 13, 13);"></i> Docker</a></li>
                    <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(3, 3, 3);"></i> Jupyter-nifi</a></li>
                    <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                    <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                    <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                    <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                    <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                    <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                    <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                      <!-- Add more subsections as needed -->
                  </ul>
            </div>
          </div>
      </div>

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">
        <div class="row gy-4">
          <h1>Building an Automated Data Pipeline (ETL) for Spotify's Globally Famous Songs Dataset on AWS</h1>
            <h3>Introduction</h3>
            <p>Music is a universal language that transcends borders and cultures. <a href="https://open.spotify.com/" target="_blank">Spotify</a>
                one of the world's leading music streaming platforms, offers a treasure trove of data on the most globally famous songs. 
                Analyzing this data can provide valuable insights into music trends, artist popularity, and more. In this project, I'll walk 
                you through building an automated data pipeline on <a href="https://aws.amazon.com/" target="_blank">Amazon Web Services (AWS)</a>
                to <a href="https://aws.amazon.com/what-is/etl/#:~:text=Extract%2C%20transform%2C%20and%20load%20(,and%20machine%20learning%20(ML)." target="_blank">extract, transform, and load (ETL)</a>
                data from Spotify, empowering you to explore and analyze globally famous songs.</p>

            
        <div class="flex-container">
            <div class="text">
            <h3>Prerequisites</h3>
            <p>Before diving into the project, you'll need the following prerequisites:</p>
            <ul style="list-style-type: disc; margin-left: 30px;">
                <li>An <a href="https://signin.aws.amazon.com/" target="_blank"> AWS account</a> with the necessary permissions to create and manage services.</li>
                <li>Basic knowledge of AWS services like S3, Lambda, Glue, and Athena.</li>
                <li><a href="https://github.com/arunp77/Python-programming" target="_blank">Python programming skills</a>.</li>
                <li><a href="https://developer.spotify.com/documentation/web-api" target="_blank">Spotify API access</a>.</li>
            </ul>
            </div>
            <div class="image">
                <img src="assets/img/portfolio/AWS-services.png" alt="Image Description">
            </div>
        </div>
        <!--
        <p>Before diving into the project, you'll need the following prerequisites:</p>
        <ul style="list-style-type: disc; margin-left: 30px;">
            <li>An <a href="https://signin.aws.amazon.com/" target="_blank"> AWS account</a> with the necessary permissions to create and manage services.</li>
            <li>Basic knowledge of AWS services like S3, Lambda, Glue, and Athena.</li>
            <li><a href="https://github.com/arunp77/Python-programming" target="_blank">Python programming skills</a>.</li>
            <li><a href="https://developer.spotify.com/documentation/web-api" target="_blank">API access</a>.</li>
        </ul>-->
                
            <h3>Project overview</h3>
            <p>This project is an end-to-end ETL (Extract, Transform, Load) pipeline designed to automate the extraction of data from a Spotify playlist 
                and process it using various AWS services. The goal is to showcase proficiency in data engineering and AWS, with a focus on data extraction, 
                transformation, and automated processing.</p>
            <p>I have successfully completed the Spotify ETL pipeline project on AWS, and here's an overview of the key steps:</p>
            <ul style="list-style-type: disc; margin-left: 30px;">
                <li><b>Data Extraction:</b> AWS Lambda functions were employed to interact with the Spotify API, 
                    fetching data on globally famous songs, and storing it in an Amazon S3 bucket as a raw data.</li>
                <li><b>Data Transformation:</b> The raw Spotify data underwent a transformation phase through Lambda 
                    functions. This process included parsing JSON responses, creating structured data frames, and applying various data cleaning operations (such as finding duplicates,
                    infinities, missing values etc).</li>
                <li><b>Data Loading:</b> The transformed data was loaded back into the S3 bucket, making it ready for analysis.</li>
                <li><b>Automated Processing:</b> For keeping the music data current, I implemented automated processes that
                     actively fetched new data from Spotify and dynamically updated the datasets (I kept daily batch processing). An AWS S3 trigger was configured to initiate the data transformation 
                     process whenever a new dataset was added to the S3 bucket. Subsequently, the transformed data was loaded into a designated destination or folder.
                </li>
                <li><strong>Automation:</strong> AWS Crawler automates metadata discovery, cataloging schema and structure, and ensures up-to-date metadata in the AWS Glue Data Catalog.
                    AWS Glue simplifies ETL processing, dynamically transforms data frames, and seamlessly integrates with the Data Catalog for automated, serverless data transformation.</li>
                <li><b>Data Analysis:</b> With the data in its prepared formats, it's now possible to explore music trends, artist popularity, 
                    genre distribution, and more. I used Athena to do the data analytics. </li>
            </ul>
                
            <h3>Project Architecture</h3>
            <img src="assets/img/portfolio/AWS-2.png" alt="Project Architecture" style="width: 800px; height: auto;">

            <h3>Project Setup</h3>
                <h5>Spotify Playlist and API:</h5>
                <ul style="list-style-type: disc; margin-left: 30px;">
                    <li>Created a <a href="https://developer.spotify.com/documentation/web-api" target="_blank">Spotify API account</a>.</li>
                    <li>Obtain API keys and client credentials ('client_id' and 'client_secret' will be needed while extracting the data from Spotify).</li>
                    <li>Located the target Spotify playlist at <a href="https://open.spotify.com/playlist/37i9dQZEVXbNG2KDcFcKOF" target="_blank">Spotify Playlist Link</a>.</li>
                </ul>

                <h5>AWS Resources:</h5>
                <ul style="list-style-type: disc; margin-left: 20px;">
                    <li>Create an <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html" target="_blank">Amazon S3 bucket to store the data</a>
                    (you can follow the step by step guide provided at official AWS documnetation page)</li>
                    <li>Configure folder structures within the S3 bucket for raw and transformed data.</li>
                    <li>Set up necessary Lambda functions for data extraction and transformation.</li>
                    <li>In the project, I created follwoing folders in s3 bucket on AWS cloud:</li>
                    <ul>
                        <p><h6><strong>Amazon S3 Bucket:</strong></h6></p>
                        <ul style="list-style-type: disc; margin-left: 20px;">
                            <li>Set up an Amazon S3 bucket named "spotify-etl-project-arun."</li>
                            <li>This bucket contains all the data files obtained from the spotify API.</li>
                        </ul>
                        <p><h6><b>Folder Structure:</b></h6></p>
                        <p>Within the S3 bucket, established a structured folder hierarchy:</p>
                        <ul style="list-style-type: disc; margin-left: 20px;">
                            <li>raw_data/
                                <ul style="list-style-type: disc; margin-left: 10px;">
                                    <li>processed/</li>
                                    <li>to_processed/</li>
                                </ul>
                            </li>
                            <li>transformed_data/
                                <ul style="list-style-type: disc; margin-left: 10px;">
                                    <li>album_data/</li>
                                    <li>artist_data/</li>
                                    <li>songs_data/</li>
                                </ul>
                            </li>
                        </ul>
                        <img src="assets/img/portfolio/AWS-directory.png" alt="Project Architecture" style="width: 700px; height: auto;">
                    </ul>
                </ul>

            <h3>Key Components</h3>
                <h5>Data Extraction (AWS Lambda - "<span style="color: rgb(128, 20, 45);">spotify_api_data_extract</span>")</h5>
                <p>In this Lambda function, we extract data from the Spotify API and save it in the raw data folder within the S3 bucket. 
                    The function uses the Spotify API credentials to access song information.</p>
                    
                <img src="assets/img/portfolio/AWS-extract-load.png" alt="Project Architecture" style="width: 900px; height: auto;">

                <ul class="indented-list">
                    <li>Created an AWS Lambda function named "spotify_api_data_extract". Here is the lambda function:</li>
                    <pre>
                        <code class="language-python">
                            import json, os, spotify, boto3
                            from datetime import datetime
                            from spotipy.oauth2 import SpotifyClientCredentials
        
                            def lambda_handler(event, context):    
                                client_id = os.environ.get('client_id')
                                client_secret = os.environ.get('client_secret')
                                client_credentials_manager = SpotifyClientCredentials(client_id = client_id, client_secret = client_secret)
                                sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)
                                playlists = sp.user_playlists('spotify')
                                playlist_link = "https://open.spotify.com/playlist/37i9dQZEVXbNG2KDcFcKOF"
                                playlist_URI = playlist_link.split("/")[-1].split("?")[0]
                                spotify_data = sp.playlist_tracks(playlist_URI)                        
                                client = boto3.client('s3')
                                filename = "spotify_raw_" +str(datetime.now()) + ".json"
                                client.put_object(
                                    Bucket = "spotify-etl-project-arun",
                                    Key = "raw_data/to_processed/" + filename,
                                    Body = json.dumps(spotify_data)
                                    )                    
                        </code>
                    </pre>
                    <li>This function extracts data from the Spotify playlist and saves it in the "raw_data/to_processed/"" folder within the S3 bucket.</li>
                    <li>The extraction process is performed using the Spotify API.</li>
                    <li>You may have to create event at <a href="https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html" target="_blank">
                        AWS Cloudwatch</a> to trigger the functio to extract the data from the source (if any data file at the spotify is updated).</li>
                    <li><b>Code explanation:</b></li>
                        <ul>
                            <li><p><span style="color: blue;">json</span>: Used for working with JSON data,</p> 
                                <p><span style="color: blue;">os</span>: Used for accessing environment variables,  </p>
                                <p><span style="color: blue;">spotify</span>: The spotipy library, which is used for working with the Spotify Web API, </p>
                                <p><span style="color: blue;">boto3</span>: The AWS SDK for Python, used for interacting with AWS services.
                                <p><span style="color: blue;">datetime</span>: Used for generating a timestamp.</p>
                                </li>
                            <li><p><span style="color: blue;">lambda_handler</span>: function is the entry point for the Lambda function which takes arguments: 
                                'event' and 'context'. This function retrieve spotify API credentials from environment variables 'client_id' and 'client_secret' via 
                                'SpotifyClientCredentials' function. The 'client_id' and 'client_secret' are necessary for authenticating with the SpotifyWeb API 
                            and 'SpotifyClientCredentials' is used an access token for the Spotify API.</p> </li>
                            <li><p><span style="color: blue;">sp</span>: This object is used to interact witth the Spotify API, and created with the 'client_credentials_manager'</p> </li>
                            <li><p><span style="color: blue;">boto3</span>: The 'boto3' is used to create to interact with AWS s3 bucket,</p> </li>
                            <li>The 'client.put_object' function of the lambda function is used to dump extracted data from Spotify playlist to
                                the 'raw_data/to_processed' directory of the AWS s3 bucket (in my case "spotify-etl-project-arun" bucket). 
                                Here extracted data is first converted to JSON style (unstructured format) and then uploaded to the specified location </li>
                        </ul>
                </ul>

                <h5>Data Transformation and Loading (AWS Lambda - "<span style="color: rgb(128, 20, 45);">spotify_transformation_load_function</span>")</h5>
                <p>This Lambda function is responsible for the transformation of raw data. It parses JSON responses, converts them into 
                    dataframes, and applies data cleaning operations such as removing duplicates and converting data types.</p>
                
                    <img src="assets/img/portfolio/AWS-trasnform-load.png" alt="Project Architecture" style="width: 900px; height: auto;">

                <ul class="indented-list">
                    <li>Developed an AWS Lambda function named spotify_transformation_load_function". Here is the lambda function which:</li>
                    <pre>
                        <code class="language-python">
                            import json, boto3, 
                            import pandas as pd
                            from datetime import datetime
                            from io import StringIO

                            def album(data):
                                album_list = []
                                for row in data['items']:
                                    album_id = row['track']['album']['id']
                                    album_name = row['track']['album']['name']
                                    album_release_date = row['track']['album']['release_date']
                                    album_total_tracks = row['track']['album']['total_tracks']
                                    album_url = row['track']['album']['external_urls']['spotify']
                                    album_element = {
                                        'album_id' : album_id, 
                                        'album_name' : album_name, 
                                        'release_date': album_release_date, 
                                        'total_tracks': album_total_tracks, 
                                        'url': album_url
                                    }
                                    album_list.append(album_element)
                                return album_list

                            def artist(data):
                                artist_list = []
                                for row in data['items']:
                                    for key, value in row.items():
                                        if key =='track':
                                            for artist in value['artists']:
                                                artist_dict = {
                                                    'artist_id': artist['id'], 
                                                    'artist_name' : artist['name'], 
                                                    'external_url' : artist['href']
                                                }
                                                artist_list.append(artist_dict)
                                return artist_list

                            def songs(data):
                                song_list = []
                                for row in data['items']:
                                    song_id = row['track']['id']
                                    song_name = row['track']['name']
                                    song_duration = row['track']['duration_ms']
                                    song_url = row['track']['external_urls']['spotify']
                                    song_popularity = row['track']['popularity']
                                    song_added = row['added_at']
                                    album_id = row['track']['album']['id']
                                    artist_id = row['track']['album']['artists'][0]['id']
                                    song_element = {
                                        'song_id' : song_id,
                                        'song_name' : song_name, 
                                        'duration_ms' : song_duration, 
                                        'url' : song_url,
                                        'popularity': song_popularity, 
                                        'song_added' : song_added, 
                                        'album_id': album_id, 
                                        'artist_id' : artist_id
                                    }
                                    song_list.append(song_element)
                                return song_list

                            '''
                            Lambda functions typically receive an event as input, which can be used to trigger the function.
                            '''
                            def lambda_handler(event, context): 
                                '''
                                s3 = boto3.client('s3') = Create an S3 Client. The S3 client is used to interact with Amazon S3, 
                                allowing you to list objects, upload/download files, and perform various 
                                other S3 operations.
                                '''
                                s3 = boto3.client('s3') 
                                Bucket = "spotify-etl-project-arun"  
                                Key = "raw_data/to_processed/"
                                
                                spotify_data = []
                                spotify_keys = []
                                song_list = []  # Define song_list here
                                for file in s3.list_objects(Bucket=Bucket, Prefix=Key)['Contents']:
                                    file_key = file['Key']
                                    if file_key.split('.')[-1] == "json":
                                        response = s3.get_object(Bucket=Bucket, Key=file_key)
                                        content = response['Body']
                                        jsonObject = json.loads(content.read())
                                        spotify_data.append(jsonObject)
                                        spotify_keys.append(file_key)
                                        
                                for data in spotify_data:
                                    album_list = album(data)
                                    artist_list = artist(data)
                                    songs_list = songs(data)
                                    song_list.extend(songs_list)
                                    
                                    print(album_list)
                                    '''
                                    print(album_list)
                                    Upto this, point we can deploy and test the processes already done for any error.
                                    Just use print(album_list) at the end of above and then deploy and test
                                    '''
                                    
                                    album_df = pd.DataFrame.from_dict(album_list)
                                    album_df = album_df.drop_duplicates(subset=['album_id'])

                                    
                                    artist_df = pd.DataFrame.from_dict(artist_list)
                                    artist_df = artist_df.drop_duplicates(subset=['artist_id'])

                                    
                                    song_df = pd.DataFrame.from_dict(song_list)
                                    
                                    album_df['release_date'] = pd.to_datetime(album_df['release_date'])
                                    song_df['song_added'] = pd.to_datetime(song_df['song_added'])
                                    
                                    songs_key = "transformed_data/songs_data/songs_transformed_" + str(datetime.now()) + ".csv"
                                    song_buffer = StringIO()
                                    song_df.to_csv(song_buffer, index=False)
                                    song_content = song_buffer.getvalue()
                                    s3.put_object(Bucket=Bucket, Key=songs_key, Body=song_content)
                                    
                                    album_key = "transformed_data/album_data/album_transformed_" + str(datetime.now()) + ".csv"
                                    album_buffer = StringIO()
                                    album_df.to_csv(album_buffer, index=False)
                                    album_content = album_buffer.getvalue()
                                    s3.put_object(Bucket=Bucket, Key=album_key, Body=album_content)
                                    
                                    artist_key = "transformed_data/artist_data/artist_transformed_" + str(datetime.now()) + ".csv"
                                    artist_buffer = StringIO()
                                    artist_df.to_csv(artist_buffer, index=False)
                                    artist_content = artist_buffer.getvalue()
                                    s3.put_object(Bucket=Bucket, Key=artist_key, Body=artist_content)
                                    
                                s3_resource = boto3.resource('s3')
                                for key in spotify_keys:
                                    copy_source = {
                                        'Bucket': Bucket,
                                        'Key': key
                                    }
                                    destination_key = 'raw_data/processed/' + key.split("to_processed/")[1] 
                                    s3_resource.meta.client.copy(copy_source, Bucket, destination_key)
                                    s3_resource.Object(Bucket, key).delete()
                        </code>
                        </pre>
                        <li>This function handles the transformation and loading of data into distinct folders:</li>
                        <ul>
                            <li>album_data/</li>
                            <li>artist_data/</li>
                            <li>songs_data/</li>
                        </ul>
                        <li>The data transformation involves:</li>
                            <ul>
                                <li>Parsing the raw JSON data.</li>
                                <li>Creating separate functions for albums, artists, and songs.</li>
                                <li>Using these functions to transform the data and convert it into dataframes.</li>
                                <li>Applying data cleaning operations such as dropping duplicates and converting data types (e.g., datetime).</li>
                                <li>Saving the transformed data as CSV files in their respective folders.</li>
                            </ul>
                        <li><b>Code explanation:</b></li>
                        <ul>
                            <li>The code can be divided into </li>
                            <ul>
                                <li>Loading important libraries</li>
                                <li>Creating functions like 'album', 'artist' and 'song's function to extract particular object items and then appending them
                                    as a list object. </li>
                                <li>Next the lambda function 'lambda_handler(event, context)' are used to do following things:</li>
                                    <ul>
                                        <li><span style="color: blue;">s3 = boto3.client('s3')</span> = Create an S3 Client. The S3 client is used to interact with Amazon S3, 
                                            allowing you to list objects, upload/download files, and perform various 
                                            other S3 operations, like trasforming the datasets to a dataframe using pandas, making right datatypes of certain 
                                        items, removing or updating any missing or NaN values.</li>
                                        <li>And in the end, the filtered, and trasnformed datsets are saved as '.csv' file in the AWS s3 bucket 
                                            (in my case, bucket name is spotify-etl-project-arun) and in the 'raw_data/processed/' folder. </li>
                                            <li>Each time a new data comes to the 'raw_data/to_processed/' folder, the trigger function in the 
                                                "spotify_transformation_load_function" function start reporcessing the data and saving again it to the processed folder and deleting 
                                            any existing older file in the the 'to_processed' folder.</li>
                                    </ul>
                            </ul>
                        </ul>
                </ul>
                
                <h5>Automation and Data Update</h5>
                <ul class="indented-list">
                    <li>To ensure data is kept up-to-date, an automated process is implemented.</li>
                    <li>One of the key goals of our ETL project is to ensure that the entire process, from data extraction to transformation, 
                        is as automated as possible. This automation not only saves time but also ensures that our datasets are constantly updated 
                        with the latest information.</li>
                    <li>New data is fetched from the Spotify playlist whenever the playlist is updated.</li>
                    <li><p><b>Trigger-Based Extraction:</b> In the extraction stage, we've implemented a trigger-based mechanism. Specifically, for each day, 
                        we've set up a trigger for our extraction Lambda function. This means that our data extraction process runs automatically on a daily 
                        basis, fetching the most current data from Spotify.</p>
                        <ul>
                            <li>The trigger becomes active daily, ensuring that no data update is missed. As soon as the trigger for the extraction Lambda 
                                function is activated, it initiates the data extraction process, fetching the latest information from the Spotify API. 
                                This data is then saved in the S3 bucket under the raw_data/to_processed/ folder.</li>
                        </ul>
                    </li>
                    <li><p>
                        <b>Seamless Transformation:</b> 
                        What sets our ETL pipeline apart is the seamless transition from data extraction to transformation. The moment new data is extracted, 
                        the trigger for the extraction function activates the transformation Lambda function. This immediate transition ensures that our datasets
                        are always up-to-date.
                        <ul>
                            <li>Our transformation function, driven by the trigger, takes the raw data and performs the necessary data manipulation and cleaning. 
                                It parses the raw JSON responses, creates structured dataframes for albums, artists, and songs, and applies data cleaning operations.</li>
                            <li>The transformed data is then neatly organized and saved in the "transformed_data/"" folder within the S3 bucket. The data is categorized 
                                into subfolders for "album_data/", "artist_data/", and "songs_data/", making it easy to access and analyze.</li>
                        </ul>
                    </p></li>
                    <li>
                        <p>
                            <b>Benefits of Automation:</b>
                            The automation of the extraction and transformation processes in our ETL pipeline offers several benefits:
                            <ul>
                                <li><b>Timely Updates:</b> Data is extracted and transformed daily, ensuring that our datasets are consistently up-to-date. 
                                    This means you're always working with the latest information.</li>
                                <li><b>Efficiency:</b> Automation reduces manual intervention and minimizes the chance of human error in the process. It's a set-it-and-forget-it approach that frees up valuable time.</li>
                                <li><b>Consistency:</b> The automation process ensures that data is consistently structured and cleaned, which is crucial for data quality and analysis.</li>
                                <li><b>Real-Time Insights:</b> The data is ready for analysis as soon as it's transformed, allowing for real-time insights and quick decision-making.</li>
                                <li></li>
                            </ul>
                            By automating our ETL pipeline, we've created a robust and efficient system for data extraction, transformation, and analysis, making it easier than ever to explore the globally famous songs dataset from Spotify.
                        </p>
                    </li>
                </ul>

            <h3>Project Results</h3>
                <ul class="indented-list">
                    <li><strong>Data Extraction:</strong> The Spotify ETL pipeline successfully extracted data from the target Spotify playlist, which included [number] songs. The data was sourced from [Spotify playlist URL].</li>
                    <li><strong>Data Transformation:</strong> The raw data was meticulously transformed, including JSON to data frame conversion, data cleaning, and data type adjustments. This transformation process resulted in clean and structured data for further analysis.</li>
                    <li><strong>Data Loading:</strong> Processed data was efficiently loaded back into AWS, utilizing the S3 bucket named [Bucket Name] with a structured folder hierarchy.</li>
                    <li><strong>Automation:</strong> Our ETL pipeline is fully automated, ensuring that any updates to the playlist are processed seamlessly without manual intervention.</li>
                    <li><strong>Performance:</strong> The pipeline demonstrates remarkable performance, with rapid processing of new data and the potential for scaling to handle larger datasets.</li>
                    <li><strong>AWS Services:</strong> AWS Glue served as the cornerstone of data cataloging and ETL, while AWS Athena facilitated querying and data analysis.</li>
                    <li><strong>Data Catalog:</strong> The AWS Glue data catalog has significantly improved our ability to discover and manage data, streamlining the entire process.</li>
                    <li><strong>Queries and Analytics:</strong> We ran a series of insightful queries using Athena, uncovering valuable insights about the playlist and the songs it contains. For example, [example query and finding].</li>
                    <li><strong>Visualization:</strong> We harnessed visualization tools like Tableau to create compelling charts and graphs, enhancing our understanding of the data. [Include a sample visualization].</li>
                    <li><strong>Challenges Overcome:</strong> Throughout the project, we encountered challenges such as [mention challenges], but our team's determination and problem-solving skills ensured successful outcomes.</li>
                    <li><strong>Future Improvements:</strong> We envision further improvements, including [mention future enhancements], to make the pipeline even more robust and capable of handling larger datasets.</li>
                    <li><strong>Data Security and Compliance:</strong> Our project adheres to AWS best practices and security measures to ensure data integrity and compliance.</li>
                    <li><strong>Feedback and Contributions:</strong> We welcome feedback, recommendations, and contributions from the community. If you're interested in collaborating or have suggestions, feel free to reach out to us.</li>
                </ul>
                <ul>
                    <p>This section highlights the key achievements and outcomes of our Spotify ETL project on AWS, showcasing the power of data engineering and AWS services in action.</p>
                    <p>Insights and observations obtained from the data, such as statistics, patterns, or visualizations will be written once the project is complete.</p>
                </ul>
                
            <h3>Challenges Faced</h3> 
                <div>
                <p>While working on this Spotify ETL project, I encountered several challenges that required creative problem-solving and continuous 
                    learning. Some of the main challenges included:</p>
                    <ul>
                        <h5>1. Spotify API Integration</h5>
                        <p><strong>Challenge:</strong> Integrating the Spotify API to access and retrieve data from the playlist was a learning curve. 
                            It required understanding the authentication mechanisms and handling rate limits. Navigating the intricacies of a third-party 
                            API was a challenge in itself.</p>
                        <p><strong>Solution:</strong> I conducted in-depth research, read through the <a href="https://developer.spotify.com/documentation/web-api" target="_blank"> API documentation</a>, and experimented with various 
                            authentication methods. Additionally, I implemented error handling to gracefully manage rate limits and ensure the data extraction process runs smoothly.</p>
                    </ul>
                    <ul>
                        <h5>2. Data Transformation</h5>
                        <p><strong>Challenge:</strong> Transforming the raw data into structured formats involved complex data manipulation. This included 
                            dealing with nested 
                            <a href="https://javaee.github.io/tutorial/jsonp001.html#:~:text=JSON%20defines%20only%20two%20data,true%2C%20false%2C%20and%20null" target="_blank">JSON 
                                data structure</a> and converting them into usable data frames using <a href="https://pandas.pydata.org/" target="_blank">Pandas library of the python</a>.
                                Ensuring that the transformed data was accurate and error-free presented its own set of challenges (more about the databases and 
                                data structure can be obtained in <a href="https://github.com/arunp77/Database-datapipeline-ETL/tree/main/Database" target="_blank">
                                    my Github repository</a>).</p>
                        <p><strong>Solution:</strong> To tackle this, I created separate functions for parsing albums, artists, and songs. These functions 
                            efficiently transformed the data and converted it into data frames, making it more manageable. Additionally, I implemented data 
                            cleaning operations to handle issues like duplicate entries and data type conversions. 
                            In the end each files were saved in the respective data folders of the 
                            <a href="https://aws.amazon.com/s3/" target="_blank"> AWS s3 bucket</a>.</p>
                    </ul>
                    <ul>
                        <h5>3. Automating ETL Process</h5>
                        <p><strong>Challenge:</strong> Designing an automated ETL process that continuously updates the data from Spotify to the S3 bucket 
                            and triggers transformations posed a unique challenge. Ensuring that the process is both efficient and reliable was a significant task.</p>
                        <p><strong>Solution:</strong> I implemented a trigger-based mechanism for the extraction function, which activates the transformation 
                            function as soon as new data is extracted. This ensures that the dataset is always up-to-date without manual intervention.</p>
                            <ul>
                                <b>Step-by-step guide to creating a trigger to automate your ETL process:</b>
                                <h6><b>Step 1: Set Up the Trigger</b></h6>
                                <ul>
                                    <li>Go to the AWS Management Console and navigate to the Amazon CloudWatch service.</li>
                                    <li>In the CloudWatch dashboard, choose "Rules" from the left-hand navigation pane.</li>
                                    <li>Click "Create Rule" to define a new rule that will trigger your Lambda function.</li>
                                </ul>
                                    
                                <h6><b>Step 2: Define the Trigger Rule</b></h6>
                                <ul>
                                    <li>In the "Create Rule" page, choose an event source. For a time-based trigger, you can select "Event Source" as "Schedule."</li>
                                    <li>Specify the schedule for your ETL process. You can use cron expressions to set the exact time and frequency (e.g., daily at midnight).</li>
                                    <li>Under "Targets," choose "<a href="https://aws.amazon.com/lambda/?p=pm&c=la&z=4" target="_blank">lambda function</a>" 
                                        and select the Lambda function you created</li>
                                    <li>Configure any other settings or conditions for your trigger, such as error handling or retries.</li>
                                    <li>You may find some error regarding the "Timed out" while running lambda function. You may need to increase the 
                                        timeout for your Lambda function. You can do this by adjusting the "Timeout" setting in your Lambda configuration. 
                                        Make sure it allows enough time for your function to complete its tasks.</li>
                                    <li> If you're dealing with a large amount of data, consider processing it in smaller batches. 
                                        This can help distribute the workload and prevent timeouts.</li>
                                    <li>You can configure your Lambda function to run concurrently. This can be helpful if you have multiple items to 
                                        process and want to speed up the execution.</li>
                                    <li>Depending on your use case, you might consider using AWS Step Functions or SNS to trigger the Lambda function asynchronously, 
                                        allowing it to continue processing after the initial function call returns.</li>
                                </ul>
                            </ul>
                    </ul>
                    <ul>
                        <h5>4. Data Cleaning</h5>
                        <ul>
                            <li><strong>Challenge:</strong> Managing and cleaning the data to remove duplicates, handle missing values, and ensure data consistency 
                                required careful attention. Any inconsistencies in the data could lead to inaccuracies in our analyses.</li>
                            <li><strong>Solution:</strong> I developed robust data cleaning procedures that addressed issues like duplicate records, missing values, 
                                and data type discrepancies. This ensured that the data remained accurate and reliable throughout the ETL process.</li>
                                <pre>
                                    <code class="language-python">
                                        album_df = pd.DataFrame.from_dict(album_list)
                                        album_df = album_df.drop_duplicates(subset=['album_id'])

                                        
                                        artist_df = pd.DataFrame.from_dict(artist_list)
                                        artist_df = artist_df.drop_duplicates(subset=['artist_id'])

                                        
                                        song_df = pd.DataFrame.from_dict(song_list)
                                        
                                        album_df['release_date'] = pd.to_datetime(album_df['release_date'])
                                        song_df['song_added'] = pd.to_datetime(song_df['song_added'])
                                    </code>
                                    </pre>
                        </ul>
                    </ul>
                    <ul>
                        <h5>5. Error Handling</h5>
                        <ul>
                            <li><strong>Challenge:</strong> Building robust error handling mechanisms to gracefully manage failures and ensure the pipeline 
                                runs smoothly was a critical aspect of the project. Failures could occur at various stages, and handling them effectively 
                                was essential.</li>
                            <li><strong>Solution:</strong> I implemented thorough error handling throughout the project, with detailed logging and notification 
                                systems to alert me to any issues. This proactive approach allowed me to address problems promptly and keep the ETL pipeline 
                                running smoothly.</li>
                        </ul>
                    </ul>
                    <p>Overcoming these challenges not only improved my technical skills but also enhanced my problem-solving abilities. It's essential 
                        to acknowledge that facing and conquering these hurdles was an integral part of the learning process. These experiences have 
                        reinforced the importance of adaptability, persistence, and continuous learning in the world of data engineering and ETL processes.</p>
                </div>

            <h3>Future Improvements</h3>
                <p>My Spotify ETL project is continually evolving, and I have some exciting plans to enhance its capabilities. Here are some areas I am 
                    focusing on for future improvements:</p>
                <ul class="indented-list">
                    <li><strong>Real-time Data Updates:</strong> My aim  is to implement real-time data extraction to ensure the dataset is always up-to-date, syncing with 
                        changes in the Spotify playlist as they occur.</li>
                    <li><strong>Efficient Parallel Processing:</strong> I am also working on enabling parallel processing to efficiently handle larger datasets, improving 
                        my project's scalability.</li>
                    <li><strong>Data Quality Assurance:</strong> To maintain data integrity, I am planning to implement data quality checks and monitoring to swiftly 
                        identify and address any issues.</li>
                    <li><strong>Streaming Analytics:</strong> Exploring streaming data platforms will provide us with real-time insights and opportunities for immediate 
                        actions.</li>
                    <li><strong>Advanced Analytics:</strong> Expanding the project's capabilities to include features like recommendation engines and sentiment analysis 
                        will offer richer data insights.</li>
                    <li><strong>User-Friendly Dashboard:</strong> I am also considering the development of an intuitive dashboard for data visualization, making my project's 
                        more user-friendly.</li>
                    <li><strong>Community Collaboration:</strong> I welcome and encourage contributions from the community to foster collaborative improvements and new 
                        ideas.</li> 
                    <li><strong>Documentation and Tutorials:</strong> I am committed to providing comprehensive project documentation and tutorials, making it easier 
                        for others and contributors to understand and engage with my project.</li> 
                </ul>
        </div>
      </div>
    </section><!-- End Portfolio Details Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
      </div>
    </div>
  </footer><!-- End  Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/typed.js/typed.umd.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>