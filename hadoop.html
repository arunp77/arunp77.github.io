<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Hadoop</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

    <!-- ======= Header ======= -->
    <header id="header">
    <div class="d-flex flex-column">
        <div class="profile">
            <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
            <h1 class="text-light"><a href="index.html">Arun</a></h1>
            <div class="social-links mt-3 text-center">
                <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
                <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
                <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
                <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
            </div>
        </div>

        <nav id="navbar" class="nav-menu navbar">
            <ul>
                <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
                <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
                <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
                <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
                <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
                <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
                <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
                <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
                <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
                <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
            </ul>
        </nav><!-- .nav-menu -->
    </div>
    </header><!-- End Header -->

    <main id="main">
        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs"> 
          <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
              <h2>Data Engineering</h2>
              <ol>
                <li><a href="Data-engineering.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
              </ol>
            </div>
    
          </div>
        </section><!-- End Breadcrumbs -->

        <!------  right dropdown menue ------->
        <div class="right-side-list">
            <div class="dropdown">
                <button class="dropbtn"><strong>Shortcuts:</strong></button>
                <div class="dropdown-content">
                    <ul>
                        <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                        <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                        <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                        <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                        <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                        <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                        <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                        <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                        <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                        <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                        <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                        <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                        <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                            <!-- Add more subsections as needed -->
                    </ul>
                </div>
            </div>
        </div>

        <!-- ======= Portfolio Details Section ======= -->
        <section id="portfolio-details" class="portfolio-details">
        <div class="container">
            <div class="row gy-4">
                <h1>Hadoop: A Comprehensive Overview of Big Data Processing and Storage</h1>
                <div class="col-lg-8">
                    <div class="portfolio-details-slider swiper">
                        <div class="swiper-wrapper align-items-center"> 
                            <figure>
                            <img src="assets/img/data-engineering/apache-ecosystem.png" alt="" style="max-width: 90%; max-height: auto;">
                            <figcaption style="text-align: center;">Apache ecosystem (<strong>Image credit:</strong><a href="https://www.oreilly.com/library/view/apache-hive-essentials/9781788995092/e846ea02-6894-45c9-983a-03875076bb5b.xhtml" target="_blank"> O'REILLY</a> )</figcaption> 
                            </figure>
                        </div>
                        <div class="swiper-pagination"></div>
                    </div>
                </div>

                <div class="col-lg-4 grey-box">
                    <div class="section-title">
                        <h3>Table of Contents</h3>
                        <ol>
                            <li><a href="#introduction">Introduction</a></li>
                            <li><a href="#key-concept"><a href="https://spark.apache.org/" target="_blanck">Apache Spark™</a> key concept</a></li>
                            <ul>
                                <li><a href="#key-features">Key Features</a></li>
                                <li><a href="#Components">Components of <a href="https://spark.apache.org/" target="_blanck">Apache Spark™</a></a></li>
                                <li><a href="#limitations">Limitations of PCA</a></li>
                                <li><a href="#use-cases">Use cases</a></li>
                                <li><a href="#principal-compoenent">What Are Principal Components?</a></li>
                            </ul>
                            <li><a href="#how-pca-works">How PCA works?</a></li>
                            <ul>
                                <li><a href="#Process">Process of doing the PCA</a></li>
                            </ul>
                            <li><a href="#example-1">Example</a></li>
                            <li><a href="#PCA-in-machine-learning">Application of PCA in machine learning</a></li>
                            <li><a href="#reference">Reference</a></li>
                        </ol>
                    </div>
                </div>
            </div>

            <section>
            <h3 id="introduction">Introduction to Hadoop</h3>
            In a straightforward scenario with smaller data volumes, one might consider a system where users interact 
            directly with standard database servers through a centralized system. However, in the case of large and scalable data, processing 
            such data through a single database bottleneck becomes a challenging task. Hadoop is a framework written in Java that utilizes a 
            large cluster of commodity hardware to maintain and store big size data. Hadoop works on MapReduce Programming Algorithm that was 
            introduced by Google. This Algorithm divides the task into small parts and assign them to many computers, and collects the results 
            from them which when integrated, from the result dataset.
            <figure>
                <img src="assets/img/data-engineering/hadoop-mapreduce.png" alt="" style="max-width: 80%; max-height: auto;">
                <figcaption style="text-align: center;"></figcaption>
            </figure>
            Using the solution provided by Google, Doug Cutting and his team developed an Open Source Project called HADOOP. 
            Hadoop runs applications using the MapReduce algorithm, where the data is processed in parallel with others. 
            Today lots of Big Brand Companies are using Hadoop in their Organization to deal with big data, eg. Facebook, Yahoo, Netflix, eBay, etc. 
            <p><strong>Definition: </strong>Hadoop, an Apache open-source framework written in Java, enables the distributed processing of large 
                datasets across computer clusters using simple programming models. Operating in an environment with distributed storage and 
                computation across clusters, Hadoop is designed to scale seamlessly from a single server to thousands of machines, each providing 
                local computation and storage.</p>

            <p><strong>Example: </strong>For example, consider a dataset of 5 terabytes. If you distribute the processing across a Hadoop cluster of 
                10,000 servers, each server would need to process approximately 500 megabytes of data. This allows the entire dataset to be processed much faster than traditional sequential processing.</p>

            <p>Hadoop has two major layers namely -- 
                <ul>
                    <li>Processing/Computation layer (MapReduce)</li>
                    <li>Storage layer (Hadoop Distributed File System).</li>
                </ul>
                    The Hadoop Architecture Mainly consists of 4 components. </p>
            <ul>
                <li>MapReduce</li>
                <li>HDFS(Hadoop Distributed File System)</li>
                <li><a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank">YARN</a>(Yet Another Resource Negotiator)</li>
                <li>Common Utilities or Hadoop Common</li>
            </ul>
            <figure>
                <img src="assets/img/data-engineering/hadoop-1.webp" alt="" style="max-width: 80%; max-height: auto;">
                <figcaption style="text-align: center;"><strong>Image credit: </strong><a href="https://www.talend.com/resources/what-is-mapreduce/#:~:text=achieving%20quicker%20results.-,What%20is%20MapReduce%3F,functioning%20of%20the%20Hadoop%20framework." target="_blanck">Talend</a> </figcaption>
            </figure>

            <h4>Advantegs of Hadoop</h4>
            <ul>
                <li><strong>Scalability: </strong>Hadoop is highly scalable, allowing for the easy addition of nodes to a cluster to handle increased data volume and processing requirements.</li>
                <li><strong>Cost-Effective:</strong>Hadoop can store and process large volumes of data on commodity hardware, making it a cost-effective solution compared to traditional database systems.</li>
                <li><strong>Parallel Processing: </strong>The MapReduce programming model enables parallel processing, distributing data across multiple nodes for faster and more efficient computation.</li>
                <li><strong>Fault Tolerance: </strong>Hadoop is designed to be fault-tolerant. If a node fails, data can be easily recovered as it exists in multiple copies across the cluster.</li>
                <li><strong>Flexibility: </strong>Hadoop can handle structured and unstructured data, making it suitable for a variety of data types, including text, images, and videos.</li>
                <li><strong>Open Source: </strong>Hadoop is an open-source framework, which means it is accessible and customizable. The community support and continuous development contribute to its robustness.</li>
                <li><strong>Ecosystem: </strong>The Hadoop ecosystem includes a variety of tools and frameworks (like Hive, Pig, Spark) that extend its functionality for different use cases, such as data warehousing, machine learning, and real-time processing.</li>
            </ul>

            <h4>Disadvantages of Hadoop</h4>
            <ul>
                <li><strong>Complexity: </strong>Implementing and managing a Hadoop cluster can be complex. It requires expertise in system administration, configuration, and tuning for optimal performance.</li>
                <li><strong>Programming Model: </strong>Developing applications in the MapReduce programming model may be challenging for those unfamiliar with it. However, higher-level abstractions like Apache Spark have addressed some of these concerns.</li>
                <li><strong>Real-time Processing: </strong>Hadoop's traditional batch processing model may not be suitable for real-time or low-latency processing requirements. Other tools like Apache Spark or Flink are better suited for these scenarios.</li>
                <li><strong>Hardware Dependency: </strong>While Hadoop can run on commodity hardware, optimal performance often requires a dedicated cluster of powerful machines, which may increase infrastructure costs.</li>
                <li><strong>Data Security: </strong>Hadoop's security features have evolved, but ensuring robust data security and access control can still be a concern, especially in large and complex deployments.</li>
                <li><strong>Data Locality: </strong>While Hadoop aims for data locality (processing data on the node where it's stored), achieving perfect data locality is not always possible, impacting performance.</li>
                <li><strong>Learning Curve: </strong>There is a learning curve associated with understanding and effectively using Hadoop and its associated tools, which may require additional training for teams unfamiliar with the ecosystem.</li>
            </ul>
            (For Apache spark see the <a href="pyspark.html">Link</a>.)

            <br><br>
            <!----------------------->
            <h3 id="mapreduce">MapReduce</h3>
            <a href="https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html" target="_blank">MapReduce</a> is a parallel programming model for writing distributed applications devised at Google for efficient processing of 
            large amounts of data (multi-terabyte data-sets), on large clusters (thousands of nodes) of commodity hardware in a reliable, 
            fault-tolerant manner. The significance of <a href="https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html" target="_blank">MapReduce</a> is to facilitate concurrent data processing. To achieve this, massive 
            volumes of data, often in the order of several petabytes, are divided into smaller chunks. These data chunks are processed 
            in parallel on Hadoop servers. After processing, the data from multiple servers is aggregated to provide a consolidated 
            result to the application. Hadoop is capable of executing <a href="https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html" target="_blank">MapReduce</a> programs written in various languages, including Java, Ruby, Python, C++, etc.
            Data access and storage are disk-based. The inputs are stored as files containing structured, semi-structured, or unstructured data. 
            The output is also stored as files.
            <figure>
                <img src="assets/img/data-engineering/Hadoop-Cluster-Schema.png" alt="" style="max-width: 70%; max-height: auto;">
                <figcaption style="text-align: center;">Mapreduce enables the analysis of massive volumes of Big Data through parallel processing. Discover 
                    everything you need to know: overview, functioning, alternatives, benefits, training...
                    (<strong>Image credit: </strong><a href="https://datascientest.com/en/mapreduce-how-to-use-it-for-big-data" target="_blanck">datascientest.com</a>)</figcaption>
            </figure>
            <p><strong>How does MapReduce work?</strong></p> The operation of MapReduce primarily revolves around two functions: Map and Reduce. 
            To put it simply, Map is used to break down and map the data, while Reduce combines and reduces the data. 
            These functions are executed sequentially. The servers running the Map and Reduce functions are referred to as Mappers and Reducers. 
            However, they can be the same servers.
            <ol>
            <li><strong>Map Phase: </strong>Input data is divided into chunks, and each chunk is processed independently by a set of mappers. 
                Mappers apply a user-defined function (map function) to transform the input data into a set of key-value pairs.</li>
            <li><strong>Shuffle and Sort Phase: </strong>The framework redistributes the output of the mappers based on the keys to 
                group related data together. This phase ensures that all data with the same key is sent to the same reducer.</li>
            <li><strong>Reduce Phase: </strong>Reducers receive the grouped data and apply a user-defined function (reduce function) to produce the final output. 
                The reduce function typically aggregates, filters, or performs some computation on the grouped data.</li>
            <li><strong>Output: </strong>The final results from the reducers are collected as the output of the MapReduce job.</li>
            </ol>
            MapReduce allows for the efficient processing of large datasets by distributing the workload across multiple nodes in a cluster. It's particularly 
            suitable for tasks that can be divided into independent subproblems, making it a powerful tool for big data analytics and processing.

            <br><br>
            <div class="grey-box">
                Filesystems that manage the storage across a network of machines are called distributed file systems.
            </div>
            <br><br>
            <!------------------------->
            <h3>HDFS (Hadoop Distributed File System)</h3>
            Hadoop Distributed File System (HDFS) is the storage component of Hadoop. All data stored on Hadoop is stored in a distributed manner across a cluster of machines. 
            The Hadoop Distributed File System (HDFS) is based on the Google File System (GFS) and provides a distributed file system that is designed to run on commodity hardware. 
            It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. 
            It is highly fault-tolerant and is designed to be deployed on low-cost hardware. It provides high throughput access to application data and is suitable for applications having large datasets.
            <br> 

            <h5>What are the components of the Hadoop Distributed File System(HDFS)?</h5>
            HDFS has two main components, broadly speaking, – data blocks and nodes storing those data blocks.
            <ul>
                <li><strong>HDFS Blocks: </strong>HDFS breaks down a file into smaller units. Each of these units is stored on different machines in the cluster. This, however, is transparent to the user working on HDFS. 
                    To them, it seems like storing all the data onto a single machine. These smaller units are the blocks in HDFS. The size of each of these blocks is 128MB by default, you can easily change it according to requirement.
                    There are several perks to storing data in blocks rather than saving the complete file.
                    <ul>
                        <li>The file itself would be too large to store on any single disk alone. Therefore, it is prudent to spread it across different machines on the cluster.</li>
                        <li>It would also enable a proper spread of the workload and prevent the choke of a single machine by taking advantage of parallelism.</li>
                    </ul>
                </li>
                <li><strong></strong></li>
                <li><strong></strong></li>
                <li><strong></strong></li>
                <li><strong></strong></li>
            </ul>


            <br><br>
            <div class="box">
                <h3>Key concepts</h3> 
                <p><strong>Hadoop Cluster?: </strong> A Hadoop cluster is a network of computers or nodes working together to handle large datasets efficiently. 
                Think of it as a team of specialists, each with their own expertise, tackling a massive project collaboratively. In this analogy, 
                the computers are the specialists, and the data is the project.</p>
                
                <hr>
                <p><strong>Hadoop daemons: </strong>
                    In the world of Hadoop, daemons are like the tireless workers behind the scenes, ensuring everything runs smoothly and efficiently. Each daemon has a 
                    specific role in managing data storage, processing, and resource allocation across the cluster. The core hadoop daemons are:</p>
                    <ol>
                        <li><strong>NameNode: </strong>Think of it as: The central authority figure, keeping track of all the files and their locations across the cluster. Their main responsibilities are:
                            <ul>
                                <li>Maintains the HDFS namespace (file system directory structure)</li>
                                <li>Manages file system operations like creating, renaming, and deleting files</li>
                                <li>Provides information about file locations to DataNodes</li>
                            </ul>
                        </li>
                        <li><strong>DataNode: </strong>Think of it as: The storage worker, tirelessly storing and retrieving data blocks on its local disk. Main responsibilities are:
                            <ul>
                                <li>Stores data blocks assigned by the NameNode</li>
                                <li>Replicates data blocks across other DataNodes for fault tolerance</li>
                                <li>Retrieves data blocks requested by MapReduce jobs or applications</li>
                            </ul>
                        </li>
                        <li><strong> ResourceManager: </strong>Think of it as: The traffic controller, managing resources (CPU, memory) and scheduling tasks across the cluster. Responsibilities are:
                            <ul>
                                <li>Receives job submissions from clients</li>
                                <li>Negotiates resource allocation with NodeManagers</li>
                                <li>Monitors the progress of running jobs</li>
                            </ul>
                        </li>
                        <li><strong>NodeManager: </strong> Think of it as: The local supervisor on each worker node, overseeing tasks and resource allocation. Main responsibilities are as follows:
                            <ul>
                                <li>Starts and monitors tasks assigned by the ResourceManager</li>
                                <li>Manages resources on the local node (CPU, memory, network)</li>
                                <li>Reports resource usage and task progress to the ResourceManager</li>
                            </ul>
                        </li>
                        <li><strong>Secondary NameNode: </strong> Think of it as: The NameNode's backup, ensuring continuity in case of a failure. Responsibilities are:
                            <ul>
                                <li>Periodically synchronizes metadata with the NameNode</li>
                                <li>Can be used to take over if the NameNode fails, minimizing downtime</li>
                            </ul>
                        </li>
                    </ol>
                    These daemons work together to form the backbone of a Hadoop cluster, enabling reliable and efficient data storage and processing.
                    The specific roles and names might vary slightly depending on the Hadoop version and configuration.
                    <hr>
                
                <p><strong>Nodes:</strong></p> Nodes are the  machines that make up a cluster. They can be physical or virtual servers in a cloud computing environment.
                These nodes are connected with each other through LAN (Local Area Network). The nodes in a cluster share the data, work on the same task and this nodes 
                are good enough to work as a single unit means all of them to work together.
                The nodes in a Hadoop cluster can be divided into two types:
                <ul>
                    <li><strong>Master Node: </strong> This is the node that manages the overall operation of the cluster. It is responsible for assigning tasks to other nodes, monitoring their progress, and handling failures. The master node in a Hadoop cluster is typically a <code>NameNode (NN)</code> in the case of HDFS (Hadoop Distributed File System).</li>
                    <li><strong>Worker (slave) Nodes: </strong> These are the nodes that perform the actual processing of data. Each worker node is responsible for executing tasks assigned by the master node. In the context of HDFS, the worker nodes are called <code>DataNodes (DN)</code>.</li>
                </ul>
                
                <figure>
                    <img src="assets/img/data-engineering/hadoop-master-slave.png" alt="" style="max-width: 90%; max-height: auto;">
                    <figcaption style="text-align: center;"><strong>Image credit: </strong><a href="https://mermaid.live/edit#pako:eNpVkU9vwjAMxb9KlFOR4DC49TAJWv4PDnBbwyG0BjLapEscaVPFd59JVxDyxfo95yn2a3huCuAxF_peZyvrC_vYCT3OklKBxgMbDN7ZJNr7Y6XQsS9zZBa-PTjsCT0JapJtpEOwbEteB6GTQNNsB854m8NGankGS0oalGlzH_yn7O3W8dkLHz74_IWPiE8DX2SpRHnX2BuZzwJcPuGQ4DzA1ROOCC4CXDfTH8g9gmMo3dWR77IVhF51zbrdJdqDLhxt7nyJrtf9LIlWdI_cVHUJCL1u9Um0NahOipzzcMXHqcbhwdOmLd7nFdhKqoKSaIRmTHC8QAWCx9QW0l4FJXSjOenR7H91zmO0Hvrc14VESJWk6Coen2TpiEKh0NhNG21IuM9rqT-N6WZuf176o8o" target="_blanck">These images can be created here.</a> </figcaption>
                </figure>
                where:
                <ul>
                    <li><strong>Client: </strong>Submits a job request to the Master Node.</li>
                    <li><strong>Master Node: </strong>Receives the job request and coordinates its execution.</li>
                    <li><strong>ResourceManager: </strong>Manages resources (CPU, memory) across the cluster and allocates them to tasks.</li>
                    <li><strong>NodeMangers: </strong>Run on each worker node, managing tasks and reporting progress to the ResourceManager.</li>
                    <li><strong>DataNodes: </strong>Store data blocks and provide access to them for task execution.</li>
                    <li><strong>Tasks: </strong> Individual units of work that make up a job, executed on worker nodes.</li>
                    <li><strong>Results: </strong>Generated by tasks and sent back to the ResourceManager, eventually reaching the client.</li>
                </ul>


                <p><strong>Types of Hadoop clusters</strong> There are mainly two types of Hadoop clusters:</p>
                <ul>
                    <li><strong>Single Node Hadoop Cluster: </strong>This type consists of a single machine running all the essential Hadoop daemons (NameNode, DataNode, ResourceManager, NodeManager) on its own.
                        <p><strong>Pros: </strong>
                            <ul>
                                <li><strong>Simple setup: </strong>Easy to install and configure, ideal for testing or learning.</li>
                                <li><strong>Low cost: </strong>Requires only one machine, minimizing hardware expenses.</li>
                                <li><strong>No additional infrastructure: </strong>No need for complex network configurations or cluster management tools.</li>
                            </ul>
                        </p>
                        <p><strong>Cons:</strong>
                            <ul>
                                <li><strong>Limited scalability: </strong>Cannot handle large datasets effectively due to resource constraints of a single machine.</li>
                                <li><strong>No fault tolerance: </strong>A single point of failure – if the machine fails, the entire cluster goes down.</li>
                                <li><strong>Not suitable for production: </strong>Primarily used for small-scale deployments or educational purposes.</li>
                            </ul>
                        </p>
                    </li>
                    <li><strong>Multiple Node Hadoop Cluster: </strong>This type distributes the workload across multiple machines, each having specific roles (master nodes, worker nodes).
                        <p><strong>Pros: </strong>
                            <ul>
                                <li><strong>Scalability: </strong>Easily add more nodes to handle growing datasets and processing demands.</li>
                                <li><strong>Fault tolerance: </strong>Distributed architecture ensures cluster functionality even if individual nodes fail.</li>
                                <li><strong>High performance: </strong>Utilizes combined resources of multiple machines for faster processing.</li>
                            </ul>
                        </p>
                        <p><strong>Cons: </strong>
                            <ul>
                                <li><strong>Complex setup: </strong>Requires more configuration and technical expertise to manage.</li>
                                <li><strong>Higher cost: </strong>Hardware and software costs increase with the number of nodes.</li>
                                <li><strong>Additional infrastructure: </strong>Requires network setup, cluster management tools, and potentially dedicated resources.</li>
                            </ul>
                        </p>
                    </li>
                </ul>
                <p><strong>Single and multi-node: </strong>Next question one can ask is how to Choose between single-node and multi-node? The choice depends on specific needs:</p>
                <ul>
                    <li>For small datasets, basic testing, or learning purposes, a single-node cluster might suffice.</li>
                    <li>For production environments with larger datasets, complex workloads, and scalability requirements, a multi-node cluster is essential.</li>
                </ul>
            </div>
















            </section>

            <!---------->
            <section>
                <div class="grey-box">
                    <h3>Apache Ecosystem</h3>
                    The Hadoop ecosystem includes a wide variety of open source Big Data tools. 
                    These various tools complement Hadoop and enhance its Big Data processing capability. Among the most popular, 
                    Apache Hive is a Data Warehouse dedicated to processing large datasets stored in HDFS. The Zookeeper tool automates failovers and reduces the impact of a NameNode failure.
                    <table>
                        <tr>
                            <th>Name</th>
                            <th>Description</th>
                        </tr>
                        <tr>
                            <td>Hadoop Distributed File System (HDFS)</td>
                            <td>HDFS is the primary storage system for Hadoop, designed to store large files across multiple nodes in a distributed manner.</td>
                        </tr>
                        <tr>
                            <td>MapReduce</td>
                            <td>The original programming model for distributed processing in Hadoop, as previously mentioned.</td>
                        </tr>
                        <tr>
                            <td>Apache Hive</td>
                            <td>A data warehousing and SQL-like query language for Hadoop. It allows users to query and analyze large datasets stored in Hadoop</td>
                        </tr>
                        <tr>
                            <td>Apache Pig</td>
                            <td>A high-level platform and scripting language built on top of Hadoop, designed for processing and analyzing large datasets</td>
                        </tr>
                        <tr>
                            <td>Apache HBase</td>
                            <td>A distributed, scalable, and NoSQL database that provides real-time read/write access to large datasets.</td>
                        </tr>
                        <tr>
                            <td>Apache Kafka</td>
                            <td>A distributed streaming platform for building real-time data pipelines and streaming applications.</td>
                        </tr>
                        <tr>
                            <td>Apache Spark</td>
                            <td>A fast and general-purpose cluster computing framework that supports in-memory processing and provides APIs for various programming languages.</td>
                        </tr>
                        <tr>
                            <td>Apache Flink</td>
                            <td>A stream processing framework for big data processing and analytics.</td>
                        </tr>
                        <tr>
                            <td>Apache Sqoop</td>
                            <td>A tool for efficiently transferring bulk data between Apache Hadoop and structured data stores such as relational databases</td>
                        </tr>
                        <tr>
                            <td>Apache Flume</td>
                            <td>A distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data.</td>
                        </tr>
                        <tr>
                            <td>Apache Oozie</td>
                            <td>A workflow scheduler for managing Hadoop jobs, allowing the coordination of various tasks in a Hadoop ecosystem.</td>
                        </tr>
                        <tr>
                            <td>Apache Zeppelin</td>
                            <td>A web-based notebook for data analytics and visualization, supporting multiple languages including SQL, Python, and Scal.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache ZooKeeper:</strong></td>
                            <td>A distributed coordination service that provides distributed synchronization and helps in managing configuration information and naming services for distributed systems.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache Mahout:</strong></td>
                            <td>A machine learning library and framework for building scalable and distributed machine learning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache Accumulo:</strong></td>
                            <td>A NoSQL database that is built on top of Apache Hadoop and based on Google's BigTable design.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache Drill:</strong></td>
                            <td>A schema-free SQL query engine for Hadoop, NoSQL databases, and cloud storage.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache Knox:</strong></td>
                            <td>A gateway for providing secure access to the Hadoop ecosystem through REST APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache Ranger:</strong></td>
                            <td>A framework for managing access and authorization to various components within the Hadoop ecosystem.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache Atlas:</strong></td>
                            <td>A metadata management and governance framework for Hadoop, providing a scalable and extensible solution for cataloging and managing metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache Ambari:</strong></td>
                            <td>A management and monitoring platform that simplifies the provisioning, managing, and monitoring of Hadoop clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache NiFi:</strong></td>
                            <td>A data integration and automation tool that facilitates the flow of data between systems.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache Kylin:</strong></td>
                            <td>An open-source distributed analytics engine designed for OLAP (Online Analytical Processing) on big data.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache Livy:</strong></td>
                            <td>A REST service for Apache Spark that enables easy interaction with Spark clusters over a web interface.</td>
                        </tr>
                        <tr>
                            <td><strong>Apache Superset:</strong></td>
                            <td>A modern, enterprise-ready business intelligence web application that facilitates data exploration and visualization.</td>
                        </tr>  
                        <tr>
                            <td><strong>Apache Airflow:</strong></td>
                            <td>
                                Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. It allows you to define a directed acyclic graph (DAG) of tasks and their dependencies, enabling the automation of complex data workflows.
                            </td>
                        </tr>                  
                    </table>
                </div>
            </section>

            <!-------Reference ------->
            <section id="reference">
                <h2>References</h2>
                <ol>
                    <li><a href="https://spark.apache.org/docs/latest/" target="_blank"><a href="https://spark.apache.org/" target="_blanck">Apache Spark™</a> Official Documentation</a>.</li>
                    <li><a href="https://www.databricks.com/learn" target="_blank">Databricks Learning Academy</a>.</li>
                    <li><a href="https://sparkbyexamples.com/" target="_blank">Spark by Examples</a>.</li>
                </ol> 
            </section>

            <hr>
        
            <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

                <h3>Some other interesting things to know:</h3>
                <ul style="list-style-type: disc; margin-left: 30px;">
                    <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
                    <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
                </ul>
            </div>
            <p></p>

            <div class="navigation">
                
                <a href="index.html#portfolio" class="clickable-box">
                    <span class="arrow-left">Portfolio section</span>
                </a>
                
                <a href="Data-engineering.html" class="clickable-box">
                    <span class="arrow-right">Content</span>
                </a>

            </div>
        </div>
        </section><!-- End Portfolio Details Section -->
    </main><!-- End #main --

    <!-- ======= Footer ======= -->
    <footer id="footer">
    <div class="container">
        <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
        </div>
    </div>
    </footer><!-- End  Footer -->

    <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="assets/vendor/typed.js/typed.umd.js"></script>
    <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function () {
        hljs.initHighlightingOnLoad();
    });
    </script>

</body>

</html>