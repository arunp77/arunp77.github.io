<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Deep Learning</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

    <!-- ======= Header ======= -->
    <header id="header">
    <div class="d-flex flex-column">
        <div class="profile">
            <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
            <h1 class="text-light"><a href="index.html">Arun</a></h1>
            <div class="social-links mt-3 text-center">
                <a href="https://www.linkedin.com/in/arunp77/" target="_blank" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                <a href="https://github.com/arunp77" target="_blank" class="github"><i class="bx bxl-github"></i></a>
                <a href="https://twitter.com/arunp77_" target="_blank" class="twitter"><i class="bx bxl-twitter"></i></a>
                <a href="https://www.instagram.com/arunp77/" target="_blank" class="instagram"><i class="bx bxl-instagram"></i></a>
                <a href="https://arunp77.medium.com/" target="_blank" class="medium"><i class="bx bxl-medium"></i></a>
            </div>
        </div>

        <nav id="navbar" class="nav-menu navbar">
            <ul>
                <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
                <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
                <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
                <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
                <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
                <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
                <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
                <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
                <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
                <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
            </ul>
        </nav><!-- .nav-menu -->
    </div>
    </header><!-- End Header -->

    <main id="main">
        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs"> 
          <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
              <h2>Machine Learning</h2>
              <ol>
                <li><a href="machine-learning.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
              </ol>
            </div>
    
          </div>
        </section><!-- End Breadcrumbs -->

        <!------  right dropdown menue ------->
        <div class="right-side-list">
            <div class="dropdown">
                <button class="dropbtn"><strong>Shortcuts:</strong></button>
                <div class="dropdown-content">
                    <ul>
                        <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                        <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                        <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                        <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                        <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                        <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                        <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                        <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                        <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                        <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                        <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                        <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                        <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                            <!-- Add more subsections as needed -->
                    </ul>
                </div>
            </div>
        </div>

        <!-- ======= Portfolio Details Section ======= -->
        <section id="portfolio-details" class="portfolio-details">
        <div class="container">
            <div class="row gy-4">
                <h1>Deep Learning Fundamentals</h1>
                <div class="col-lg-8">
                    <div class="portfolio-details-slider swiper">
                        <div class="swiper-wrapper align-items-center"> 
                            <figure>
                            <img src="assets/img/machine-ln/deep-ml1.png" alt="" style="max-width: 90%; max-height: auto;">
                            <figcaption style="text-align: center;">Machine learning and Deep learning differences (<strong>Image credit: </strong> Â© <a href="index.html">Arun</a>)</figcaption>
                            </figure>
                        </div>
                        <div class="swiper-pagination"></div>
                    </div>
                </div>

                <div class="col-lg-4 grey-box">
                    <div class="section-title">
                        <h3>Content</h3>
                        <ol>
                            <li><a href="#introduction">Introduction</a></li>
                            <li><a href="#key-difference">Key Differences Between Machine Learning And Deep Learning</a></li>
                            <li><a href="#applications">Applications of Deep Learning</a></li>
                            <li><a href="#classification">Deep Learning model classification</a></li>
                            <li><a href="#neural">Neural Networks</a></li>
                            <li><a href="#types">Types of Neural Network Diagrams</a></li>
                            <li><a href="#key-points">Key Points</a></li>
                            <li><a href="#neural-network">Understanding the Neural Network Jargon</a></li>
                            <li><a href="#learning-tech">Learning Terminology</a></li>
                            <li><a href="#other-term">Other Important Terms</a></li>
                            <li><a href="#activation">Activation function</a></li>
                            <li><a href="#how-learn">How does the network learn?</a></li>
                            <li><a href="#hardware">Deep Learning hardware requirements</a></li>
                            <li><a href="#reference">Reference</a></li>
                        </ol>
                    </div>
                </div>
            </div>

            <section>
            <hr>
            <br/><br/>

            <h3 id="introduction">Introduction</h3>
            Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers, enabling the model to learn hierarchical representations of data. 
            These neural networks are composed of multiple layers of interconnected nodes, called neurons, that can learn hierarchical representations of the data. Deep learning algorithms 
            excel at processing complex, high-dimensional data, such as images, speech, and text. DL has gained significant attention and success in recent years, achieving state-of-the-art 
            performance in various domains, including computer vision, natural language processing, and speech recognition.
            
            <br><br>
            <!------------------------->
            <h4 id="key-difference">Key Differences Between Machine Learning And Deep Learning</h4>
            <p>If deep learning is a subset of machine learning, how do they differ? Deep learning distinguishes itself from classical machine learning by the type of data that it works with 
                and the methods in which it learns.</p>
            <p>Machine learning algorithms leverage structured, labeled data to make predictionsâmeaning that specific features are defined from the input data for the model and organized into 
                tables. This doesnât necessarily mean that it doesnât use unstructured data; it just means that if it does, it generally goes through some pre-processing to organize it into a 
                structured format.</p>
            <p>Deep learning eliminates some of data pre-processing that is typically involved with machine learning. These algorithms can ingest and process unstructured data, like text and 
                images, and it automates feature extraction, removing some of the dependency on human experts. </p>
            <p><strong>Example: </strong>For example, letâs say that we had a set of photos of different pets, and we wanted to categorize by âcatâ, âdogâ, âhamsterâ, et cetera. Deep learning 
                algorithms can determine which features (e.g. ears) are most important to distinguish each animal from another. In machine learning, this hierarchy of features is established 
                manually by a human expert.
                Then, through the processes of gradient descent and backpropagation, the deep learning algorithm adjusts and fits itself for accuracy, allowing it to make predictions about a 
                new photo of an animal with increased precision. 
            </p>
            <table border="1">
                <tr>
                <th></th>
                    <th>Machine Learning</th>
                    <th>Deep Learning</th>
                </tr>
                <tr>
                    <td><b>Approach</b></td>
                    <td>Requires structured data</td>
                    <td>Does not require structured data</td>
                </tr>
                <tr>
                    <td><b>Human intervention</b></td>
                    <td>Requires human intervention for mistakes</td>
                    <td>Does not require human intervention for mistakes</td>
                </tr>
                <tr>
                    <td><b>Hardware</b></td>
                    <td>Can function on CPU</td>
                    <td>Requires GPU / Significant computing power</td>
                </tr>
                <tr>
                    <td><b>Time</b></td>
                    <td>Takes seconds to hours</td>
                    <td>Takes weeks</td>
                </tr>
                <tr>
                    <td><b>Uses</b></td>
                    <td>Forecasting, predicting and other simple applications</td>
                    <td>More complex applications like autonomous vehicles</td>
                </tr>
            </table><br>

            <!------------------- applications ------------->
            <h4 id="applications">Applications of Deep Learning</h4>
            Key application areas and some specific examples:
            <ol>
                <li><strong>Computer Vision: </strong>
                    <ul>
                        <li><strong>Image Classification:</strong> Categorizing images (cats vs. dogs, types of clothing, etc.).</li>
                        <li><strong>Object Detection:</strong> Finding and identifying objects within an image (for self-driving cars, robotics, security).</li>
                        <li><strong>Semantic Segmentation:</strong> Labeling every pixel in an image with its corresponding class (road, building, etc.). This has applications in urban planning and augmented 
                            reality.</li>
                        <li><strong>Image Generation and Style Transfer:</strong> Creating realistic images with tools like GANs, or applying artistic styles to photos. </li>
                    </ul>
                </li>
                <li><strong>Natural Language Processing (NLP): </strong>
                    <ul>
                        <li><strong>Machine Translation:</strong> Automatic translation between languages (Google Translate, DeepL).</li>
                        <li><strong>Text Generation:</strong> Creating human-quality text, from creative writing to writing marketing copy.</li>
                        <li><strong>Sentiment Analysis:</strong> Analyzing the emotional tone of reviews, social media posts, and more.</li>
                        <li><strong>Chatbots and Virtual Assistants:</strong> Interacting with customers, providing support (Siri, Alexa).</li>
                    </ul>
                </li>
                <li><strong>Healthcare: </strong>
                    <ul>
                        <li><strong>Medical Image Analysis:</strong> Detecting tumors, diagnosing diseases from X-rays, CT Scans, MRIs, and more.</li>
                        <li><strong>Drug Discovery:</strong> Analyzing vast chemical databases to design potentially effective new drugs.</li>
                        <li><strong>Genomics:</strong> Understanding the relationship between genes and diseases, improving personalized medicine.</li>
                        <li><strong>Predictive Healthcare:</strong> Predicting patient outcomes and helping with treatment plans.</li>
                    </ul>
                </li>
                <li><strong>Recommender Systems: </strong>
                    <ul>
                        <li><strong>Product recommendations:</strong> Predicting which products someone might like (Amazon, Netflix).</li>
                        <li><strong>Content Personalization:</strong> Tailoring news feeds, music playlists, and other content to a user's preferences.</li>
                        <li><strong>Advertising:</strong> Using deep learning to better target online ads based on a user's interests.</li>
                    </ul>
                </li>
                <li><strong>Finance: </strong>
                    <ul>
                        <li><strong>Fraud Detection:</strong> Analyzing patterns to flag suspicious financial activity.</li>
                        <li><strong>Algorithmic Trading:</strong> Using deep learning to develop trading strategies and automatically execute trades. </li>
                        <li><strong>Risk Assessment:</strong> Assessing the creditworthiness of customers or organizations.</li>
                    </ul>
                </li>
                <li><strong>Other Notable Applications: </strong>
                    <ul>
                        <li><strong>Self-driving Cars:</strong> Perception systems in autonomous vehicles heavily rely on deep learning for recognizing objects, lane lines, and making decisions.</li>
                        <li><strong>Robotics:</strong> Deep learning enables robots to perceive the world, navigate complex environments, and manipulate objects.</li>
                        <li><strong>Gaming:</strong> Creating intelligent game AI, generating complex game environments, and enhancing the player experience.</li>
                        <li><strong>Agriculture:</strong> Monitoring crop health, identifying pests, and improving farming practices. </li>
                        <li><strong>Accessibility:</strong> Creating tools for people with visual or hearing impairments through image captioning and automated sign language translation.</li>
                    </ul>
                </li>
            </ol>

            <br>
            <!----------------------->
            <h3 id="classification">Deep Learning model classification</h3>
            <p>Deep learning models can be classified into several categories based on their architecture and the type of data they are designed to process. Here are some common types of deep 
                learning models along with 
                examples:</p>
            <ol>
                <li><strong>Convolutional Neural Networks (CNNs): </strong>
                    <ul>
                        <li><strong>Description: </strong>Convolutional Neural Networks (CNNs) are a class of deep learning models primarily used for image and video processing (computer vision) tasks. 
                            They excel at 
                            learning spatial hierarchies and capturing local patterns. CNNs consist of convolutional layers that apply filters to input images, enabling feature extraction. These filters 
                            detect patterns 
                            such as edges, textures, and shapes. Following convolutional layers, pooling layers are often used to downsample the data, reducing dimensionality while preserving important 
                            features. Finally, 
                            fully connected layers are employed for classification tasks, where the extracted features are combined to make predictions. CNNs have achieved breakthrough performance in 
                            various computer 
                            vision tasks, including image classification, object detection, and image segmentation.</li>
                        <li><strong>Example: </strong>An example of CNN is the VGG (Visual Geometry Group) network, which achieved high performance on image classification tasks. Another example is the 
                            popular ResNet 
                            (Residual Network), known for its deep architecture and efficient training.</li>
                    </ul>
                </li>
                <li><strong>Recurrent Neural Networks (RNNs): </strong>
                    <ul>
                        <li><strong>Description: </strong>Recurrent Neural Networks (RNNs) are a class of deep learning models specifically designed for processing sequential data, such as time series or 
                            natural language. 
                            They are equipped with internal memory, which enables them to maintain a memory of past inputs and process information with temporal dependencies. This memory allows RNNs to 
                            capture sequential 
                            patterns and context, making them well-suited for tasks like speech recognition, language modeling, and machine translation. RNNs incorporate recurrent connections within their 
                            architecture, 
                            forming loops that allow information to persist and be passed from one time step to another, facilitating the modeling of sequential relationships in the data.</li>
                        <li><strong>Example: </strong>An example of an RNN is the Long Short-Term Memory (LSTM) network, which addresses the vanishing gradient problem in traditional RNNs and is widely used 
                            for tasks 
                            like speech recognition, language translation, and text generation.</li>
                    </ul>
                </li>
                <li><strong>Generative Adversarial Networks (GANs): </strong>
                    <ul>
                        <li><strong>Description: </strong>GANs consist of two neural networksâa generator and a discriminatorâcompeting against each other. The generator network learns to create realistic 
                            data instances, such as images, while the discriminator network tries to distinguish between the generated data and real data. GANs are popular for generating synthetic data, 
                            image synthesis, image-to-image translation, and even creating deepfake videos.</li>
                        <li><strong>Example: </strong>An example of GAN is the DCGAN (Deep Convolutional Generative Adversarial Network), which extends the GAN architecture with convolutional layers. 
                            DCGANs are used 
                            for generating high-quality images and video synthesis.</li>
                    </ul>
                </li>
                <li><strong>Autoencoders: </strong>
                    <ul>
                        <li><strong>Description: </strong>Autoencoders are neural networks trained to reconstruct input data, typically used for dimensionality reduction and feature learning. They consist 
                            of an encoder 
                            network that compresses the input into a lower-dimensional representation (encoding), and a decoder network that reconstructs the input from the encoding.</li>
                        <li><strong>Example: </strong>An example of an autoencoder is the Variational Autoencoder (VAE), which adds a probabilistic interpretation to the latent space learned by the encoder. 
                            VAEs are used for generating new data samples and performing data imputation.</li>
                    </ul>
                </li>
                <li><strong>Transformer Models: </strong>
                    <ul>
                        <li><strong>Description: </strong>Transformer models are designed for processing sequential data, particularly suited for natural language processing tasks. They rely on self-attention 
                            mechanisms to capture long-range dependencies in input sequences without recurrent connections.</li>
                        <li><strong>Example: </strong>The Transformer architecture, introduced in the paper "Attention is All You Need", is widely used in state-of-the-art NLP models such as BERT (Bidirectional 
                            Encoder Representations from Transformers) for tasks like text classification, question answering, and language translation.</li>
                    </ul>
                </li>
            </ol>
            </section>

            <!----------------->
            <h3 id="neural">Neural Networks</h3>
            <p>Neural networks are a computational model inspired by the structure and function of biological neural networks in the human brain. They consist of interconnected nodes, called neurons, 
                organized in layers. Each neuron receives input signals, performs a computation, and produces an output signal. Neural networks are capable of learning complex patterns and relationships 
                in data, making them powerful tools for various machine learning tasks.</p>
            <p>Here's the core idea:</p>
            <ul>
                <li><strong>Neurons: </strong>The basic units of computation. In an artificial neural network, these are usually simple mathematical functions. They receive inputs, process them, and produce 
                    an output.</li>
                <li><strong>Connections: </strong> Like neurons of the brain are connected by synapses, artificial neurons are linked. Each connection has a weight that represents its importance.</li>
                <li><strong>Learning: </strong>The power of neural networks lies in their ability to adjust the weights of these connections based on training data. This allows them to learn complex patterns 
                    and relationships.</li>
            </ul>
            <h5 id="types">Types of Neural Network Diagrams</h5>
            <ol>
                <li><strong><a href="https://learnopencv.com/understanding-feedforward-neural-networks/" target="_blank">Simple Feedforward Network</a>: </strong>
                    <ul>
                        <li><strong>Structure: </strong>Neurons arranged in layers. Data flows in one direction, from input to output.</li>
                        <li><strong>Layers: </strong>
                            <ul>
                                <li><strong>Input Layer: </strong>Takes the raw data.</li>
                                <li><strong>Hidden Layers: </strong>One or more layers where computation happens.</li>
                                <li><strong>Output Layer: </strong>Generates the final prediction or classification.</li>
                            </ul>
                        </li>
                    </ul>
                    <figure>
                        <img src="assets/img/machine-ln/deep-smf.png" alt="" style="max-width: 70%; max-height: auto;">
                        <figcaption style="text-align: center;">An example of a feed forward Neural Network with one hidden layer (with 3 neurons)(<strong>Image Credit: </strong> Â© <a href="index.html">Arun</a>) </figcaption>
                    </figure>
                </li>
                <li><strong><a href="https://learnopencv.com/understanding-convolutional-neural-networks-cnn/" target="_blank">Convolutional Neural Network (CNN)</a>: </strong>
                    <ul>
                        <li><strong>Specialization: </strong>Highly effective for image and video analysis.</li>
                        <li><strong>Convolutional Layers: </strong> Extract features from images using filters. Think of these filters as sliding windows searching for edges, shapes, textures, etc.</li>
                        <li><strong>Pooling Layers: </strong>Reduce data size and make the network more robust to slight variations in the input.</li>
                        <figure>
                            <img src="assets/img/machine-ln/deep-convoluition.png" alt="" style="max-width: 70%; max-height: auto;">
                            <figcaption style="text-align: center;">Schematic diagram of a basic convolutional neural network (CNN) architecture (<strong>Image Credit: </strong> Â© <a href="https://www.mdpi.com/2076-3417/9/21/4500">Van Hiep Phung and Eun Joo Rhee</a>) </figcaption>
                        </figure>
                    </ul>
                </li>
                <li><strong>Recurrent Neural Network (RNN) </strong>
                    <ul>
                        <li><strong>Memory: </strong> Designed to handle sequential data (text, time series). These networks have internal memory (hidden states) which allows them to 'remember' past information.</li>
                        <li><strong>Common Architectures: </strong>LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) networks are more advanced types of RNNs designed to address potential shortcomings 
                            in the plain RNN.</li>
                    </ul>
                    <figure>
                        <img src="assets/img/machine-ln/deep-rnn.png" alt="" style="max-width: 30%; max-height: auto;"> 
                        <figcaption style="text-align: center;">Schematic diagram of a Recurrent neural network architecture (<strong>Image Credit: </strong> Â© <a href="index.html">Arun</a>) </figcaption>
                    </figure>
                </li>
                <li><strong>Autoencoders: </strong>Used for dimensionality reduction and data compression. They consist of an encoder and decoder; the encoder learns to compress data into a compact representation, 
                    while the decoder tries to reconstruct the original input from this compressed form.</li>
                <figure>
                    <img src="assets/img/machine-ln/deep-autoencoders.png" alt="" style="max-width: 60%; max-height: auto;">
                    <figcaption style="text-align: center;">Schematic diagram of a Autoencoders architecture (<strong>Image Credit: </strong> Â© <a href="index.html">Arun</a>) </figcaption>
                </figure>
                <li><strong>Generative Adversarial Networks (GANs): </strong>Two neural networks playing a game. One network (generator) produces synthetic data, while the other (discriminator) tries to distinguish 
                    real data from the generated data. This leads to models capable of creating incredibly realistic fake images, audio, and even text.</li>
                <figure>
                    <img src="assets/img/machine-ln/deep-gans1.png" alt="" style="max-width: 50%; max-height: auto;">
                    <figcaption style="text-align: center;">Schematic diagram of a GANs architecture (<strong>Image Credit:</strong> Â© <a href="index.html" target="_blank">Arun</a>) </figcaption>
                </figure>
            </ol>

            
            <h5 id="key-points">Key Points:</h5>
            <ul>
                <li>Neural networks are incredibly flexible â the right architecture depends on the problem you're solving.</li>
                <li>Diagrams make a complex topic visual and easier to grasp.</li>
                <li>Deep learning utilizes multiple layers of neurons, which is where the 'deep' comes from.</li>
            </ul>

            <h5>Why Deep Learning Is So Effective</h5>
            <ul>
                <li><strong>Learning Complex Patterns:</strong> Deep learning systems can discover intricate patterns in huge datasets too complex for humans to spot.</li>
                <li><strong>Generalization:</strong> They generalize well to new, unseen data because these complex patterns help them make intelligent inferences.</li>
                <li><strong>Feature Engineering:</strong> Traditionally, extracting meaningful features from data was labor-intensive. Deep learning automates much of this, allowing the model to discover helpful 
                    representations.</li>
            </ul>
            

            


            <!-------------------------->
            <h3 id="neural-network">Understanding the Neural Network Jargon</h3>
            The important jargon in the world of neural networks are:
            <ul>
                <li><strong>Neuron (or Perceptron): </strong> The basic computational unit in a neural network. It receives inputs, performs a weighted sum of those inputs, potentially applies an activation 
                    function, and produces an output.
                    <figure>
                        <img src="assets/img/machine-ln/deep-neuron.png" alt="" style="max-width: 50%; max-height: auto;">
                        <figcaption style="text-align: center;">An example of a neuron showing the input \(x_1, x_2, ...x_n\) their corresponding weights \((w_1 - w_n)\), a bias \(b\) and the activation 
                            function \(f\) applied to the weighted sum of the inputs. (<strong>Image Credit:</strong> Â© <a href="index.html"> Arun</a>) </figcaption>
                    </figure>
                    As seen in the figure, it works in two steps - it calculates the weighted sum of its inputs and then applies an activation function to normalize the sum. The activation functions can be 
                    linear or nonlinear. Also, there are weights associated with each input of a neuron. These are the parameters which the network has to learn during the training phase.
                </li>
                <li><strong>Input Layer: </strong>Where you feed the raw data (pixels of an image, words in a sentence) into the network.</li>
                <li><strong>Hidden Layers: </strong> Layers between the input and output. This is where the majority of the computation and "learning" occurs. The more hidden layers, the 'deeper' the neural 
                    network.</li>
                <li><strong>Output Layer: </strong>Delivers the final prediction or classification from the network. The activation function to be used in this layer is different for different problems. 
                    For a binary classification problem, we want the output to be either 0 or 1. Thus, a sigmoid activation function is used. For a Multiclass classification problem, a Softmax 
                    (think of it as a generalization of sigmoid to multiple classes ) is used. For a regression problem, where the output is not a predefined category, we can simply use a linear unit.</li>
                <li><strong>Weights: </strong>Each connection between neurons has a weight, signifying its relative importance to the neural network's final prediction. Training a neural network 
                    involves fine-tuning these weights.</li>
                <li><strong>Bias: </strong>Acts as a sort of intercept term, adding an extra value to a neuron's calculation. This helps give the network extra flexibility when fitting the data.</li>
            </ul>

            <!-------------------------->
            <h4 id="learning-tech">Learning Terminology</h4>
            <ul> 
                <li><strong>Activation Function:</strong> A non-linear function (e.g., Sigmoid, ReLU, Tanh) applied to the output of a neuron. It introduces non-linearity, essential for the network 
                    to represent complex patterns.</li> 
                <li><strong>Backpropagation:</strong> The core algorithm for training neural networks. Through iterations of forward and backward passes, it calculates how much each weight contributes 
                    to the final error and adjusts those weights accordingly.</li> 
                <li><strong>Gradient Descent:</strong> An optimization algorithm widely used in deep learning. It guides the process of updating weights during backpropagation by moving "downhill" on 
                    a function representing the error of the network. (for more details, see <a href="Linear-reg.html">Gradient Descent method and linear regression</a>.)</li> 
                <li><strong>Loss Function:</strong> Measures the error between the network's prediction and the true label. During training, the aim is to minimize the loss function.</li> 
                <li><strong>Epoch:</strong> One full pass through the entire training dataset.</li> 
                <li><strong>Batch Size:</strong> The number of data samples processed before updating weights during training.</li> 
            </ul>

            <!-------------------->
            <h4 id="other-term">Other Important Terms</h4>
            <ul>
                <li><strong>Hyperparameters:</strong> Settings you choose before training like the number of layers, learning rate, etc. Fine-tuning these is part of the art of deep learning.</li>
                <li><strong>Overfitting:</strong> When a model is too complex for the data and starts memorizing specific examples rather than learning general patterns.</li>
                <li><strong>Regularization:</strong> Techniques to prevent overfitting (like <a href="Ridge-lasso-elasticnet.html">L1/L2 regularization</a>, dropout).</li>
            </ul>

            <!------------------->
            <h4 id="activation">Activation function</h4>
            Activation functions are mathematical functions applied to the output of neurons in neural networks. They introduce non-linearity into the network, allowing it to learn complex patterns 
            and relationships in the data. Here are some common activation functions used in machine learning along with their formulas:
            <figure>
                <img src="assets/img/machine-ln/deep-activationfun-1.png" alt="" style="max-width: 100%; max-height: auto;">
                <figcaption style="text-align: center;">Various activation functions (for more details, see <a href="https://github.com/arunp77/Machine-Learning/tree/main/Deep-learning" target="_blank">my Github repo</a> )</figcaption>
            </figure> 
            <figure>
                <img src="assets/img/machine-ln/deep-activationfun-derivatives.png" alt="" style="max-width: 100%; max-height: auto;">
                <figcaption style="text-align: center;">Derivatives of these activation functions (for more details, see <a href="https://github.com/arunp77/Machine-Learning/tree/main/Deep-learning" target="_blank">my Github repo</a>)</figcaption>
            </figure>
            <ol>
                <li><strong>Sigmoid Function: </strong>
                    <ul>
                        <li><strong>Formula: </strong></li>
                        $$\sigma(x) = \frac{1}{1+ e^{-x}}$$
                        <li><strong>Range: </strong> (0, 1)</li>
                        <li><strong>Description: </strong>Sigmoid function squashes the input values to a range between 0 and 1. It is useful in binary classification tasks where the output needs 
                            to be interpreted as probabilities.</li>
                        <li><strong>Shortcomings: </strong>
                            <ul>
                                <li><strong>Vanishing Gradient: </strong>Sigmoid functions saturate for large positive or negative inputs, leading to vanishing gradients during backpropagation, 
                                    which can slow down or hinder learning, especially in deep networks.</li>
                                <li><strong>Output Range: </strong>The output of the sigmoid function is not centered around zero, which may result in unstable gradients and slower convergence 
                                    when used in deep networks.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>Hyperbolic Tangent (Tanh) Function: </strong>
                    <ul>
                        <li><strong>Formula: </strong></li>
                        $$\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
                        <li><strong>Range: </strong>(-1,1)</li>
                        <li><strong>Description: </strong>Tanh function squashes the input values to a range between -1 and 1, making it suitable for classification tasks where the output needs 
                            to be centered around zero.
                            Tanh functions are commonly used in hidden layers of neural networks, especially in recurrent neural networks (RNNs), to capture non-linearities and maintain gradients 
                            within a centered range.
                        </li>
                        <li><strong>Shortcomings: </strong>
                            <ul>
                                <li><strong>Vanishing Gradient: </strong>Similar to the sigmoid function, tanh functions also suffer from the vanishing gradient problem for large inputs, particularly 
                                    in deep networks.</li>
                                <li><strong>Saturation: </strong>Tanh functions saturate for large inputs, leading to slower convergence and potentially unstable gradients.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>Rectified Linear Unit (ReLU): </strong>
                    <ul>
                        <li><strong>Formula: </strong></li>
                        $$f(x) = \text{max}(0,x)$$
                        <li><strong>Range: </strong>\([0, +â]\)</li>
                        <li><strong>Description: </strong>ReLU function returns 0 for negative inputs and the input value for positive inputs. It is the most commonly used activation function in deep 
                            learning due to its simplicity and effectiveness.
                            ReLU functions are widely used in deep learning due to their simplicity and effectiveness. They allow for faster convergence and are less prone to vanishing gradients compared 
                            to sigmoid and tanh functions.
                        </li>
                        <li><strong>Shortcomings: </strong>
                            <ul>
                                <li><strong>Dying ReLU: </strong>ReLU neurons can become inactive (or "die") for negative inputs during training, leading to dead neurons and a sparse representation of the 
                                    input space. This issue is addressed by variants such as Leaky ReLU and Parametric ReLU.</li>
                                <li><strong>Unbounded Output: </strong>ReLU functions have an unbounded output for positive inputs, which may lead to exploding gradients during training, especially in deeper 
                                    networks.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>Leaky ReLU: </strong>
                    <ul>
                        <li><strong>Formula: </strong>
                            <div>
                                $$f(x) = \begin{cases} x, & \text{if } x > 0 \\ \alpha x, & \text{otherwise} \end{cases}$$
                            </div>
                            where \(\alpha\) is a small constant (\(<1\))
                        </li>
                        <li><strong>Range: </strong>(-â, +â)</li>
                        <li><strong>Description: </strong>Leaky ReLU addresses the "dying ReLU" problem by allowing a small gradient for negative inputs, preventing neurons from becoming inactive.</li>
                        <li><strong>Shortcomings: </strong>
                            <ul>
                                <li><strong>Hyperparameter Tuning: </strong> Leaky ReLU introduces a hyperparameter (the leak coefficient) that needs to be manually tuned, which can be cumbersome and 
                                    time-consuming.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>Exponential Linear Unit (ELU):</strong>
                    <ul>
                        <li><strong>Formula: </strong>
                            <div>
                                $$f(x) = \begin{cases} x, & \text{if } x > 0 \\ \alpha (e^x - 1), & \text{otherwise} \end{cases}$$
                            </div>
                            where \(\alpha\) is a Hyperparameter.
                        </li>
                        <li><strong>Range: </strong>(-â, +â)</li>
                        <li><strong>Description: </strong>ELU function smoothly handles negative inputs and can converge faster than ReLU, but it may be computationally more expensive.
                            ELU functions smoothly handle negative inputs and can converge faster than ReLU. They have a mean activation closer to zero, which helps to alleviate the vanishing gradient problem.
                        </li>
                        <li><strong>Shortcomings: </strong>
                            <ul>
                                <li><strong>Computational Cost: </strong>ELU functions involve exponential operations, which may be computationally more expensive compared to ReLU and its variants.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>Softmax Function: </strong>
                    <ul>
                        <li><strong>Formula: </strong>
                            $$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^k e^{x_j}}$$
                        for \(i=1,2, ... k\) where \(k\) is the number of classes.
                        </li>
                        <li><strong>Range: </strong>(0, 1) for each class, with all probabilities summing up to 1</li>
                        <li><strong>Description: </strong>Softmax function is commonly used in the output layer of a neural network for multi-class classification tasks. It converts raw scores into probabilities.
                            Softmax functions are used in the output layer of neural networks for multi-class classification tasks. They convert raw scores into probabilities, enabling the model to make 
                            predictions across multiple classes.
                        </li>
                        <li><strong>Shortcomings: </strong>
                            <ul>
                                <li><strong>Sensitivity to Outliers: </strong>Softmax functions are sensitive to outliers and large input values, which may affect the stability and reliability of the 
                                    predicted probabilities.</li>
                            </ul>
                        </li>
                    </ul>
                    <figure>
                        <img src="assets/img/machine-ln/deep-activationfun-softmax.png" alt="" style="max-width: 40%; max-height: auto;">
                        <figcaption style="text-align: center;"></figcaption> 
                    </figure>
                </li>
            </ol>
            <p>These activation functions play a crucial role in the training and performance of neural networks by controlling the output of neurons and enabling the network to learn complex 
                relationships in the data.</p>
            <p>In summary, the choice of activation function depends on the specific requirements of the task, the architecture of the neural network, and empirical performance on the validation data. 
                It is often beneficial to experiment with different activation functions and monitor the training dynamics and model performance to select the most suitable one for a given problem. 
                Additionally, using advanced techniques such as batch normalization and adaptive learning rate methods can help mitigate some of the shortcomings associated with activation functions.</p>

            <!--------------------->
            <h4 id="how-learn">How does the network learn?</h4>
            Neural networks learn through a process called backpropagation, which is a fundamental algorithm for training deep learning models. The process of learning involves adjusting the parameters 
            (weights and biases) of the neural network to minimize a chosen loss function, which measures the difference between the predicted outputs and the true targets in the training data.

            <p>Here's a step-by-step overview of how neural networks works learn:</p>
            <ol>
                <li><strong>Initialization:</strong> The parameters of the neural network (weights and biases) are initialized with random values.</li>
                <li><strong>Forward Propagation:</strong> During the forward propagation step, input data is passed through the network, layer by layer, to generate predictions. Each neuron performs a 
                    weighted sum of its inputs, applies an activation function, and passes the result to the neurons in the next layer. This process continues until the output is produced.</li>
                <li><strong>Loss Calculation:</strong> Once the predictions are obtained, a loss function is used to quantify the difference between the predicted outputs and the true targets in the 
                    training data. Common loss functions include mean squared error (MSE) for regression tasks and categorical cross-entropy for classification tasks.</li>
                <li><strong>Backpropagation:</strong> Backpropagation is the heart of the learning process in neural networks. It involves computing the gradients of the loss function with respect to 
                    the network parameters using the chain rule of calculus. The gradients represent the direction and magnitude of the changes needed to minimize the loss.</li>
                <li><strong>Gradient Descent:</strong> With the gradients computed, the network parameters are updated iteratively using optimization algorithms such as gradient descent. The parameters 
                    are adjusted in the direction that reduces the loss, with the size of the updates controlled by the learning rate.</li>
                <li><strong>Iteration:</strong> Steps 2 to 5 are repeated for multiple iterations (epochs) over the entire training dataset. Each iteration consists of forward propagation to generate 
                    predictions, backpropagation to compute gradients, and parameter updates using gradient descent.</li>
                <li><strong>Validation:</strong> Periodically, the model's performance is evaluated on a separate validation dataset to monitor its generalization ability and prevent overfitting. The 
                    validation loss is used as a proxy for the model's performance on unseen data.</li>
                <li><strong>Termination:</strong> Training continues until a stopping criterion is met, such as reaching a maximum number of epochs or observing no improvement in the validation loss.</li>
            </ol>


            <h4 id ="hardware">Deep learning hardware requirements</h4>
            Deep learning requires a tremendous amount of computing power. High performance graphical processing units (GPUs) are ideal because they can handle a large volume of calculations in multiple cores with copious memory available. However, managing multiple GPUs on-premises can create a large demand on internal resources and be incredibly costly to scale.
            However, here are some general guidelines for the hardware needed for deep learning:
            <ol>
                <li><strong>GPU (Graphics Processing Unit): </strong>
                    <ul>
                        <li>GPUs are essential for accelerating deep learning training and inference processes. They are highly parallel processors capable of performing large-scale matrix operations in parallel, which are common in deep learning computations.</li>
                        <li>NVIDIA GPUs are the most commonly used for deep learning due to their superior performance and support for popular deep learning frameworks such as TensorFlow and PyTorch.</li>
                        <li>The choice of GPU depends on budget and requirements, with higher-end GPUs offering more compute power and memory for training larger models and handling larger datasets.</li>
                    </ul>
                </li>
                <li><strong>CPU (Central Processing Unit): </strong>
                    <ul>
                        <li>While not as critical as GPUs for deep learning, CPUs are still necessary for handling various tasks such as data preprocessing, managing I/O operations, and running parts of the deep learning pipeline that are not GPU-accelerated.</li>
                        <li>Modern CPUs with multiple cores and high clock speeds can improve overall system performance and multitasking capabilities.</li>
                    </ul>
                </li>
                <li><strong>RAM (Random Access Memory): </strong>
                    <ul>
                        <li>Sufficient RAM is crucial for loading and manipulating large datasets, storing model parameters and intermediate computations during training, and running multiple processes concurrently.</li>
                        <li>The amount of RAM needed depends on the size of the datasets and models being used. Deep learning models with larger numbers of parameters and larger batch sizes require more RAM.</li>
                    </ul>
                </li>
                <li><strong>Storage: </strong>
                    <ul>
                        <li>Fast and ample storage is necessary for storing datasets, model checkpoints, training logs, and other resources.</li>
                        <li>SSDs (Solid State Drives) are preferred over HDDs (Hard Disk Drives) for faster data access and better overall performance, especially during data loading and model training.</li>
                    </ul>
                </li>
                <li><strong>Cluster/Cloud Computing: </strong>
                    <ul>
                        <li>For large-scale deep learning tasks, such as training complex models on massive datasets, distributed computing resources such as GPU clusters or cloud computing platforms may be required.</li>
                        <li>Cloud-based services like Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure offer GPU-accelerated instances and managed services for deep learning at scale.</li>
                    </ul>
                </li>
                <li><strong>Power Supply and Cooling: </strong>High-performance GPUs can consume a significant amount of power and generate a lot of heat during intensive computations. Adequate power supply and cooling solutions, such as high-wattage power supplies and efficient cooling systems, are necessary to ensure stable operation and prevent overheating.</li>
            </ol>
            Overall, the hardware requirements for deep learning depend on factors such as the complexity of the models, size of the datasets, desired training speed, and budget constraints. It's essential to carefully consider these factors when selecting hardware for deep learning tasks to ensure optimal performance and efficiency.







            <!-------Reference ------->
            <section id="reference">
                <h2>References</h2>
                <ol>
                    <li><a href="https://learnopencv.com/getting-started-with-tensorflow-keras/" target="_blank">Getting started with Keras, Tensorflow and Deep Learning</a>.</li>
                    <li><a href="https://www.geeksforgeeks.org/introduction-deep-learning/?ref=lbp" target="_blank">Introduction to Deep Learning</a></li>
                    <li><a href="https://www.ibm.com/topics/deep-learning?cm_sp=ibmdev-_-developer-articles-_-ibmcom" target="_blank">What is deep learning?</a></li>
                    <li><a href="https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures/" target="_blank">Deep learning architectures</a></li>
                    <li>Hands on Machine Learning with Scikit-Learn, Keras, & TensorFlow, Aurelien Geron</li>
                    <li><a href="https://github.com/arunp77/Machine-Learning/tree/main/Deep-learning" target="_blank">Github repo</a></li>
                </ol> 
            </section>

            <hr>
        
            <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

                <h3>Some other interesting things to know:</h3>
                <ul style="list-style-type: disc; margin-left: 30px;">
                    <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
                    <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
                </ul>
            </div>
            <p></p>

            <div class="navigation">
                <a href="index.html#portfolio" class="clickable-box">
                    <span class="arrow-left">Portfolio section</span>
                </a>
                
                <a href="machine-learning.html" class="clickable-box">
                    <span class="arrow-right">Content</span>
                </a>
            </div>
        </div>
        </section><!-- End Portfolio Details Section -->
    </main><!-- End #main --

    <!-- ======= Footer ======= -->
    <footer id="footer">
    <div class="container">
        <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
        </div>
    </div>
    </footer><!-- End  Footer -->

    <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="assets/vendor/typed.js/typed.umd.js"></script>
    <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function () {
        hljs.initHighlightingOnLoad();
    });
    </script>

</body>

</html>