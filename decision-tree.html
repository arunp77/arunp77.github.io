<!DOCTYPE html>
<html lang="en">

<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1.0" name="viewport">

<title>Decision Tree</title>
<meta content="" name="description">
<meta content="" name="keywords">

<!-- Favicons -->
<link href="assets/img/Favicon-1.png" rel="icon">
<link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

<!-- Google Fonts -->
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

<!-- Vendor CSS Files -->
<link href="assets/vendor/aos/aos.css" rel="stylesheet">
<link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
<link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
<link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
<link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
<link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
<!-- Creating a python code section-->
<link rel="stylesheet" href="assets/css/prism.css">
<script src="assets/js/prism.js"></script>

<!-- Template Main CSS File -->
<link href="assets/css/style.css" rel="stylesheet">

<!-- To set the icon, visit https://fontawesome.com/account-->
<script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
<!-- end of icon-->

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
======================================================== -->
</head>

<body>
    <!-- ======= Mobile nav toggle button ======= -->
    <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>
    <!-- ======= Header ======= -->
    <header id="header">
        <div class="d-flex flex-column">
            <div class="profile">
                <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
                <h1 class="text-light"><a href="index.html">Arun</a></h1>
                <div class="social-links mt-3 text-center">
                    <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                    <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
                    <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
                    <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
                    <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
                </div>
            </div>

            <nav id="navbar" class="nav-menu navbar">
                <ul>
                    <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
                    <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
                    <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
                    <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
                    <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
                    <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
                    <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
                    <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
                    <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
                    <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
                </ul>
            </nav><!-- .nav-menu -->
        </div>
    </header><!-- End Header -->

    <main id="main">
        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs">
        <div class="container">
            <div class="d-flex justify-content-between align-items-center">
                <h2>Machine Learning</h2>
                <ol>
                    <li><a href="machine-learning.html" class="clickable-box">Content section</a></li>
                    <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
                </ol>
            </div>
        </div>
        </section><!-- End Breadcrumbs -->

        <!------  right dropdown menue ------->
        <div class="right-side-list">
            <div class="dropdown">
                <button class="dropbtn"><strong>Shortcuts:</strong></button>
                <div class="dropdown-content">
                    <ul>
                        <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                        <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                        <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                        <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                        <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                        <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                        <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                        <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                        <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                        <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                        <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                        <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                        <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                        <!-- Add more subsections as needed -->
                    </ul>
                </div>
            </div>
        </div>

        <!-- ======= Portfolio Details Section ======= -->
        <section id="portfolio-details" class="portfolio-details">
            <div class="container">
                <div class="row gy-4">
                    <h1>Decision Tree: Classification methods</h1>
                    <div class="col-lg-8">
                        <div class="portfolio-details-slider swiper">
                            <div class="swiper-wrapper align-items-center">
                                <div class="swiper-slide">
                                    <figure>
                                        <img src="assets/img/machine-ln/classfication-decision-tree.png" alt="" style="max-width: 90%; max-height: auto;">
                                        <figcaption style="text-align: center;"></figcaption>
                                    </figure>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-4 grey-box">
                        <div class="section-title">
                            <h3>Content</h3>
                            <ol>
                                <li><a href="#introduction">Introduction</a></li>
                                <li><a href="#principle">Principle of KNN</a></li>
                                <ul>
                                    <li><a href="#distance">Distance metrics for k-nearest neighbours</a></li>
                                    <li><a href="#when">When Do We Use the KNN Algorithm?</a></li>
                                    <li><a href="#steps-to-follow">Steps to Effective K-Nearest Neighbors (KNN) Algorithm Implementation</a></li>
                                    <li><a href="#what-k">What value should you choose for k in k-nearest neighbours</a></li>
                                    <li><a href="why-knn">Why Do We Need the KNN Algorithm?</a></li>
                                    <li><a href="#pros">Pros of using KNN</a></li>
                                    <li><a href="#cons">Cons of Using KNN</a></li>
                                </ul>
                                <li><a href="#example">Example</a></li>
                                <li><a href="#reference">Reference</a></li>  
                            </ol>
                        </div>
                    </div>
                </div>

                <section>
                <!-------------------- Introduction ---------------------->
                <h2 id="introdction">Introduction</h2>
                <p>Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier consisting of a root node, branches, internal nodes, and leaf nodes, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.</p>
                <P>The decision rules are generally in form of if-then-else statements. The deeper the tree, the more complex the rules and fitter the model.</P>
                <div class="grey-box">
                    <ul>
                        <li><strong>Root Node:</strong> This is the starting point of the tree. It represents the entire dataset and is where the first decision is made. The root node corresponds to the feature that best splits the data into distinct groups, maximizing the homogeneity of the target variable within each group.</li>

                        <li><strong>Internal Nodes/Decision Nodes:</strong> These nodes represent features or attributes that are used to partition the data further. Each internal node corresponds to a decision based on a specific feature, leading to one or more branches.</li>
                        
                        <li><strong>Branches:</strong> Branches emanate from internal nodes and represent the possible outcomes of the decision based on the feature at that node. Each branch corresponds to a specific value or range of values of the feature being evaluated.</li>
                        
                        <li><strong>Leaf Nodes:</strong> These are the terminal nodes of the tree, where no further splitting occurs. Each leaf node represents a class label (in classification) or a predicted value (in regression). The decision tree algorithm aims to make the predictions at the leaf nodes as accurate as possible based on the features and their values.</li>
                    </ul>
                </div>

                <!--------------------------------->
                <!---------------Example----------->
                <!--------------------------------->
                <h4 id="example">Example</h4>
                <p>Let’s understand decision trees with the help of an dataset of last 10 days:</p>
                <table>
                    <tr>
                        <th>Day</th>
                        <th>Weather</th>
                        <th>Teperature</th>
                        <th>Humidity</th>
                        <th>Wind</th>
                        <th>Play?</th>
                    </tr>
                    <tr>
                        <td>1</td>
                        <td>Sunny</td>
                        <td>Hot</td>
                        <td>High</td>
                        <td>Weak</td>
                        <td>No</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>Cloudy</td>
                        <td>Hot</td>
                        <td>High</td>
                        <td>Weak</td>
                        <td>Yes</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>Sunny</td>
                        <td>Mild</td>
                        <td>Normal</td>
                        <td>Strong</td>
                        <td>Yes</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>Cloudy</td>
                        <td>Mild</td>
                        <td>High</td>
                        <td>Strong</td>
                        <td>Yes</td>
                    </tr>
                    <tr>
                        <td>5</td>
                        <td>Rainy</td>
                        <td>Mild</td>
                        <td>High</td>
                        <td>Strong</td>
                        <td>No</td>
                    </tr>
                    <tr>
                        <td>6</td>
                        <td>Rainy</td>
                        <td>Cool</td>
                        <td>Normal</td>
                        <td>Strong</td>
                        <td>No</td>
                    </tr>
                    <tr>
                        <td>7</td>
                        <td>Rainy</td>
                        <td>Mild</td>
                        <td>High</td>
                        <td>Weak</td>
                        <td>Yes</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>Sunny</td>
                        <td>Hot</td>
                        <td>High</td>
                        <td>Strong</td>
                        <td>No</td>
                    </tr>
                    <tr>
                        <td>9</td>
                        <td>Cloudy</td>
                        <td>Hot</td>
                        <td>Normal</td>
                        <td>Weak</td>
                        <td>Yes</td>
                    </tr>
                    <tr>
                        <td>10</td>
                        <td>Rainy</td>
                        <td>Mild</td>
                        <td>High</td>
                        <td>Strong</td>
                        <td>No</td>
                    </tr>
                </table>
                <p>Decision trees are nothing but a bunch of if-else statements in layman terms. It checks if the condition is true and if it is then it goes to the next node attached to that decision.</p>

                <p>In the diagram below, the tree will first ask what is the weather? Is it sunny, cloudy, or rainy? If yes then it will go to the next feature which is humidity and wind. It will again check if there is a strong wind or weak, if it’s a weak wind and it’s rainy then the person may go and play.</p>

                <figure>
                    <img src="assets/img/machine-ln/classfication-decision-tree-example.png" alt="" style="max-width: 90%; max-height: auto;">
                    <figcaption style="text-align: center;"></figcaption>
                </figure>
                <p></p>

                <!--------------------------------->
                <!--          Principle          -->
                <!--------------------------------->
                <h2 id="Principle">Working Principle</h2>
                <p>In a decision tree, for predicting the class of the given dataset, the algorithm starts from the root node of the tree. This algorithm compares the values of root attribute with the record (real dataset) attribute and, based on the comparison, follows the branch and jumps to the next node. For the next node, the algorithm again compares the attribute value with the other sub-nodes and move further. It continues the process until it reaches the leaf node of the tree.</p>
                <p>The algorithm works by recursively partitioning the data into subsets based on the most significant attribute at each node using the <strong>Attribute Selection Measure (ASM)</strong>. This process continues until the subsets at a node have the same target variable or reach a specified maximum depth.</p>
                <p>The working principle of a decision tree involves the following steps:</p>
                <ul>
                    <li><strong>Feature Selection:</strong> The algorithm evaluates different features in the dataset to determine the best feature that splits the data into distinct groups. It selects the feature that maximizes the homogeneity (or purity) of the target variable within each group.</li>

                    <li><strong>Splitting:</strong> After selecting the best feature, the algorithm splits the dataset into subsets based on the values of that feature. Each subset corresponds to a different branch of the tree.</li>
                    
                    <li><strong>Recursive Partitioning:</strong> This process of feature selection and splitting continues recursively for each subset until a stopping criterion is met. Common stopping criteria include reaching a maximum tree depth, having a minimum number of samples in a node, or no further improvement in purity.</li>
                    
                    <li><strong>Leaf Node Assignment:</strong> Once the recursive partitioning process reaches a stopping point, the algorithm assigns a class label (in classification) or a predicted value (in regression) to each leaf node based on the majority class or average target variable value of the samples in that node.</li>
                    
                    <li><strong>Prediction:</strong> To make predictions for new data points, the algorithm traverses the decision tree from the root node down to a leaf node, following the decision rules at each internal node based on the features of the data point. The prediction at the leaf node reached by the traversal is then assigned to the data point.</li>
                </ul>


                <!--------------------------------->
                <!--     splitting area          -->
                <!--------------------------------->
                <h3 id="splitting-criteria">Splitting criteria in Decision trees: Attribute Selection Measure (ASM)</h3>
                <p>The splitting criteria in decision trees are used to determine how the data should be partitioned at each node of the tree. Attribute Selection Measure (ASM) is a term often used in the context of decision trees, specifically regarding the selection of the best attribute to split the data at each node. ASM refers to the criterion or metric used to evaluate and rank the attributes based on their effectiveness in partitioning the data and improving the homogeneity of the resulting subsets.</p>
                <h5 id="asm">What is Attribute Selection Measure (ASM)?</h5>
                <p>ASM is a criterion or metric used in decision tree algorithms to assess the importance of different attributes in making splitting decisions. It helps determine which attribute should be chosen as the splitting criterion at each node of the decision tree. The attribute with the highest ASM score is typically selected for splitting, as it leads to more informative and discriminative splits.</p>
                <h5 id="why-asm">Why is ASM important?</h5>
                <p>ASM plays a crucial role in the construction of decision trees by guiding the algorithm in selecting the most relevant attributes for partitioning the data. By choosing attributes with high ASM scores, decision trees can effectively divide the dataset into subsets that are more homogeneous with respect to the target variable. This, in turn, leads to the creation of accurate and interpretable decision tree models.</p>
                <h5 id="common-asm">Common ASM Methods:</h5>
                <p>Several methods exist for calculating ASM, each with its own strengths and considerations. Some common ASM methods include:</p>
                <ul>
                    <li><strong>Information Gain (Entropy):</strong> Information gain measures the reduction in entropy (or uncertainty) achieved by splitting the data based on a particular feature. The goal is to select the feature that maximizes information gain, thereby improving the purity of the resulting subsets. Higher information gain implies better separation of classes or reduced randomness in the subsets.
                    <p><strong>Working principle:</strong>The decision tree algorithm evaluates each feature and calculates the entropy of the dataset before and after splitting based on that feature. Information gain is then computed as the difference between the initial entropy and the weighted sum of entropies of the resulting subsets.</p>
                    <p><strong>Formulation:</strong>Entropy is a measure of randomness or uncertainty in a dataset. Mathematically, it is defined as:</p>
                    $$H(X) = - \sum_{i=1}^n p_i \log_2(p_i)$$
                    where:
                    <ul>
                        <li>\(H(X)\) is the entropy of the dataset \(X\).</li>
                        <li>\(p_i\) is the probability of class \(i\) in the dataset.</li>
                        <li>\(n\) is the total number of classes.</li>
                    </ul>
                    <p>Information gain is used to select the best feature for splitting the data in decision trees. It measures the reduction in entropy (or increase in purity) achieved by splitting the data based on a particular feature. Mathematically, information gain is calculated as:</p>
                    $$\text{Information Gain} = H(X) - \sum_{j=1}^m \frac{N_j}{N} H(X_j).$$
                    where:
                    <ul>
                        <li>\(H(X)\) is the entropy of the original ataset \(X\).</li>
                        <li>\(m\) is the number of subsets after splitting based on the feature.</li>
                        <li>\(N_j\) is the number of samples in subset \(i\).</li>
                        <li>\(N\) is the total number of samples in the original dataset.</li>
                        <li>\(H(X_j)\) is the entropy of subset \(j\).</li>
                    </ul>
                    <p><strong>Interpreation:</strong>Higher information gain indicates that splitting based on a certain feature leads to more homogeneous subsets with respect to the target variable. In other words, it signifies that the feature provides more discriminatory power for classification.</p>
                    <p><strong>Application:</strong>  Information gain is commonly used in decision tree algorithms such as ID3 (Iterative Dichotomiser 3) and C4.5 (successor to ID3) for building classification trees.</p>
                    </li>
                    <li><strong>Gini Index (Impurity):</strong> Gini impurity measures the probability of misclassifying a randomly chosen sample if it were labeled according to the class distribution in the dataset. The goal is to select the feature that minimizes Gini impurity, thereby improving the homogeneity of the resulting subsets. Lower Gini impurity implies purer subsets with fewer mixed-class samples.
                    <p><strong>Working Principle:</strong> Similar to information gain, the decision tree algorithm evaluates each feature and calculates the Gini impurity of the dataset before and after splitting based on that feature. The feature with the lowest Gini impurity (or highest purity) is selected for splitting. Mathematically, Gini impurity is calculated as:</p>
                    $$G(X) = 1- \sum_{i=1}^n p_i^2$$
                    where:
                    <ul>
                        <li>\(G(X)\) is the Gini impurity of the dataset \(X\).</li>
                        <li>\(p_i\) is the probability of class \(i\) in the dataset.</li>
                        <li>\(n\) is the total number of classes.</li>
                    </ul>
                    <p><strong>Interpretation:</strong>Lower Gini impurity indicates that splitting based on a certain feature leads to more homogeneous subsets with respect to the target variable. It signifies that the feature effectively separates the classes in the dataset.</p>
                    <p><strong>Application:</strong>Gini impurity is commonly used in decision tree algorithms such as CART (Classification and Regression Trees) for building both classification and regression trees.</p>
                    </li>
                    <li><strong>Gain Ratio:</strong> Gain Ratio is a modification of Information Gain that aims to overcome its bias towards attributes with a large number of distinct values. It penalizes attributes with many distinct values, thereby helping to prevent overfitting.
                    <p><strong>Formula:</strong></p>
                    $$\text{Gain Ratio} = \frac{\text{Information Gain}}{\text{Split Information}}$$
                    <p><strong>Explanation:</strong>Gain Ratio adjusts Information Gain by considering the intrinsic information of each attribute. It divides the Information Gain by the Split Information to normalize the gain by the attribute's intrinsic information. This normalization helps in avoiding the bias towards attributes with many distinct values.</p>
                    </li>
                    <li><strong>Chi-Square Test:</strong>Chi-Square Test evaluates the independence between attributes and the target variable by comparing the observed distribution of class labels in each subset to the expected distribution. It helps determine whether the splits based on a particular attribute are statistically significant.
                    <p><strong>Formula:</strong>The Chi-Square Test statistic is calculated as follows:</p>
                    $$\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}$$
                    where:
                    <ul>
                        <li>\(\chi^2\) is the Chi-Square test statistic.</li>
                        <li>\(O_i\) is the observed frequency of class \(i\) in the subset.</li>
                        <li>\(E_i\) is the expected frequency of class \(i\) based on the overall distribution.</li>
                    </ul>
                    <p><strong>Explanation:</strong> Chi-Square Test compares the observed frequencies of class labels in each subset to the expected frequencies based on the overall distribution. A higher Chi-Square Test statistic indicates a greater difference between the observed and expected frequencies, suggesting that the splits based on the attribute are more informative and significant.</p>
                    </li>
                </ul>
                <p>Each of these methods evaluates attributes based on different criteria, such as the reduction in entropy, impurity, or independence between attributes and the target variable. The choice of ASM method depends on the specific characteristics of the dataset and the problem being addressed.</p>
                <h5 id="application-asm">Application of ASM</h5>
                <p>ASM is widely used in decision tree algorithms such as ID3, C4.5, and CART for attribute selection. These algorithms leverage ASM to determine the optimal splitting criteria at each node, leading to the creation of decision trees that effectively capture the underlying patterns and relationships in the data.</p>

                <p>In conclusion, Attribute Selection Measure (ASM) is a fundamental concept in decision tree algorithms, guiding the selection of attributes for splitting the data at each node. By evaluating and ranking attributes based on their effectiveness in partitioning the data, ASM helps construct decision trees that are accurate, interpretable, and well-suited for a variety of machine learning tasks.</p>
                















                </section>

                <!----------- Reference ----------->
                <section id="reference">
                <h2>References</h2>
                <ul>
                    <li><a href="https://arunp77.github.io/logistic-regression.html#con-mat" target="_blank">Confusion matrix details</a>.</li>
                    <li>My github Repositories on Remote sensing <a href="https://github.com/arunp77/Machine-Learning/" target="_blank">Machine learning</a></li>
                    <li><a href="https://mlu-explain.github.io/linear-regression/" target="_blank">A Visual Introduction To Linear regression</a> (Best reference for theory and visualization).</li>
                    <li>Book on Regression model: <a href="https://avehtari.github.io/ROS-Examples/" target="_blank">Regression and Other Stories</a></li>
                    <li>Book on Statistics: <a href="https://hastie.su.domains/Papers/ESLII.pdf" target="_blank">The Elements of Statistical Learning</a></li>
                    <li><a href="https://www.javatpoint.com/machine-learning-naive-bayes-classifier" target="_blank">Naïve Bayes Classifier Algorithm, JAVAPoint.com</a></li>
                    <li><a href="https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf">https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf</a></li>
                    <li><a href="https://datahacker.rs/002-machine-learning-linear-regression-model/" target="_blank">One of the best description on Linear regression</a>.</li>
                </ul>
                </section>

                <hr>
            
                <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">
                    <h3>Some other interesting things to know:</h3>
                    <ul style="list-style-type: disc; margin-left: 30px;">
                        <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
                        <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
                    </ul>
                </div>
                <p></p>

                <div class="navigation">
                    <a href="index.html#portfolio" class="clickable-box">
                        <span class="arrow-left">Portfolio section</span>
                    </a>
                    
                    <a href="machine-learning.html" class="clickable-box">
                        <span class="arrow-right">Content</span>
                    </a>
                </div>
            </div>
        </section><!-- End Portfolio Details Section -->
    </main><!-- End #main --

    <!-- ======= Footer ======= -->
    <footer id="footer">
    <div class="container">
        <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
        </div>
    </div>
    </footer><!-- End  Footer -->

    <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="assets/vendor/typed.js/typed.umd.js"></script>
    <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function () {
        hljs.initHighlightingOnLoad();
    });
    </script>

</body>
</html>