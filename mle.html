<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>MLE</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

    <!-- ======= Header ======= -->
    <header id="header">
    <div class="d-flex flex-column">
        <div class="profile">
            <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
            <h1 class="text-light"><a href="index.html">Arun</a></h1>
            <div class="social-links mt-3 text-center">
                <a href="https://www.linkedin.com/in/arunp77/" target="_blank" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                <a href="https://github.com/arunp77" target="_blank" class="github"><i class="bx bxl-github"></i></a>
                <a href="https://twitter.com/arunp77_" target="_blank" class="twitter"><i class="bx bxl-twitter"></i></a>
                <a href="https://www.instagram.com/arunp77/" target="_blank" class="instagram"><i class="bx bxl-instagram"></i></a>
                <a href="https://arunp77.medium.com/" target="_blank" class="medium"><i class="bx bxl-medium"></i></a>
            </div>
        </div>

        <nav id="navbar" class="nav-menu navbar">
            <ul>
                <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
                <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
                <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
                <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
                <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
                <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
                <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
                <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
                <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
                <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
            </ul>
        </nav><!-- .nav-menu -->
    </div>
    </header><!-- End Header -->

    <main id="main">
        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs"> 
          <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
              <h2>Machine learning</h2>
              <ol>
                <li><a href="machine-learning.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
              </ol>
            </div>
    
          </div>
        </section><!-- End Breadcrumbs -->

        <!------  right dropdown menue ------->
        <div class="right-side-list">
            <div class="dropdown">
                <button class="dropbtn"><strong>Shortcuts:</strong></button>
                <div class="dropdown-content">
                    <ul>
                        <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                        <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                        <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                        <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                        <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                        <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                        <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                        <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                        <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                        <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                        <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                        <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                        <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                            <!-- Add more subsections as needed -->
                    </ul>
                </div>
            </div>
        </div>

        <!-- ======= Portfolio Details Section ======= -->
        <section id="portfolio-details" class="portfolio-details">
        <div class="container">
            <div class="row gy-4">
                <h1>Maximum Likelihood Estimation (MLE)</h1>
                <div class="col-lg-8">
                    <div class="portfolio-details-slider swiper">
                        <div class="swiper-wrapper align-items-center"> 
                            <figure>
                            <img src="assets/img/machine-ln/mle-logo.png" alt="" style="max-width: 50%; max-height: auto;">
                            <figcaption></figcaption>
                            </figure>
                        </div>
                        <div class="swiper-pagination"></div>
                    </div>
                </div>
            </div>

            <section>
            <h3 id="introduction"><strong>Introduction</strong></h3>
            <p>In our study of random variables, we've encountered various distributions, each characterized by parametersâ€”numbers that define the distribution's behavior. Typically, we either have these parameter values explicitly provided or can infer them from our understanding of the data-generating process.</p>

            <p>However, what if we lack knowledge of these parameters and can't deduce them from our expertise? What if, instead of knowing the random variables themselves, we possess numerous instances of data generated from the same underlying distribution?</p>

            <p>Here, we delve into formal methodologies for estimating parameters based on data. These techniques are pivotal in artificial intelligence, as they underpin the workings of modern machine learning algorithms.</p>

            <p><strong>Parameters</strong></p>
            <p>Before delving into parameter estimation, let's refresh our understanding of parameters. In a given model, parameters are the numerical values that define the actual distribution. For instance, in a Bernoulli random variable, the parameter is a single value denoted as \( p \). In the case of a Uniform random variable, parameters are represented as \( a \) and \( b \), defining the minimum and maximum values. Here's a summary of various random variables and their corresponding parameters. Going forward, we'll denote the parameters collectively as \( \theta \).</p>

            <table>
                <thead>
                  <tr>
                    <th>Distribution</th>
                    <th>Parameters</th>
                    <th>Forumla</th>
                    <th>Explanation</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Bernoulli (\(p\))</td>
                    <td>\(\theta = p\)</td>
                    <td>\(P(X= k) = p^k (1-p)^{1-k}\)</td>
                    <td>This function gives the probability of getting a particular outcome \( k \) (0 or 1) in a single Bernoulli trial with parameter \( p \).</td>
                  </tr>
                  <tr>
                    <td>Poisson (\(\lambda\))</td>
                    <td>\(\theta = \lambda \)</td>
                    <td>\(P(X=k) = \frac{e^{-\lambda } \lambda^k}{k!}\)</td>
                    <td>This function calculates the probability of observing \( k \) events in a fixed interval of time or space, given the average rate of occurrence \( \lambda \).</td>
                  </tr>
                  <tr>
                    <td>Uniform (\(a, b\))</td>
                    <td>\(\theta = a, b\)</td>
                    <td>\(f(x) = \frac{1}{b-a}\)</td>
                    <td>This function represents a uniform distribution between the values \( a \) and \( b \), where all values within this range are equally likely.</td>
                  </tr>
                  <tr>
                    <td>Normal (\(\mu, \sigma^2\))</td>
                    <td>\(\theta = (\mu, \sigma^2)\)</td>
                    <td>\(f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</td>
                    <td>This function describes the probability density of a continuous random variable following a normal distribution with mean \( \mu \) and variance \( \sigma^2 \).</td>
                  </tr>
                  <tr>
                    <td>\(Y = mX +b\)</td>
                    <td>\(\theta = (m, b)\)</td>
                    <td>\(\)</td>
                    <td>Not a probability distribution function, but a linear equation representing a straight line with slope \( m \) and y-intercept \( b \).</td>
                  </tr>
                </tbody>
            </table>

            <p>In real-world scenarios, the true parameters of a model are often unknown, but we have the opportunity to observe data. In the following sections, we'll delve into methodologies for utilizing data to estimate these model parameters.</p>
            <p>It turns out there isn't just one way to estimate the value of parameters. There are two main approaches: </p>
            <ol>
                <li>Maximum Likelihood Estimation (MLE) and </li>
                <li>Maximum A Posteriori (MAP)</li>
            </ol>
            <p>Both MLE and MAP approaches rely on the assumption that your data consist of IID (independently and identically distributed) samples, denoted as \( X_1, X_2, \ldots, X_n \), where each \( X_i \) is independent and drawn from the same distribution.</p>

            
            <!---------------------------->
            <h3 id="mle-fundamentals">Maximum Likelihood Estimation (MLE)</h3>
            <p>Maximum Likelihood Estimation (MLE) is a statistical method used to estimate the parameters of a statistical model. The fundamental idea behind MLE is to find the values of the model parameters \(\theta\) that maximize the likelihood of observing the given data. In other words, MLE seeks the parameter values that make the observed data most probable under the assumed statistical model.</p>

            <p><strong>Key concepts of MLE:</strong></p>
            <ol>
                <li><strong>Likelihood Function:</strong>
                    The likelihood function is defined as the product of the probability density functions for each observation.
                <ul>
                    <li>The likelihood function, denoted as \(L(\theta)\), is a measure of how well the model explains the observed data for a given set of parameter \(\theta\).</li>
                    <li>It is defined as the joint probability (or probability density) of the observed data given the parameters:
                    $$L(\theta) = P(D | \theta),$$
                    where \(D\) is the observed data.
                    </li>
                </ul>
                </li>
                <li><strong>Log-Likelihood Function: </strong>
                <ul>
                    <li>In practice, it is common to work with the log-likelihood function \(l(\theta)\), which is the natural logrithm of the likelihood function.</li>
                    <li>Maximizing the log-likekihood is equivalent to Maximizing the likelihood, and it simplifies computations.</li>
                </ul>
                </li>
                <li><strong>Parameter Estimation: </strong>
                <ul>
                    <li>The goal of MLE is to find the values of the model parameters that maximize the likelihood (for log-likelihood) function.</li>
                    <li>Mathematically, this is expressed as 
                    $$\hat{\theta}_{\text{MLE}} = \text{arg max}_\theta ~~L(\theta),$$ 
                    or 
                    $$\hat{\theta}_{\text{MLE}} = \text{arg max}_\theta ~~l(\theta).$$
                    </li>
                </ul>
                </li>
                <li><strong>Optimization: </strong>
                <ul>
                    <li>Typically, optimization algorithms (such as gradient descent, Newton's method, or other numerical optimization techniques) are employed to find the maximum of the likelihood function.</li>
                    <li>In some cases, analytical solutions exist, making it possible to find the values of the parameters directly.</li>
                </ul>
                </li>
            </ol>

            <strong>Example:</strong> Consider a simple example of coin flipping, where we want to estimate the probability of getting heads. If we observe a sequence of coin flips (data),
            the likelihood function would be the product of the probabilities of obtaining the observed outcomes (head or tails) under the assumed 
            probability parameter \(\theta\) that maximizes this likelihood.
            <br>
            <strong>Mathematical Notation:</strong>
            <p>Likelihood FUnction: \(L(\theta) = P(D | \theta)\)</p>
            <p>Log-Likelihood FUnction: \(l(\theta) = \text{log}L(\theta)\).</p>
            
            In summary, Maximum Likelihood Estimation is a powerful method for estimating the parameters of a statistical model by maximizing the likelihood (or log-likelihood) of the observed data. It is widely used in various fields, including statistics, machine learning, and econometrics.

            <p><strong>Likelihood function for linear regression</strong></p>
            The likelihood function for linear regression is based on the assumption that the errors (\(\epsilon\)) are normally distributed. Therefore, the likelihood funstion is expressed as the product of the probabilities 
            of observing the actual data points given the parameters \(\theta_0, \theta_1,..., \theta_n\). 
            $$L(\theta) = \Pi_{i=1}^n \frac{1}{\sqrt{\pi \sigma^2}} \text{exp}\left(-\frac{y^{(i)} - h_\theta(x^{(i)})}{2\sigma^2}\right)$$
            
            where:
            <ul>
                <li>n is the number of data points</li>
                <li>\((x_i, y^{(i)})\) are the observed data pairs</li>
                <li>\(\sigma^2\) is the variance of the error term.</li>
            </ul>

            <p><strong>Log-Likelihood Function:</strong></p>
            It is defined as log of the likelihood function defined above. 

            <p><strong>MLE Objectives:</strong> The objective of MLE is to find the values of the coefficients that maximize the log-likelihood function:</p>
            $$\hat{\theta}_0, \hat{\theta}_1, ..., \hat{\theta}_n = \text{arg}~\text{max}_{\theta_0, \theta_1, ..., \theta_n} l(\theta_0, \theta_1, ..., \theta_n)$$
            
            <p><strong>Optimization:</strong> Optimization techniques, such as gradient descent or analytical solutions (set partial derivatives to zero), are used to find the values of the \(\hat{\theta}_0, \hat{\theta}_1, ..., \hat{\theta}_n\)
            that mazimize the log-likelihood. 
            </p>

            <p>In practice, the MLE estimates for the \(\theta_0, \theta_1, ..., \theta_n\) are often equivalent to the least squares estimates obtained through the minimizing the sum of squaresd residuals.</p>        
            <br>

            <h3>Example:</h3>
            Let's consider a simple example: flipping a coin. Assume you have a biased coin, and you want to estimate the probability of getting heads (H). Your model parameter is the probability of heads, denoted as "p."
            <ul>
                <li><strong>Define the Likelihood Function: </strong>The likelihood function is the probability of observing your data given the parameter. For a coin flip, it follows a Bernoulli distribution. The likelihood function is then: 

                    $$L(p) = p^k * (1-p)^{n-k} $$
    
                where \(k\) is the number of heads observed, \(n\) is the total number of coin flips.</li>
                <li><strong>Maximize the Likelihood: </strong>Take the logarithm of the likelihood function (log-likelihood) for easier calculations:

                    $$ \log L(p) = k \log(p) + (n-k) \log(1-p) $$
        
                    To find the MLE, differentiate the log-likelihood with respect to \(p\), set it to zero, and solve for \(p\).</li>
                <li><strong>Simple coin tossing example: </strong>Suppose you flip the coin 10 times and observe 7 heads (\(k=7\)). The likelihood function becomes:
                    $$ L(p) = p^7 * (1-p)^3 $$
                    Taking the log-likelihood:
                    $$ \log L(p) = 7 \log(p) + 3 \log(1-p) $$

                    Differentiate and set to zero:
        
                    $$ \frac{d}{dp} \log L(p) = \frac{7}{p} - \frac{3}{1-p} = 0 \Rightarrow p = \frac{7}{10} $$
                    In this way, MLE helps find the parameter value (probability of heads in this case) that maximizes the likelihood of observing the given data (7 heads in 10 coin flips).
                </li>
            </ul>
            
            <!------------------------->
            <h3> Maximum Likelihood Estimation (MLE) in Python</h3>
            <pre class="language-python"><code>
                import numpy as np
                import pandas as pd
                import matplotlib.pyplot as plt
                from scipy.optimize import minimize

                # Generate synthetic data for linear regression
                np.random.seed(42)

                true_slope = 2
                true_intercept = 5

                x = np.linspace(0, 10, 100)
                y_true = true_slope * x + true_intercept

                noise = np.random.normal(0, 2, size=len(x))

                y_observed = y_true + noise

                # Create a DataFrame
                df = pd.DataFrame({'X': x, 'Y': y_observed})
                df.head()
            </code></pre>
            The log-likelihood function for linear regression with normally distributed errors is typically expressed as follows:
            $$\text{Log-liklihood} = -\frac{1}{2} \sum_{i=1}^n \left(\frac{(y_i - (m x_i +b))^2}{\sigma^2} +\text{log}(2 \pi \sigma^2)\right)$$

            where:
            <ul>
                <li>`n` is the number of observations.</li>
                <li>\(y_i\)   is the observed response for the i-th observation, and </li>
                <li>\(x_i\) is the corresponding predictor variable for the i-th observations</li>
                <li>\(m\) is the slope of regression line </li>
                <li>\(b\)  is the intercept of the regression line</li>
                <li>\(\sigma\) is the standard deviation of the normally distributed errors.</li>
            </ul>
            <pre><code class="language-python">
                # Define the likelihood function for linear regression
                def linear_regression_likelihood(params, data):
                    slope, intercept, sigma = params
                    y_pred = slope * data['X'] + intercept
                    residuals = data['Y'] - y_pred
                    ll = -0.5 * np.sum((residuals / sigma) ** 2 + np.log(2 * np.pi * sigma ** 2))
                    return -ll  # Negative log-likelihood for minimization
                
                # Initial guess for parameters
                initial_params = [1, 1, 1]
                
                # Use scipy's minimize function to find MLE for linear regression
                result = minimize(linear_regression_likelihood, initial_params, args=(df,), method='L-BFGS-B')
                
                # Extract MLE estimates
                estimated_slope, estimated_intercept, estimated_sigma = result.x
                
                # Plot the results
                plt.scatter(df['X'], df['Y'], label='Observed Data')
                plt.plot(x, y_true, label='True Regression Line', color='red', linestyle='--')
                plt.plot(x, estimated_slope * x + estimated_intercept, label='Estimated Regression Line', color='green', linestyle='--')
                plt.legend()
                plt.title('Linear Regression with MLE')
                plt.show()
                
                # Print results
                print(f"True Slope: {true_slope}, True Intercept: {true_intercept}")
                print(f"Estimated Slope: {estimated_slope:.4f}, Estimated Intercept: {estimated_intercept:.4f}, Estimated Noise Sigma: {estimated_sigma:.4f}")
            </code></pre>
            <figure>
                <img src="assets/img/machine-ln/mle-plot.png" alt="" style="max-width: 90%; max-height: auto;">
                <figcaption style="text-align: center;">Linear regression</figcaption>
            </figure>


            </section>

            <!-------Reference ------->
            <section id="reference">
                <h2>References</h2>
                <ol>
                    <li><a href="Linear-reg.html" target="_blanck">For linear regression model</a>.</li>
                    <li><a href="Linear-Parameter-estimation.html">Linear-Parameter-estimation</a></li>
                    <li>For more details on supervised regression models, please check <a href="machine-learning.html">link</a>.</li>
                </ol> 
            </section>

            <hr>
        
            <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

                <h3>Some other interesting things to know:</h3>
                <ul style="list-style-type: disc; margin-left: 30px;">
                    <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
                    <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
                </ul>
            </div>
            <p></p>

            <div class="navigation">
                
                <a href="index.html#portfolio" class="clickable-box">
                    <span class="arrow-left">Portfolio section</span>
                </a>
                
                <a href="machine-learning.html" class="clickable-box">
                    <span class="arrow-right">Content</span>
                </a>

            </div>
        </div>
        </section><!-- End Portfolio Details Section -->
    </main><!-- End #main --

    <!-- ======= Footer ======= -->
    <footer id="footer">
    <div class="container">
        <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
        </div>
    </div>
    </footer><!-- End  Footer -->

    <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="assets/vendor/typed.js/typed.umd.js"></script>
    <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function () {
        hljs.initHighlightingOnLoad();
    });
    </script>

</body>

</html>