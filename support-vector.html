<!DOCTYPE html>
<html lang="en">

<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1.0" name="viewport">

<title>SVM</title>
<meta content="" name="description">
<meta content="" name="keywords">

<!-- Favicons -->
<link href="assets/img/Favicon-1.png" rel="icon">
<link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

<!-- Google Fonts -->
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

<!-- Vendor CSS Files -->
<link href="assets/vendor/aos/aos.css" rel="stylesheet">
<link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
<link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
<link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
<link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
<link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
<!-- Creating a python code section-->
<link rel="stylesheet" href="assets/css/prism.css">
<script src="assets/js/prism.js"></script>

<!-- Template Main CSS File -->
<link href="assets/css/style.css" rel="stylesheet">

<!-- To set the icon, visit https://fontawesome.com/account-->
<script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
<!-- end of icon-->

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
======================================================== -->
</head>

<body>

<!-- ======= Mobile nav toggle button ======= -->
<i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

<!-- ======= Header ======= -->
<header id="header">
<div class="d-flex flex-column">

    <div class="profile">
    <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
    <h1 class="text-light"><a href="index.html">Arun</a></h1>
    <div class="social-links mt-3 text-center">
        <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
        <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
        <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
        <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
        <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
    </div>
    </div>

    <nav id="navbar" class="nav-menu navbar">
    <ul>
        <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
        <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
        <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
        <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
        <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
        <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
        <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
        <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
        <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
        <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
    </ul>
    </nav><!-- .nav-menu -->
</div>
</header><!-- End Header -->

<main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
    <div class="container">

    <div class="d-flex justify-content-between align-items-center">
        <h2>Machine Learning</h2>
        <ol>
        <li><a href="machine-learning.html" class="clickable-box">Content section</a></li>
        <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
        </ol>
    </div>

    </div>
    </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
    <div class="dropdown">
        <button class="dropbtn"><strong>Shortcuts:</strong></button>
        <div class="dropdown-content">
            <ul>
                <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                <!-- Add more subsections as needed -->
            </ul>
        </div>
    </div>
    </div>

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
    <div class="container">
    <div class="row gy-4">
        <h1>Support Vector Algorithm: Classification methods</h1>
        <div class="col-lg-8">
        <div class="portfolio-details-slider swiper">
            <div class="swiper-wrapper align-items-center">

                <figure>
                    <img src="assets/img/machine-ln/classification-svm.png" alt="" style="max-width: 90%; max-height: auto;">
                    <figcaption style="text-align: center;"></figcaption>
                </figure>

            </div>
        </div>
    </div>

    <div class="col-lg-4 grey-box">
        
        <div class="section-title">
        <h3>Content</h3>
        <ol>
            <li><a href="#introduction">Introduction</a>
                <ul>
                    <li><a href="#explanation-1">Example 1: Support-vector-machine explanation: 2D</a></li>
                    <li><a href="#explanantion-2">Example 2: Classification problem with higher dimension data</a></li>
                    <li><a href="#types-svm">Types of Support Vector Machine (SVM) Algorithms</a></li>
                    <li><a href="#terminology">Important terminologies</a></li>
                    <li><a href="#application">Useful application of SVM</a></li>
                    <li><a href="#intuition">Intuition: example</a></li>
                    <li><a href="#pros">Pros</a></li>
                <li><a href="#cons">Cons</a></li>
                </ul>
            </li>
            <li><a href="#math-svm">Mathematical intuition behind the Support Vector Machine</a></li>
            <ul>
                <li><a href="#kernel-fun">Kernel Functions</a></li>
                <li><a href="#purpose-kernel">Purpose of Kernel Functions</a></li>
                <li><a href="#types-kernel">Types of Kernel Functions</a></li>
            </ul>
            <li><a href="#reference">Reference</a></li>  
        </ol>
        </div>
    </div>
    </div>

    <section>
    <!-------------------- Introduction ---------------------->
    <h2 id="introdction">Introduction</h2>
    <ul>
        <li>Support Vector Machine (SVM) is undoubtedly one of the most popular ML algorithms used by machine learning practitioners. It is a supervised machine learning algorithm that is robust to outliers and generalizes well in many cases. However, the intuitive idea behind SVM can be a bit tricky to understand for a beginner. The name in itself is quite intimidating, Support, Vector, and Machine.</li>
        <li>It is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning.</li>
        <li>In this algorithm, we try to find a hyperplane that best separates the two classes. It is to be noted that, it may seems that the SVM and logisitc regression are similar. Both the algorithms try to find the best hyperplane, but the main difference is logistic regression is a probabilistic approach whereas support vector machine is based on statistical approaches.</li>
        <li>Now the question is which hyperplane does it select? There can be an infinite number of hyperplanes passing through a point and classifying the two classes perfectly. So, which one is the best? Well, SVM does this by finding the maximum margin between the hyperplanes that means maximum distances between the two classes.</li>
    </ul>
    <div class="grey-box">
        <p><strong>Logistic Regression vs Support Vector Machine (SVM): </strong>
        Depending on the number of features you have you can either choose Logistic Regression or SVM. SVM works best when the dataset is small and complex. It is usually advisable to first use logistic regression and see how does it performs, if it fails to give a good accuracy you can go for SVM without any kernel. Logistic regression and SVM without any kernel have similar performance but depending on your features, one may be more efficient than the other.</p>
        <table>
            <tr>
                <th>Feature</th>
                <th>Logisitc Regression</th>
                <th>Support Vecotr Machine (SVM)</th>
            </tr>
            <tr>
                <td>Type</td>
                <td>Discriminative model</td>
                <td>Discriminative model</td>
            </tr>
            <tr>
                <td>Decision Boundary</td>
                <td>Linear or Non-linear</td>
                <td>Linear or Non-linear</td>
            </tr>
            <tr>
                <td>Interpretability</td>
                <td>Easy to interpret coefficients</td>
                <td>Less interpretable due to complex decision boundaries</td>
            </tr>
            <tr>
                <td>Training Speed</td>
                <td>Faster</td>
                <td>Slower, especially with large datasets</td>
            </tr>
            <tr>
                <td>Regularization</td>
                <td>L1 or L2 regularization</td>
                <td>Regularization via margin parameter (C)</td>
            </tr>
            <tr>
                <td>Handling Noise</td>
                <td>Sensitive to noisy data</td>
                <td>More robust to noisy data</td>
            </tr>
            <tr>
                <td>Scalability</td>
                <td>Suitable for large datasets</td>
                <td>Can be computationally expensive with large datasets</td>
            </tr>
            <tr>
                <td>Performance on Small Data</td>
                <td>May underperform if features are not linearly separable</td>
                <td>Can handle non-linear data well, even with small datasets</td>
            </tr>
            <tr>
                <td>Parameters to Tune</td>
                <td>Regularization strength, threshold</td>
                <td>Regularization parameter (C), kernel function, gamma</td>
            </tr>
            <tr>
                <td>Application Areas</td>
                <td>Commonly used in binary classification tasks and probability estimation</td>
                <td>Widely used in classification tasks with non-linear decision boundaries</td>
            </tr>
            <tr>
                <td>Implementation</td>
                <td>Available in most machine learning libraries</td>
                <td>Available in most machine learning libraries</td>
            </tr>
            
        </table>
    </div>

    <!--------------------------------->
    <h5 id="explanation-1">Example-1: Support-vector-machine explanation: 2D</h5>
    <p>Imagine you have a scatter plot where the green points represent one class and the orange points represent another class. To separate these two classes, an SVM seeks to find the best-fitting line or plane (hyperplane) that maximizes the margin between the two classes.</p>
    <figure>
        <img src="assets/img/machine-ln/support-vector-machine.png" alt="" style="max-width: 40%; max-height: auto;">
        <figcaption style="text-align: center;">Support vectors in circle (<strong>Image credit:</strong> &#169; <a href="index.html">@arunp77</a>)</figcaption>
    </figure>
    <p>The next step is to draw a decision boundary, or a line separating the two classes to help classify the new data points. We  can actually draw several boundaries, as shown above. Then, we need to find the line of best fit that clearly separates those two groups. The correct line will help us classify the new data point. <strong><em>We can find the best line by computing the maximum margin from equidistant support vectors</em></strong>. The support vectors are the data points closest to this separating hyperplane. These points play a critical role because they help determine the position and orientation of the hyperplane. By choosing support vectors strategically, the SVM ensures that the margin between the two classes is maximized. Now, let's talk about the margin. The margin is the distance between the support vectors and the hyperplane. Maximizing this margin is essential because it provides a buffer zone, making the classification more robust to noise and outliers. Essentially, the larger the margin, the more confident we can be in the classification. However, not all data points are equal in determining the hyperplane and margin. The SVM assigns different weights to each data point based on their proximity to the hyperplane. The farther a data point is from the hyperplane, the less influence it has on determining the hyperplane's position.</p>
    <p>So in summary when using an SVM to classify data points into different classes:</p>
    <ul>
        <li>Support vectors are chosen as the data points closest to the separating hyperplane.</li>
        <li>The margin, or the distance between the support vectors and the hyperplane, is maximized to ensure robust classification.</li>
        <li>Each data point's influence on determining the hyperplane's position is weighted based on its proximity to the hyperplane.</li>
    </ul>

    <h5 id="explanantion-2">Example 2: Classification problem with higher dimension data</h5>
    <p>The data set shown below in Image-(a) has no clear linear separation between the two classes. In machine learning parlance, we would say that these are not linearly separable. How can we get the support vector machine to work on such data? </p>
    <figure>
        <img src="assets/img/machine-ln/support-vector-machine-3D.png" alt="" style="max-width: 80%; max-height: auto;">
        <figcaption style="text-align: center;">Support vectors in circle (<strong>Image credit:</strong> &#169; <a href="index.html">@arunp77</a> )</figcaption>
    </figure>
    <p>Since we can't separate it into two classes using a line, we need to transform it into a higher dimension by employing a kernel function to the data set. A higher dimension enables us to clearly separate the two groups with a plane. Here, we can draw some planes between the green dots and the orange dots — with the end goal of maximizing the margin. If we let R=the number of dimensions, the kernel function will convert a two-dimensional space (R2) to a three-dimensional space (R3). Once the data is separated into three dimensions, we can apply SVM and separate the two groups using a two-dimensional plane. For a higher dimension, we would have to use higher diemntsional curve. </p>

    <!--------------------------------->
    <h3 id="types-svm">Types of Support Vector Machine (SVM) Algorithms</h3>
    <p>Support Vector Machines (SVMs) offer various algorithms for classification and regression tasks, primarily distinguished by the types of decision boundaries they create and the techniques they employ for optimization. Here are some common types of SVM algorithms:</p>
    <ol>
        <li><strong>Linear SVM:</strong> This algorithm is used when the data is linearly separable. Perfectly linearly separable means that the data points can be classified into 2 classes by using a single straight line(if 2D). It aims to find the optimal hyperplane that separates the classes with the maximum margin.</li>

        <li><strong>Non-linear SVM:</strong> When the data is not linearly separable then we can use Non-Linear SVM, which means when the data points cannot be separated into 2 classes by using a straight line (if 2D) then we use some advanced techniques like kernel tricks to classify them. In most real-world applications we do not find linearly separable datapoints hence we use kernel trick to solve them.</li>
        
        <li><strong>Support Vector Regression (SVR):</strong> Unlike classification tasks, SVR is used for regression tasks. It aims to find the optimal hyperplane that best fits the data while minimizing the margin violations.</li>
        
        <li><strong>Nu-SVM:</strong> This algorithm is an extension of the traditional SVM, introducing a parameter "nu" that replaces the regularization parameter "C." It offers a more intuitive control over the number of support vectors and margin errors.</li>
        
        <li><strong>One-Class SVM:</strong> Primarily used for anomaly detection, this algorithm learns a decision boundary that encompasses the majority of the data points while treating the rest as outliers.</li>
        
        <li><strong>Sequential Minimal Optimization (SMO):</strong> SMO is an algorithm for training SVMs, particularly useful for solving large-scale optimization problems by decomposing them into smaller sub-problems.</li>
        
        <li><strong>Least Squares SVM (LS-SVM):</strong> This variant of SVM reformulates the optimization problem as a set of linear equations, making it computationally efficient for large-scale datasets.</li>
        
        <li><strong>Budgeted SVM:</strong> Designed for scenarios with limited computational resources, this algorithm dynamically selects a subset of support vectors to reduce computational complexity while maintaining classification accuracy.</li>
    </ol>

    <!---------------------------------------->
    <h3 id="terminology">Important terminologies</h3>
    <p>Here are some important terms in the context of Support Vector Machines (SVM):</p>
    <ul>
        <li><strong>Hyperplane:</strong> In SVM, a hyperplane is a decision boundary that separates classes in a feature space. For a binary classification problem, it's a line in 2D space, a plane in 3D space, and a higher-dimensional surface in higher dimensions.</li>
        <li><strong>Margin:</strong> The margin is the distance between the hyperplane and the nearest data points of each class. SVM aims to maximize this margin, as it represents the separation between classes. Maximizing the margin helps improve generalization and reduces the risk of overfitting.</li>
        <li><strong>Support Vectors:</strong> These are the data points that lie closest to the decision boundary (hyperplane). They determine the position and orientation of the hyperplane and are crucial in defining the margin. Only these points influence the construction of the hyperplane; hence, they are called support vectors.</li>
        <li><strong>Kernel Trick:</strong> SVMs can efficiently handle non-linear decision boundaries by using kernel functions. The kernel trick involves implicitly mapping the input features into a higher-dimensional space, where the classes are more likely to be separable. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.</li>
        <li><strong>Regularization Parameter (C):</strong> The regularization parameter (C) controls the trade-off between maximizing the margin and minimizing the classification error. A smaller C value leads to a larger margin but may result in misclassification of some training examples (soft margin). Conversely, a larger C value allows more training examples to be correctly classified but may lead to a smaller margin (hard margin). Proper tuning of C is crucial to prevent overfitting.</li>
        <li><strong>Kernel Parameters:</strong> For kernelized SVMs, there are additional parameters to tune, such as gamma (\(\gamma\)) for RBF kernel and degree for polynomial kernel. These parameters control the flexibility of the decision boundary. Proper selection of kernel parameters is essential for achieving good performance and avoiding overfitting.</li>
        <li><strong>Dual Problem:</strong> The optimization problem in SVM can be reformulated into its dual form, which involves maximizing a function subject to constraints. Solving the dual problem is often more computationally efficient, especially when using kernel functions.</li>
        <li><strong>Nu Parameter:</strong> In Nu-SVM, the nu parameter replaces the C parameter and controls the upper bound on the fraction of margin errors and support vectors. It offers a more intuitive way to adjust the model complexity.</li>
    </ul>

    <!--------------------------------------->
    <h4 id="application">Useful application of SVM</h4>
    <p>Support Vector Machines (SVMs) are versatile algorithms that find applications in various fields due to their ability to handle both linear and non-linear classification problems efficiently. Here are some common applications of SVM:</p>
    <ul>
        <li><strong>Image Classification:</strong> SVMs are widely used in image classification tasks, such as identifying objects or scenes within images. They can efficiently handle high-dimensional feature spaces and are robust to noise.</li>

        <li><strong>Text Classification:</strong> SVMs are employed in text classification tasks, such as spam email detection, sentiment analysis, and document categorization. They can effectively classify text data represented as high-dimensional feature vectors.</li>
        
        <li><strong>Handwriting Recognition:</strong> SVMs are used in handwriting recognition systems, where they classify handwritten characters or digits into different classes. They can handle complex patterns and variations in handwriting styles.</li>
        
        <li><strong>Biomedical Applications:</strong> SVMs find applications in various biomedical tasks, including disease diagnosis, protein classification, and gene expression analysis. They can analyze high-dimensional biomedical data and identify patterns indicative of specific conditions.</li>
        
        <li><strong>Financial Forecasting:</strong> SVMs are utilized in financial forecasting tasks, such as stock price prediction, credit scoring, and fraud detection. They can analyze historical financial data and identify patterns to make predictions about future trends or events.</li>
        
        <li><strong>Bioinformatics:</strong> SVMs are applied in bioinformatics for tasks such as protein structure prediction, DNA sequence analysis, and protein-protein interaction prediction. They can handle large-scale biological data and extract meaningful patterns.</li>
        
        <li><strong>Remote Sensing:</strong> SVMs are used in remote sensing applications, such as land cover classification, crop classification, and environmental monitoring. They can analyze satellite imagery and classify different land cover types accurately.</li>
        
        <li><strong>Face Detection:</strong> SVMs are employed in face detection systems, where they classify image regions as containing faces or background. They can effectively distinguish between facial features and background clutter.</li>
        
        <li><strong>Medical Diagnosis:</strong> SVMs are used in medical diagnosis tasks, such as cancer detection, disease prognosis, and patient outcome prediction. They can analyze medical data from various sources and assist healthcare professionals in decision-making.</li>
        
        <li><strong>Anomaly Detection:</strong> SVMs are utilized in anomaly detection systems to identify unusual or unexpected patterns in data. They can detect outliers or anomalies that deviate significantly from normal behavior.</li>
    </ul>

    <!---------------------------------->
    <h4 id="intuition">Intuition: example</h4>
    (Apart from the given examples given below, some good examples can be found at: <a href="https://www.simplilearn.com/tutorials/data-science-tutorial/svm-in-r" target="_blank">Simplilearn SVM examples</a>)
    <ul>
        <li><strong>Intuition-1: Border classification</strong>
            <p>Consider a scenario where countries A and B share a border and maintain a peaceful truce (This example is taken from <a href="https://www.newtechdojo.com/understanding-support-vector-machines-svm/" target="_blank">newtechdojo</a>). This border serves as a dividing line between the citizens of each country. Both nations deploy their armed forces to vigilantly patrol the border, ensuring strict adherence to the agreement and preventing any unauthorized crossings. Additionally, there exists a disputed piece of land, known as No Man’s Land, which falls outside the jurisdiction of either country. Visually, this arrangement can be depicted as shown in image-(a):</p>

            <figure>
                <img src="assets/img/machine-ln/classification-svm-example.png" alt="" style="max-width: 90%; max-height: auto;">
                <figcaption style="text-align: center;">(<strong>Image credit:</strong> <a href="https://www.newtechdojo.com/understanding-support-vector-machines-svm/" target="_blank">newtechdojo</a>)</figcaption>
            </figure>
            <p>In this diagram, observe that the border's integrity is upheld solely by the presence of soldiers along the solid line. Essentially, these soldiers are responsible for maintaining the shape and security of the border. Meanwhile, citizens of each respective country are free to move within their territories without constraint. However, should even a single soldier withdraw from their post, there exists a risk of the enemy advancing and altering the border's position. This precarious situation is illustrated in figure-(b). It's important to emphasize that the border serves as a demarcation between the citizens of each country, with only the soldiers tasked with its protection. Surrounding the border is unclaimed land, devoid of ownership by either nation. Notably, soldiers are strategically positioned closest to the border, while civilians reside at a distance.</p>
            <p>A few things to note here:</p>
            <ul>
                <li>The border separates citizens of one country from that of another.</li>
                <li>Only the soldiers are required to keep the border.</li>
                <li>There is an empty land around the border that no one claims.</li>
                <li>Soldiers are closest to the border and other citizens are far away.</li>
            </ul>
        
            <p>Now imagine the same situation as above but replace the scenario as follows:</p>
            <ul>
                <li>Country A as a set of loan defaulters & Country B as a set of usual customers.</li>
                <li>Citizens as Data samples (customers)</li>
                <li>Soldiers as supporting/necessary data samples</li>
                <li>The border as Decision Boundary (Separator)</li>
            </ul>
            <p>If we imagine the above scenario, then we have reached a way to separate our data samples by using only a minimal subset (Soldiers) of the whole data. This idea is the fundamental essence of classification using the very Support Vector Machine. In SVM terminology, this border is a Separating Hyperplane; the citizens are data points; the soldiers are Support Vector; the distance between soldiers and the border is the Margin and, the whole setup is a Support Vector Machine Classifier. This idea is assimilated in the diagram-(c) of the above picture.</p>
        </li>
        <li><strong>Intuition-2: Image classification</strong>
            <p>Imagine you are a customs officer at an airport tasked with identifying potentially dangerous items in passengers' luggage. You have training data consisting of examples of dangerous and non-dangerous items, each described by various features (e.g., shape, density, material). Using SVM, you aim to draw a decision boundary (hyperplane) in the feature space that separates the dangerous items from the non-dangerous ones. The SVM algorithm will help you find the optimal hyperplane that maximizes the margin between these two classes, ensuring a clear separation even for new items not seen during training. </p>
        </li>
        <li><strong>Intuition-3: Email classification</strong>
        <p>Support Vector Machines (SVMs) play a crucial role in email classification, particularly in tasks such as spam detection. Their effectiveness lies in their ability to accurately separate spam emails from non-spam ones by identifying the optimal hyperplane that maximizes the margin between the two classes. SVMs are adept at handling high-dimensional feature vectors representing email content, ensuring precise classification even with complex datasets. Additionally, SVMs exhibit robustness to overfitting, leading to better generalization and reduced misclassification of new emails. One of their key advantages is their flexibility and tunability, offering parameters like kernel functions and regularization that allow developers to fine-tune models for optimal performance. SVMs also boast efficient training algorithms, making them suitable for large-scale email datasets and real-time email filtering applications. Furthermore, the decision boundary learned by SVMs can provide insights into distinguishing features of spam and non-spam emails, aiding in model interpretation. Overall, SVMs offer a powerful and versatile solution for email classification, providing robustness, efficiency, and flexibility in handling diverse email datasets.</p>
        <figure>
            <img src="assets/img/machine-ln/classfication-email-svm-example.png" alt="" style="max-width: 90%; max-height: auto;">
            <figcaption style="text-align: center;">(<strong>Image credit:</strong> <a href="https://www.newtechdojo.com/understanding-support-vector-machines-svm/" target="_blank">newtechdojo</a>)</figcaption>
        </figure>
        </li>
    </ul>


    <!--------------------------->
    <h4 id="pros">Pros</h4>
    <ol>
        <li><strong>Effective in High-Dimensional Spaces:</strong> SVMs perform well in high-dimensional spaces, making them suitable for tasks involving a large number of features, such as image classification and text categorization.</li>

        <li><strong>Robust to Overfitting:</strong> SVMs are less prone to overfitting compared to other algorithms like decision trees. By maximizing the margin between classes, SVMs generalize well to unseen data.</li>
        
        <li><strong>Versatile Kernel Functions:</strong> SVMs can handle non-linear decision boundaries using various kernel functions (e.g., polynomial, radial basis function), allowing them to capture complex relationships in the data.</li>
        
        <li><strong>Works Well with Small Datasets:</strong> SVMs can perform well even with relatively small datasets, as they focus on the points closest to the decision boundary (support vectors) rather than the entire dataset.</li>
        
        <li><strong>Global Optimization:</strong> SVMs use convex optimization techniques to find the optimal hyperplane, ensuring that the solution is globally optimal and not affected by local minima.</li>
        
        <li><strong>Regularization Parameter:</strong> SVMs offer a regularization parameter (C) that allows users to control the trade-off between maximizing the margin and minimizing classification errors, providing flexibility in model tuning.</li>
    </ol>

    <!--------------------------->
    <h4 id="cons">Cons</h4>
    <ol>
        <li><strong>Sensitivity to Parameter Tuning:</strong> SVMs require careful selection of parameters, such as the choice of kernel function and regularization parameter. Improper parameter tuning can lead to suboptimal performance.</li>

        <li><strong>Computationally Intensive:</strong> Training an SVM can be computationally intensive, especially for large datasets. The training time and memory requirements increase significantly with the number of data points.</li>
        
        <li><strong>Limited Interpretability:</strong> The decision boundary produced by SVMs may be difficult to interpret, particularly when using complex kernel functions or in high-dimensional spaces. This limits the insight into the underlying relationships in the data.</li>
        
        <li><strong>Memory Usage:</strong> SVMs require storing all support vectors in memory, which can become impractical for very large datasets with numerous support vectors, leading to high memory usage.</li>
        
        <li><strong>Binary Classification:</strong> SVMs are inherently binary classifiers and need additional techniques (e.g., one-vs-all) to handle multi-class classification problems, which can increase complexity.</li>
        
        <li><strong>Sensitive to Noise:</strong> SVMs can be sensitive to noise in the training data, as outliers or mislabeled points near the decision boundary may significantly affect the model's performance.</li>
    </ol>



    <!------------------------------->
    <h2 id="math-svm">Mathematical intuition behind the Support Vector Machine</h2>
    <p>The mathematical intuition behind Support Vector Machines (SVMs) revolves around finding the optimal hyperplane that separates different classes in a dataset. The detailed explanation of the mathematical principles underlying SVMs:</p>
    <ol>
        <li><strong>Linear Separability: </strong>SVMs are designed for binary classification tasks where the goal is to separate two classes of data points using a hyperplane in feature space. If the classes are linearly separable, meaning they can be separated by a straight line (or hyperplane in higher dimensions), SVM aims to find the hyperplane that maximizes the margin between the two classes. This hyperplane is called the "maximum-margin hyperplane."</li>
        <li><strong>Margin Maximization:</strong> The margin is defined as the distance between the hyperplane and the nearest data points from each class, known as support vectors. SVM aims to maximize this margin, as it represents the region where the classifier is most confident in its predictions and is less likely to be influenced by noise or outliers.</li>
        <li><strong>Optimization Problem:</strong> Mathematically, finding the maximum-margin hyperplane can be formulated as an optimization problem. SVM seeks to minimize the classification error while maximizing the margin. This is typically expressed as a convex optimization problem, where the objective function is subject to constraints that ensure the correct classification of training examples and the maximization of the margin.</li>
        <li><strong>Decision Function:</strong> Once the optimal hyperplane is found, the decision function of SVM can be defined as \(f(x) = \text{sign}(w^T w +b)\), where \(w\) is the weight vector perpendicular to the hyperplane, \(x\) is the input vector, \(b\) is the bias term, and 'sign' function assigns the class label based on the sign of the dot product. </li>
        <li><strong>Kernel Trick:</strong> In cases where the data is not linearly separable, SVMs can employ the kernel trick to map the input data into a higher-dimensional feature space where it becomes linearly separable. This allows SVMs to handle non-linear relationships between features without explicitly computing the transformation, making them more flexible and powerful.</li>
        <li><strong>Support Vectors:</strong> In SVM, only a subset of training examples, known as support vectors, influences the position of the hyperplane. These support vectors are the data points closest to the decision boundary and determine the orientation of the hyperplane.</li>
    </ol>
    <p>In summary, the mathematical intuition behind SVMs involves finding the optimal hyperplane that maximizes the margin between different classes of data points. By maximizing the margin, SVMs create robust decision boundaries that generalize well to unseen data, making them effective tools for binary classification tasks.</p>

    <!--------------------->
    <h3 id="kernel-fun">Kernel Functions</h3> 
    <p>Kernel functions play a fundamental role in Support Vector Machines (SVMs), enabling them to handle non-linear relationships between features and find complex decision boundaries. Let's delve into a detailed explanation of kernel functions: </p>

    <!------------------->
    <h3 id="purpose-kernel">Purpose of Kernel Functions</h3>
    <p>Kernel functions are mathematical transformations applied to input data in SVMs. Their primary purpose is to map the original input space into a higher-dimensional feature space, where the classes may be more easily separable by a hyperplane. By transforming the data into a higher-dimensional space, kernel functions enable SVMs to capture non-linear relationships and complex patterns in the data. A kernel function, denoted as \(K(\textbf{x}, \textbf{x}')\), takes two input vectors \(\textbf{x}\) and \(\textbf{x}'\) and computes a measure of similarity or distance between them. This measure is often represented as the dot product of the transformed feature vectors in the higher-dimensional space.</p>  
    
    
    <h3 id="types-kernel">Types of Kernel Functions</h3>
    <p>There are several types of kernel functions commonly used in SVMs. For the input vectors \(\textbf{x}\) and \(\textbf{x}'\):</p>
    <ol>
        <li><strong>Linear Kernel \(K_{\text{linear}}(\textbf{x},\textbf{x}')\):</strong> This is the simplest kernel function, where the dot product of the input vectors is computed directly in the original feature space. It is suitable for linearly separable data.
        $$K_{\text{linear}}(\textbf{x}, \textbf{x}') = \textbf{x}^T \textbf{x}'$$
        here \(\textbf{x}^T\) represents the transpose of vector \(\textbf{x}\), and \(\textbf{x}'\) is another vector.
        <ul>
            <li><strong>Interpretation:</strong> The linear kernel measures the similarity between two vectors by taking into account the alignment of their features. It computes the weighted sum of the products of corresponding features in the input vectors.</li>
            <li><strong>Features:</strong> In the original feature space, each input vector \(\textbf{x}\) is represented by a set of features. The dot product \(\textbf{x}^T \textbf{x}'\) calculates the sum of the products of corresponding features in vector \(\textbf{x}\) and \(\textbf{x}'\), effectively measuring their similalrity.</li>
            <li><strong>Linear Separation:</strong> The linear kernel is suitable for linearly separable data, where a straight line (or hyperplane in higher dimensions) can effectively separate the classes. It works well when the decision boundary between classes can be represented by a linear equation.</li>
            <li><strong>Advantages: </strong>
            <ul>
                <li><strong>Simplicity:</strong> The linear kernel is straightforward and computationally efficient.</li>
                <li><strong>Interpretability:</strong> The decision boundary learned by SVM with a linear kernel can be easily interpreted as a linear combination of input features.</li>
            </ul>
            </li>
            <li><strong>Limitations: </strong>
            <ul>
                <li><strong>Limited Complexity:</strong> The linear kernel may not capture complex relationships between features and classes, making it less suitable for datasets with non-linear separability.</li>
                <li><strong>Performance on Non-linear Data:</strong> In cases where the data is not linearly separable, the linear kernel may lead to suboptimal classification performance.</li>
            </ul>
            </li>
            <li><strong>Applications:</strong>
            <ul>
                <li><strong>Text Classification:</strong> Linear kernels are commonly used in text classification tasks where the feature space is often high-dimensional.</li>
                <li><strong>Linear Regression:</strong> Linear kernels can be applied in linear regression problems to model relationships between input features and target variables.</li>
            </ul>
            </li>
            <li><strong>Example:</strong>Let's consider a simple example with two input vectors \(\textbf{x}\) and \(\textbf{x}'\) represented by two-dimensional feature vectors. The linear kernel computes the dot product of these vectors directly in the original feature space. Suppose we have two input vectors
            $$x = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$$
            and 
            $$x' = \begin{pmatrix} x'_1 \\ x'_2 \end{pmatrix}$$
            <p>The linear Kernel between the two vectors is computed as:</p>
            $$K_{\text{linear}}(\textbf{x}, \textbf{x}') = \textbf{x}^T \textbf{x} =\begin{pmatrix} x_1 & x_2 \end{pmatrix} \begin{pmatrix} x'_1 \\ x'_2 \end{pmatrix} = x_1 x_1'+x_2 x_2'$$
            This equation represents the similarity between the two input vectors \(\textbf{x}\) and \(\textbf{x}'\) based on the alignment of their features. The larger the value of the Kernel \(K_{\text{linear}}(\textbf{x}, \textbf{x}')\), the more similar the vectors \(\textbf{x}\) and \(\textbf{x}'\) are considered to be. For example two vectors:
            $$x = \begin{pmatrix} 2 \\ 3 \end{pmatrix}$$
            and 
            $$x' = \begin{pmatrix} 4 \\ 1 \end{pmatrix}$$
            Kernel \(K_{\text{linear}}(\textbf{x}, \textbf{x}') = 11\) indicating a relatively high similalrity between the two inputs vectors. 
            </li>
        </ul>
        </li>
        <li><strong>Polynomial Kernel \(K_{\text{poly}}(\textbf{x},\textbf{x}')\):</strong>
            The Polynomial Kernel is a type of kernel function used in Support Vector Machines (SVMs) to handle non-linear relationships between features. It transforms the input data into a higher-dimensional space, where it may be easier to find a separating hyperplane. The polynomial kernel raises the dot product of the input vectors to a certain power \(d\), allowing SVMs to capture polynomial decision boundaries. It is defined as
        $$K_{\text{poly}}(\textbf{x},\textbf{x}') = \left(\textbf{x}\cdot \textbf{x}'+c\right)^d,$$
        where \(c\) is a constant and \(d\) is the degree of the polynomial.
        <ul>
            <li><strong>Role of Parameters: </strong>
                <ul>
                    <li><strong>Degree \((d)\):</strong> The degree parameter determines the degree of the polynomial transformation applied to the input data. Higher values of \(d\) result in more complex decision boundaries.</li> 
                    <li><strong>Constant \((c)\):</strong>The constant term acts as a bias and helps control the influence of lower-degree polynomials. It shifts the decision boundary away from the origin in the feature space.</li>         
                </ul>
            </li> 
            <li><strong>Kernel Properties:</strong>
                <ul>
                    <li><strong>Linearity: </strong>Despite its name, the Polynomial Kernel is capable of capturing non-linear relationships between features. By raising the dot product of the input vectors to a higher power, the kernel can represent complex decision boundaries.</li> 
                    <li><strong>Computational Complexity: </strong>The computational complexity of the Polynomial Kernel depends on the degree parameter \(d\). Higher values of \(d\) increase the computational cost, as they require computing higher-order polynomials. </li>         
                </ul>
            </li> 
            <li><strong>Choice of Parameters: </strong>
                <ul>
                    <li><strong>Degree \((d)\): </strong>The degree parameter should be chosen based on the complexity of the data and the desired level of model flexibility. Higher values of \(d\) may lead to overfitting if the data is not complex enough to warrant such a high-degree polynomial.</li> 
                    <li><strong>Constant Term \((c)\): </strong> The constant term \(c\) can be set empirically or through cross-validation. It helps adjust the bias of the model and fine-tune the decision boundary.</li>
                </ul>
            </li> 
            <li><strong>Example Application:</strong> The Polynomial Kernel is commonly used in SVMs for tasks such as image classification, text classification, and pattern recognition. It is effective in capturing non-linear relationships between features and constructing complex decision boundaries in high-dimensional feature spaces. </li>
        </ul>
        <p>In summary, the Polynomial Kernel is a versatile kernel function that allows SVMs to handle non-linear relationships between features by transforming the input data into a higher-dimensional space. It offers flexibility in capturing complex patterns and can be customized through parameters like degree and constant term to suit the characteristics of the data.</p>
        </li>
        <li><strong>Radial Basis Function (RBF) Kernel \(K_{\text{RBF}}(\textbf{x}, \textbf{x}')\):</strong> The Radial Basis Function (RBF) kernel, also known as the Gaussian kernel, is a popular choice in Support Vector Machines (SVMs) for its ability to capture complex non-linear relationships between data points. The RBF kernel computes the similarity between two input vectors \(\textbf{x}\) and \(\textbf{x}'\) based on the Euclidean distance between them in the input space. Mathematically, the RBF kernel is defined as:
            $$K_{\text{RBF}}(\textbf{x}, \textbf{x}') = \exp\left(-\gamma \|\textbf{x} - \textbf{x}'\|^2\right)$$
            <p>Here, \(\gamma>0\) is a parameter controlling the kernel's width, and \(\|\textbf{x}-\textbf{x}'\|\) represents the Euclidean distance between vectors \(\textbf{x}\) and \(\textbf{x}'\).</p>
            <ul>
                <li><strong>Interpretation: </strong>
                    <ul>
                        <li>The RBF kernel measures the similarity or closeness between two data points based on their distance in the input space.</li>
                        <li>When \(\textbf{x}\) and \(\textbf{x}'\) are close to each other, their Euclidean distance  \(\|\textbf{x}-\textbf{x}'\|\) is small, resulting in a high similalrity value.</li>
                        <li>Conversely, when \(\textbf{x}\) and \(\textbf{x}'\) are far apart, their distance increases, leading to a lower similalrity value. </li>
                        <li>The paramter \(\gamma\) controls the kernel's width, determing how quickly the similalrity decays as the distance between points increases. A smaller \(\gamma\) results in a broder kernel, capturing more global relationships. while a larger \(\gamma\) focuses on local relationships.</li>
                    </ul>
                </li>
                <li><strong>Effect on SVM Decision Boundary:</strong>
                    <ul>
                        <li>In SVMs, the RBF kernel allows for the construction of non-linear decision boundaries that can adapt to complex data distributions.</li>
                        <li>By mapping the input data into a higher-dimensional feature space implicitly, the RBF kernel transforms the data in a way that enables SVMs to find optimal separating hyperplanes even for non-linearly separable data.</li>
                    </ul>
                </li>
                <li><strong>Choice of \(\gamma\):</strong>
                    <ul>
                        <li>The choice of the \(\gamma\) parameter is critical in determining the behavior of the RBF kernel.</li>
                        <li>A smaller \(\gamma\) value results in a smoother decision boundary, capturing broader patterns in the data but potentially leading to underfitting.</li>
                        <li>Conversely, a larger \(\gamma\) value creates a more complex decision boundary, capturing finer details in the data but increasing the risk of overfitting.</li>
                    </ul>
                </li>
                <li><strong>Application: </strong>
                    <ul>
                        <li>The RBF kernel is widely used in SVMs for various classification tasks, including image recognition, text classification, and bioinformatics.</li>
                        <li>Its flexibility in capturing non-linear relationships makes it suitable for datasets with complex structures and intricate patterns.</li>
                    </ul>
                </li>
            </ul>
            <p>In summary, the Radial Basis Function (RBF) kernel is a powerful tool in SVMs for capturing non-linear relationships between data points. Its ability to adapt to complex data distributions and construct flexible decision boundaries makes it a popular choice for a wide range of classification tasks.</p>
        </li>
        <li><strong>Sigmoid Kernel:</strong> The sigmoid kernel computes the similarity between two vectors using the hyperbolic tangent function:
        $$K_{\text{sigmoid}}(\textbf{x}, \textbf{x}') = \text{tanh}\left(\alpha x^T x' +\beta\right)$$
        In this equation, \(\alpha\) and \(\beta\) are parameters.
        </li>
        <li><strong>Kernel Trick:</strong> The kernel trick is a computational shortcut that allows SVMs to implicitly compute the dot product between feature vectors in the higher-dimensional space without explicitly calculating the transformation. This is achieved by expressing the optimization problem in terms of kernel functions, avoiding the need to compute the transformation explicitly.</li>
    </ol>

    

    
    

    

    


    </section>

    <!----------- Reference ----------->
    <section id="reference">
    <h2>References</h2>
    <ul>
        <li><a href="https://arunp77.github.io/logistic-regression.html#con-mat" target="_blank">Confusion matrix details</a>.</li>
        <li>My github Repositories on Remote sensing <a href="https://github.com/arunp77/Machine-Learning/" target="_blank">Machine learning</a></li>
        <li><a href="https://mlu-explain.github.io/linear-regression/" target="_blank">A Visual Introduction To Linear regression</a> (Best reference for theory and visualization).</li>
        <li>Book on Regression model: <a href="https://avehtari.github.io/ROS-Examples/" target="_blank">Regression and Other Stories</a></li>
        <li>Book on Statistics: <a href="https://hastie.su.domains/Papers/ESLII.pdf" target="_blank">The Elements of Statistical Learning</a></li>
        <li><a href="https://www.javatpoint.com/machine-learning-naive-bayes-classifier" target="_blank">Naïve Bayes Classifier Algorithm, JAVAPoint.com</a></li>
        <li><a href="https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf">https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf</a></li>
        <li><a href="https://datahacker.rs/002-machine-learning-linear-regression-model/" target="_blank">One of the best description on Linear regression</a>.</li>
    </ul>
    </section>

    <hr>
    
    <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

    <h3>Some other interesting things to know:</h3>
    <ul style="list-style-type: disc; margin-left: 30px;">
        <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
        <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
    </ul>
    </div>
    <p></p>

    <div class="navigation">
        <a href="index.html#portfolio" class="clickable-box">
            <span class="arrow-left">Portfolio section</span>
        </a>
        
        <a href="machine-learning.html" class="clickable-box">
            <span class="arrow-right">Content</span>
        </a>
    </div>
</div>
</div>
</section><!-- End Portfolio Details Section -->
</main><!-- End #main --

<!-- ======= Footer ======= -->
<footer id="footer">
  <div class="container">
    <div class="copyright">
      &copy; Copyright <strong><span>Arun</span></strong>
    </div>
  </div>
</footer><!-- End  Footer -->

<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/typed.js/typed.umd.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    hljs.initHighlightingOnLoad();
  });
</script>

</body>

</html>