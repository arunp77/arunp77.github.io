<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <title>Pyspark: ML Pipeline</title>
    <meta content="" name="description">
    <meta content="" name="keywords">
    <!-- Favicons -->
    <link href="assets/img/Favicon-1.png" rel="icon">
    <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

    <!-- Vendor CSS Files -->
    <link href="assets/vendor/aos/aos.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
    <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
    <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
    <!-- Creating a python code section-->
    <link rel="stylesheet" href="assets/css/prism.css">
    <script src="assets/js/prism.js"></script>

    <!-- Template Main CSS File -->
    <link href="assets/css/style.css" rel="stylesheet">

    <!-- To set the icon, visit https://fontawesome.com/account-->
    <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
    <!-- end of icon-->

    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!-- =======================================================
    * Template Name: iPortfolio
    * Updated: Sep 18 2023 with Bootstrap v5.3.2
    * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
    * Author: BootstrapMade.com
    * License: https://bootstrapmade.com/license/
    ======================================================== -->
</head>

<body>

    <!-- ======= Mobile nav toggle button ======= -->
    <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

    <!-- ======= Header ======= -->
    <header id="header">
    <div class="d-flex flex-column">
        <div class="profile">
            <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
            <h1 class="text-light"><a href="index.html">Arun</a></h1>
            <div class="social-links mt-3 text-center">
                <a href="https://www.linkedin.com/in/arunp77/" target="_blank" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                <a href="https://github.com/arunp77" target="_blank" class="github"><i class="bx bxl-github"></i></a>
                <a href="https://twitter.com/arunp77_" target="_blank" class="twitter"><i class="bx bxl-twitter"></i></a>
                <a href="https://www.instagram.com/arunp77/" target="_blank" class="instagram"><i class="bx bxl-instagram"></i></a>
                <a href="https://arunp77.medium.com/" target="_blank" class="medium"><i class="bx bxl-medium"></i></a>
            </div>
        </div>

        <nav id="navbar" class="nav-menu navbar">
            <ul>
                <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
                <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
                <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
                <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
                <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
                <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
                <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
                <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
                <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
                <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
            </ul>
        </nav><!-- .nav-menu -->
    </div>
    </header><!-- End Header -->

    <main id="main">
        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs"> 
        <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
            <h2>Data Engineering</h2>
            <ol>
                <li><a href="Data-engineering.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
            </ol>
            </div>
    
        </div>
        </section><!-- End Breadcrumbs -->

        <!------  right dropdown menue ------->
        <div class="right-side-list">
            <div class="dropdown">
                <button class="dropbtn"><strong>Shortcuts:</strong></button>
                <div class="dropdown-content">
                    <ul>
                        <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                        <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                        <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                        <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                        <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                        <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                        <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                        <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                        <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                        <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                        <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquery</a></li>
                        <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                        <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                            <!-- Add more subsections as needed -->
                    </ul>
                </div>
            </div>
        </div>

        <!-- ======= Portfolio Details Section ======= -->
        <section id="portfolio-details" class="portfolio-details">
            <div class="container">
                <div class="row gy-4">
                    <h1>Machine learning with Pyspark: Use of ML pipeline</h1>
                    <div class="col-lg-8">
                        <div class="portfolio-details-slider swiper">
                            <div class="swiper-wrapper align-items-center"> 
                                <figure>
                                    <img src="assets/img/data-engineering/Apache_Spark_logo.svg.png" alt="" style="max-width: 50%; max-height: auto;">
                                    <figcaption></figcaption>
                                </figure>
                            </div>
                            <div class="swiper-pagination"></div>
                        </div>
                    </div>

                    <div class="col-lg-4 grey-box">
                        <div class="section-title">
                            <h3>Table of Contents</h3>
                            <ol>
                                <li><a href="#introduction">Introduction</a></li>
                                <li><a href="#objective">Objective</a></li>
                                <ul>
                                    <li><a href="#buildsparksession">Building the SparkSession and loading the datasets</a></li>
                                    <li><a href="#select-categorical">Selecting the Categorical variables</a></li>
                                    <li><a href="#pipelines">Pipelines</a></li>
                                    <li><a href="#svmlib">Formatting the database in svmlib format</a></li>
                                    <li><a href="#application">Application of a Spark ML classifier</a></li>
                                    <li><a href="#model">Model prediction</a></li>
                                    <li><a href="#model-evaluate">Model evaluation</a></li>
                                </ul>
                                <li><a href="#reference">Reference</a></li>
                            </ol>
                        </div>
                    </div>
                </div>

                <section>
                    <div class="grey-box">
                        <h3 id="introduction">Introduction</h3>
                        A Spark ML Pipeline is an API provided by Apache Spark for building and deploying end-to-end machine learning workflows. It allows you to chain together multiple stages of machine learning algorithms and data transformations into a single pipeline, making it easier to manage and deploy complex machine learning models.
                        <ul>
                            <li><strong>Step-1: Import necessary libraries: </strong></li>
                            <pre class="language-python"><code>
                                from pyspark.ml import Pipeline
                                from pyspark.ml.feature import VectorAssembler, StringIndexer
                                from pyspark.ml.classification import RandomForestClassifier
                            </code></pre>
                            <li><strong>Step-2: Define stages: </strong>Define the stages of the pipeline. Stages can include data preprocessing, feature engineering, and model training.</li>
                            <pre class="language-python"><code>
                                # Define feature vector assembler
                                feature_cols = ["feature1", "feature2", ...]
                                feature_assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
                                
                                # Define label indexer (if target variable is categorical)
                                label_indexer = StringIndexer(inputCol="label", outputCol="indexedLabel")
                                
                                # Define classifier (e.g., RandomForestClassifier)
                                classifier = RandomForestClassifier(featuresCol="features", labelCol="indexedLabel", seed=42)
                            </code></pre>
                            <li><strong>Step-3: Create pipeline: </strong>Create a pipeline by specifying the stages in sequential order.</li>
                            <pre class="language-python"><code>pipeline = Pipeline(stages=[feature_assembler, label_indexer, classifier])</code></pre>
                            <li><strong>Step-4: Fit pipeline: </strong>Fit the pipeline to your training data.</li>
                            <pre class="language-python"><code>pipeline_model = pipeline.fit(train_data)</code></pre>
                            <li><strong>Step-5: Make predictions: </strong>Use the fitted pipeline to make predictions on new data.</li>
                            <pre class="language-python"><code>predictions = pipeline_model.transform(test_data)</code></pre>
                            <li><strong>Step-6: Evaluate model: </strong>Evaluate the performance of the model using appropriate evaluation metrics.</li>
                            <pre class="language-python"><code>
                                from pyspark.ml.evaluation import BinaryClassificationEvaluator

                                evaluator = BinaryClassificationEvaluator(labelCol="indexedLabel")
                                accuracy = evaluator.evaluate(predictions)
                            </code></pre>
                            <li><strong>Step-7: Parameter tuning: </strong>We can perform hyperparameter tuning using techniques like cross-validation.</li>
                            <pre class="language-python"><code>
                                from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

                                param_grid = ParamGridBuilder() \
                                    .addGrid(classifier.maxDepth, [5, 10, 15]) \
                                    .addGrid(classifier.numTrees, [10, 20, 30]) \
                                    .build()
                                
                                crossval = CrossValidator(estimator=pipeline,
                                                            estimatorParamMaps=param_grid,
                                                            evaluator=evaluator,
                                                            numFolds=3)
                                
                                cv_model = crossval.fit(train_data)
                            </code></pre>
                        </ul>
                    </div>
                    <hr>

                    <!----------------------->
                    <h3 id="objective">Objective</h3>
                    <p>In this project, the objective is to develop a classification machine learning model using Spark MLlib's pipeline framework. The goal is to leverage the power of Spark's distributed computing capabilities to build a 
                        robust and scalable pipeline that can efficiently preprocess the HR dataset and train a classification algorithm to predict employee terminations. By implementing the pipeline, we aim to explore the relationships 
                        between various features such as employee demographics, performance metrics, and engagement scores, and the likelihood of termination. Through this analysis, we seek to achieve a high level of accuracy in 
                        predicting employee terminations, thereby providing valuable insights for HR management and facilitating proactive decision-making in employee retention strategies.</p> 
                    
                        <div class="grey-box">
                            <p><strong>About data:</strong> In the present project, we have considered following datasets on at <a href="https://www.kaggle.com/datasets/pankeshpatel/hrcommasep?select=HR_comma_sep.csv" target="_blank">Kaggle on "Human Resources Dataset"</a>.
                                This is dataset useful for HR in a company. A company records different parameters of an employee (such as satisfaction level, Salary, number of promotion, left the company etc.) . This dataset can be used to predict whether an employee 
                                leave the company or stay in the company.</p>
                            <p>Following are the data columns:</p>
                            <ul>
                                <li><code>satisfaction_level</code>: Level of employee satisfaction.</li>
                                <li><code>last_evaluation</code>: Last performance evaluation score.</li>
                                <li><code>number_project</code>: Number of projects assigned to the employee.</li>
                                <li><code>average_montly_hours</code>: Average number of monthly work hours.</li>
                                <li><code>time_spend_company</code>: Number of years spent in the company.</li>
                                <li><code>Work_accident</code>: Indicator for whether the employee had a work accident (e.g., 1 for yes, 0 for no).</li>
                                <li><code>left</code>: Indicator for whether the employee left the company (e.g., 1 for yes, 0 for no).</li>
                                <li><code>promotion_last_5</code>: Indicator for whether the employee was promoted in the last 5 years (e.g., 1 for yes, 0 for no).</li>
                                <li><code>years</code>: Number of years the employee has been with the company.</li>
                                <li><code>Department</code>: Department in which the employee works.</li>
                                <li><code>salary</code>: Level of salary for the employee (e.g., low, medium, high).</li>
                            </ul>
                        </div>

                        <h5 id="buildsparksession">Building the <code>SparkSession</code> and loading the datasets:</h5>
                        <pre class="language-python"><code>
                            from pyspark.sql import SparkSession
                            from pyspark import SparkContext
                            
                            # Defining a SparkContext locally
                            sc = SparkContext.getOrCreate()
                            
                            # Building a Spark Session
                            spark = SparkSession \
                                .builder \
                                .appName("Pipelines Spark ML") \
                                .getOrCreate()
                                
                            spark
                        </code></pre>

                        Loading the datasets
                        <pre class="language-python"><code>
                            hr = spark.read.csv('HR_comma_sep.csv', header = True)

                            # Displaying an extract from the data frame
                            hr.sample(False, 0.001 , seed = 222).toPandas()
                        </code></pre>

                        <h5>Selecting the Categorical variables:</h5>
                        Before training classification models, it is important to understand that the <code>svmlib</code> 
                        format does not support <code>Strings</code>. It is therefore necessary to use an indexer, 
                        i.e. a model to transform a categorical variable into a series of indices. In the data column <code>left</code>
                        is the variable to be predicted, indicates that if the employee left the company voluntarily or not.
                        <pre class="language-python"><code>
                            hr = hr.select( 'left',
                                'satisfaction_level',
                                'last_evaluation',
                                'number_project',
                                'average_montly_hours',
                                'time_spend_company',
                                'Work_accident',
                                'promotion_last_5years',
                                'sales',
                                'salary'
                                )

                            # Display of a description of the variables
                            hr.describe().toPandas()
                        </code></pre>
                        <figure>
                            <img src="assets/img/data-engineering/pyspark-ml-pipeline1.png" alt="" style="max-width: 100%; max-height: auto;">
                            <figcaption></figcaption>
                        </figure>
                        <div class="grey-box">    
                        <strong>Note: </strong>The <code>StringIndexer</code> is a feature transformer in Apache Spark MLlib that is commonly used for handling categorical variables. It indexes each distinct string value in a column 
                        to a numerical index. This transformation is useful for algorithms that expect numerical input features, such as decision trees or logistic regression. It is to be noted that the input column should contain 
                        string values representing categorical variables (e.g., department names, job titles, etc.). The output column will contain numerical 
                        indices corresponding to the string values in the input column. The <code>StringIndexer</code>  assigns a unique numerical index to each distinct string value in the input column.
                        The indices start from 0 and increase sequentially for each unique string value. After indexing, the transformed numerical indices can be used as input features for machine learning algorithms. However, 
                        some algorithms may require additional processing, such as one-hot encoding for handling categorical variables properly.
                        <pre class="language-python"><code>
                            from pyspark.ml.feature import StringIndexer

                            # Define the StringIndexer transformer
                            string_indexer = StringIndexer(inputCol="department", outputCol="department_index")
                            
                            # Fit the StringIndexer to the input data
                            indexed_data = string_indexer.fit(input_data).transform(input_data)
                            
                            # Show the transformed data
                            indexed_data.show()
                        </code></pre>
                        In this example, the StringIndexer is applied to the "department" column in the input data, and the transformed output is stored in a new column named "department_index". Each distinct department name in 
                        the "department" column will be assigned a unique numerical index in the "department_index" column.
                        </div>
                        <br>
                        The <code>StringIndexer</code> is used in two steps:
                        <ul>
                            <li>Create an indexer by specifying the input and output columns (<code>inputCol=, outputCol=</code>) and search the database for discrete values using the <code>fit</code> method.</li>
                            <li>Apply the indexer to the database using the method transform</li>
                        </ul>
                        In the present case, 
                        <pre class="language-python"><code>
                        from pyspark.ml.feature import StringIndexer

                        # Creation of an indexer transforming a dirty variable into indexedSales
                        salesIndexer = StringIndexer(inputCol='sales', outputCol='indexedSales').fit(hr)

                        # Creating a DataFrame hrSalesIndexed indexing the variable sales
                        hrSalesIndexed = salesIndexer.transform(hr)

                        # Displaying an extract from the hrSalesIndexed data frame 
                        hrSalesIndexed.sample(False, 0.001 , seed = 222).toPandas()
                        </code></pre>
                        <figure>
                            <img src="assets/img/data-engineering/pyspark-ml-indexed.png" alt="" style="max-width: 100%; max-height: auto;">
                            <figcaption></figcaption>
                        </figure>
                        An indexer built this way keeps in memory the indexed values. The <code>IndexToString</code> function allows you to find an original variable from an indexed variable, or from a prediction of this variable. It works with the following arguments:
                        <ul>
                            <li><code>inputCol</code> : the name of the indexed input column</li>
                            <li><code>outputCol</code> : the name of the output column to be rebuilt</li>
                            <li><code>labels</code> : the location of the labels</li>   
                        </ul>
                        <pre class="language-python"><code>
                            ### Insert your code here
                            from pyspark.ml.feature import IndexToString
                            
                            # Creating a new salesReconstructed column
                            SalesReconstructor = IndexToString(inputCol='indexedSales',
                                                                outputCol='salesReconstructed',
                                                                labels = salesIndexer.labels)
                            
                            # Apply the SalesReconstructor transformer
                            hrSalesReconstructed = SalesReconstructor.transform(hrSalesIndexed)
                            
                            # Displaying an extract from the database
                            hrSalesReconstructed.sample(False, 0.001 , seed = 222).toPandas()                            
                        </code></pre>
                        Here we create a variable <code>salesReconstructed</code> from <code>indexedSales</code> and a new table <code>hrSalesReconstructed</code> is created.
                        <figure>
                            <img src="assets/img/data-engineering/pyspark-ml-reindexed.png" alt="" style="max-width: 100%; max-height: auto;">
                            <figcaption></figcaption>
                        </figure>

                        <h5 id="pipelines">Pipelines</h5>
                        A pipeline is a sequence of stages that are executed in a specific order to process and transform data. Each stage typically represents a transformation or an estimator (machine learning algorithm) that operates on the data. 
                        The pipeline allows you to chain together multiple data processing and modeling steps, making it easy to manage and deploy complex machine learning workflows. A example syntex is given by:
                        <code>Pipeline(stages=[estimator1, estimator2, estimator3, ...])</code>.
                        <pre class="language-python"><code>
                            from pyspark.ml import Pipeline

                            # Creation of indexers
                            SalesIndexer = StringIndexer(inputCol='sales', outputCol='indexedSales')
                            SalaryIndexer = StringIndexer(inputCol='salary', outputCol='indexedSalary')
                            
                            # Creating a pipeline
                            indexer = Pipeline(stages =  [SalaryIndexer, SalesIndexer])
                            
                            # Index the variables of "hr"
                            hrIndexed = indexer.fit(hr).transform(hr)
                            
                            # Displaying an extract
                            hrIndexed.sample(False, 0.001 , seed = 222).toPandas()
                        </code></pre>    
                        This will first import <code>Pipeline</code> from <code>pyspark.ml</code> and then a >code>SalesIndexer trasnfroming a sales variable from <code>indexedSales</code>. 
                        Similarly, a SalesIndexer is also created for Salary variable indo <code>indexedSalary</code>. A pipeline is created using <code>Pipeline</code>
                        <figure>
                            <img src="assets/img/data-engineering/pyspark-ml-pipeline2.png" alt="" style="max-width: 100%; max-height: auto;">
                            <figcaption></figcaption>
                        </figure>


                        <h5 id="svmlib">Formatting the database in <code>svmlib</code> format</h5>
                        At this point, the variables are of numerical type. It is therefore possible to transform the database. First of all, it is necessary to exclude the non-indexed versions of the variables.
                        <pre class="language-python"><code>
                            from pyspark.ml.linalg import DenseVector

                            # Creation of a database excluding non-indexed variables
                            hrNumeric = hrIndexed.select('left',
                                                            'satisfaction_level',
                                                            'last_evaluation',
                                                            'number_project',
                                                            'average_montly_hours',
                                                            'time_spend_company',
                                                            'Work_accident',
                                                            'promotion_last_5years',
                                                            'indexedSales',
                                                            'indexedSalary')
                            
                            # Creation of a DenseVector variable containing the features via the RDD structure
                            hrRdd = hrNumeric.rdd.map(lambda x: (x[0], DenseVector(x[1:])))
                            
                            # transformation into DataFrame and naming variables to get a base of the form libsvm
                            hrLibsvm = spark.createDataFrame(hrRdd, ['label', 'features'])
                            
                            # Displaying an extract
                            hrLibsvm.sample(False, .001, seed = 222).toPandas()
                        </code></pre>
                        <figure>
                            <img src="assets/img/data-engineering/pyspark-ml-label-feature.png" alt="" style="max-width: 100%; max-height: auto;">
                            <figcaption></figcaption>
                        </figure>
                        The conversion of a database to libsvm format is done as follows :
                        <ul>
                            <li>Import the function DenseVector</li>
                            <li>Transform each line into a pair containing a label and a feature vector using the method map</li>
                            <li>Transform this rdd into a DataFrame whose variables are named label and features</li>
                        </ul>


                        <!------------------->
                        <h5 id="application">Application of a Spark ML classifier</h5>
                        Import <code>VectorIndexer</code> from <code>pyspark.ml.feature</code>. Create a featureIndexer transformer indexing features for a maximum number of values: maxCategories = 10
                        <pre class="language-python"><code>
                            from pyspark.ml.feature import VectorIndexer

                            # Creation of a transformer indexing the features
                            featureIndexer = VectorIndexer(inputCol="features",
                                                        outputCol="indexedFeatures",
                                                        maxCategories = 10).fit(hrLibsvm)
                        </code></pre>
                        <div class="grey-box">
                            <strong>Note: </strong>  A maximum number of discrete values is one of the limitations of Spark. Indeed, the current version of Spark ML (2.2) does not allow the data scientist to select the continuous and categorical features by himself.

                            For example, 'sales' is a categorical variable with 10 different values, but the variable 'number_project' contains integers between 2 and 7. Setting the threshold to 10 would therefore push the algorithm to consider the variable 'number_project' as categorical. The algorithm then loses the order of this variable and considers that the difference between 2 and 3 realized projects is the same as between 2 and 7. Conversely, if the threshold is set to 4, the algorithm creates an artificial order (depending on the frequency of the discrete values) in the variable 'sales'.
                            
                            Faced with this problem, it is necessary to test several thresholds and look at the importance of the variables concerned. You are also invited to look at the documentation Spark ML if this problem has been solved - Spark is indeed very recent and in constant improvement.
                        </div>
                        Then, Random Forest is the classifier proposed as a learning algorithm. Random forests are composed of a set of decision trees. These trees are distinguished from each other by the subsample of data on which they are trained. These sub-samples are randomly selected from the initial data set.

                        The principle of a random forest is simple: many small classification trees are produced on a random fraction of data. Random Forest votes on the classification trees in order to deduce the order and importance of the explanatory variables.

                        The function RandomForestClassifier() of the package pyspark.ml.classification creates a classifier and takes the following arguments :
                        <ul>
                            <li><code>labelCol</code> : name of the column to be used as a label</li>
                            <li><code>featuresCol</code> : name of the column to use containing the feature vector</li>
                            <li><code>predictionCol</code> : name of the column containing the predictions (by default = prediction)</li>
                            <li><code>seed</code> : fixed integer to make the results reproducible</li>
                            <li>The Random Forest parameters to refine the analysis, available in the documentation of Random Forest.</li>
                        </ul>
                        In the present case, do following:
                        <ul>
                            <li>Import the function RandomForestClassifier from the package pyspark.ml.classification</li>

                            <li>Create two transformers labelIndexer and featureIndexer indexing label and features of hrLibsvm</li>

                            <li>Create a Random Forest Classifier, rf, working on indexed versions of features and labels</li>

                            <li>Using IndexToString, create a transformer labelConverter to restore prediction labels</li>

                            <li>Create a Pipeline, pipeline, containing the 4 estimators/transformers created</li>

                            <li>Create two sets train and test containing respectively 70% and 30% of hrLibsvm</li>

                            <li>Create a model model that fits the Pipeline to the database train</li>
                        </ul>
                        <pre class="language-python"><code>
                            from pyspark.ml.feature import IndexToString
                            # Import of the RandomForestClassifier from the pyspark.ml.classification package
                            from pyspark.ml.classification import RandomForestClassifier
                            
                            # Creation of transformers
                            labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(hrLibsvm)
                            featureIndexer = VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories = 10).fit(hrLibsvm)
                            
                            # Creation of a classifier 
                            rf = RandomForestClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures", predictionCol='prediction', seed = 222)
                            
                            # Creation of a transformer to restore the labels of predictions
                            labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel",
                                                            labels=labelIndexer.labels)
                            
                            # Creating a Pipeline 
                            pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])
                            
                            # Splitting data into two sets: training and test 
                            (train, test) = hrLibsvm.randomSplit([0.7, 0.3], seed = 222)
                            
                            # Training the model using the training data
                            model = pipeline.fit(train)
                        </code></pre>
                        <h5 id="model">Model prediction</h5>
                        <pre class="language-python"><code>
                            predictions = model.transform(test)

                            # Display of an extract of the predictions  
                            predictions.sample(False, 0.001 , seed = 222).toPandas()
                        </code></pre>
                        <figure>
                            <img src="assets/img/data-engineering/pyspark-ml-decisiontree.png" alt="" style="max-width: 100%; max-height: auto;">
                            <figcaption></figcaption>
                        </figure>
                        <h5 id="model-evaluate">Model evaluation:</h5>
                        Once the model is built, it is important to check the reliability of the predictions, both to compare it with other classification models and to optimize the parameters. For this purpose, there is a submodule pyspark.ml.evaluation containing all the evaluation metrics. In particular, you will find the function MulticlassClassificationEvaluator to evaluate classification models. This function takes 3 main arguments :

                        <ul>
                            <li><code>metricName</code> : the metric to be used, typically : 'accuracy'</li>
                            <li><code>labelCol</code> : the name of the column to be predicted</li>
                            <li><code>predictionCol</code> : the name of the prediction column</li>
                        </ul>
                        <pre class="language-python"><code>
                        from pyspark.ml.evaluation import MulticlassClassificationEvaluator

                        # Creation of an evaluator 
                        evaluator = MulticlassClassificationEvaluator(metricName='accuracy',
                                                                    labelCol= 'indexedLabel',
                                                                    predictionCol= 'prediction')
                        # Calculation and display of model accuracy 
                        accuracy = evaluator.evaluate(predictions)
                        print(accuracy)
                        </code></pre>
                        Which gives:
                        <pre>0.963758389261745</pre>
                        The metric accuracy corresponds to the number of correct predictions divided by the number of predictions made. It is therefore between 0 and 1; 0 accuracy corresponds to completely false predictions and an accuracy of 1 corresponds to the absence of errors in the prediction.
                </section>

                <div class="box">
                    <strong>Disclaimer: </strong> This notebook is a part of my study on various topics related to data enegineering course provided by <a href="Datascientist.com" target="_blank">Datascientist.com</a> Germany.
                </div>

                <!-------Reference ------->
                <section id="reference">
                    <h2>References</h2>
                    <ol>
                        <li><a href="https://spark.apache.org/documentation.html" target="_blank"> Official Documentation</a></li>
                        <li><a href="https://www.databricks.com/learn/training/login" target="_blank">Databricks Learning Academy</a></li>
                        <li><a href="https://sparkbyexamples.com/" target="_blank">Spark by Examples</a></li>
                        <li><a href="https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark" target="_blank">Datacamp tutorial</a>.</li>
                        <li>For databricks, you can look at tutorial videos on youtube at <a href="https://www.youtube.com/watch?v=ChISx0-cMpU" target="_blank">youtube video by Bryan Cafferky</a>, 
                            writer of the book "Master Azure Databricks". A great playlist for someone who just want to learn about the big data analytics at Databricks Azure cloud platform.</li>
                        <li>See the video for <a href="https://www.youtube.com/watch?v=_C8kWso4ne4" target="_blank">pyspark basics by Krish Naik</a>. Great video for starter.</li>
                        <li><a href="https://www.youtube.com/watch?v=QLGrLFOzMRw" target="_blank">Great youtube on Apache spark</a> one premise working.</li>
                    </ol> 
                </section>

                <hr>
            
                <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

                    <h3>Some other interesting things to know:</h3>
                    <ul style="list-style-type: disc; margin-left: 30px;">
                        <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
                        <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
                    </ul>
                </div>
                <p></p>

                <div class="navigation">
                    
                    <a href="index.html#portfolio" class="clickable-box">
                        <span class="arrow-left">Portfolio section</span>
                    </a>
                    
                    <a href="Data-engineering.html" class="clickable-box">
                        <span class="arrow-right">Content</span>
                    </a>

                </div>
            </div>
        </section><!-- End Portfolio Details Section -->
    </main><!-- End #main --

    <!-- ======= Footer ======= -->
    <footer id="footer">
    <div class="container">
        <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
        </div>
    </div>
    </footer><!-- End  Footer -->

    <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="assets/vendor/typed.js/typed.umd.js"></script>
    <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function () {
        hljs.initHighlightingOnLoad();
    });
    </script>

</body>

</html> 