<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Regression models</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Arun</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
          <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
          <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
          <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
          <a href="https://medium.com/@arunp77" class="medium"><i class="bx bxl-medium"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
          <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
          <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
          <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
          <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
          <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
          <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
          <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
          <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
          <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

<main id="main">

        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs"> 
          <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
              <h2>Machine learning</h2>
              <ol>
                <li><a href="portfolio-details-1.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
              </ol>
            </div>
    
          </div>
        </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
      <div class="dropdown">
          <button class="dropbtn"><strong>Shortcuts:</strong></button>
          <div class="dropdown-content">
              <ul>
                  <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                  <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                  <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                  <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                  <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                  <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                  <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                  <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                  <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                  <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                  <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                  <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                  <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                    <!-- Add more subsections as needed -->
                </ul>
          </div>
        </div>
    </div>

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">
        <div class="row gy-4">
          <h1>Linear Regression & Gradient decent method</h1>
          <div class="col-lg-8">
            <div class="portfolio-details-slider swiper">
              <div class="swiper-wrapper align-items-center"> 
                <figure>
                  <img src="assets/img/data-engineering/Linear-reg1.png" alt="" style="max-width: 60%; max-height: auto;">
                  <figcaption></figcaption>
                </figure>
              </div>
            <div class="swiper-pagination"></div>
          </div>
        </div>

        <div class="col-lg-4 grey-box">
          
          <div class="section-title">
            <h3>Content</h3>
            <ol>
              <li><a href="#introduction">Introduction</a></li>
              <li><a href="#Relationship-of-regression-lines">Relationship of regression lines</a></li>
              <li><a href="#Types-of-Linear-Regression">Types of Linear Regression</a></li>
              <li><a href="#Mathematical-1">Mathematical Explanation</a></li>
              <li><a href="#Assumption-of-LR">Assumptions of Linear Regression</a></li>
              <li><a href="#evaluation-metrics-for-LR">Evaluation Metrics for Linear Regression</a></li>
              <li><a href="#gradient-decent">Gradient-decent method for linear regression</a></li>
              <li><a href="#Maximum-likelihood-estimation">Maximum-likelihood-estimation</a></li> 
              <li><a href="Example-simple-linear">Example on simple linear regression in python</a></li>
              <li><a href="#Example-multiple-regression">Example on multiple linear regression in python</a></li>
              <li><a href="#reference">Reference</a></li>
            </ol>
          </div>
        </div>
      </div>

      <section id="introduction">
        <h2>Intorduction</h2>
        <p>Linear regression is a popular and widely used algorithm in machine learning for predicting continuous numeric values. It models the relationship between independent variables (input features) and a dependent variable (target variable) by fitting a linear equation to the observed data. In this section, we will provide a brief overview of linear regression, including the mathematical explanation and figures to aid understanding</p>
        
        <h4>Mathematical Explanation</h4>
        <p>The linear regression algorithm aims to find the best-fit line that represents the relationship between the input features (<code>x</code>) and the target variable (<code>y</code>). The equation for a simple linear regression can be expressed as:</p>
          <figure>
            <img src="assets/img/data-engineering/Linear-reg0.png" alt="" style="max-width: 40%; max-height: 40%;">
            <figcaption></figcaption>
          </figure>
        
        $$y = m x +c$$

        <p>where</p>
          <ul>
            <li><code>y</code> represents the target variable or the dependent variable we want to predict.</li>
            <li><code>x</code> represents the input feature or the independent variable.</li>
            <li><code>m</code> represents the slope of the line, which represents the rate of change of <code>y</code> with respect to <code>x</code>.</li>
            <li><code>c</code> represents the <code>y</code>-intercept, which is the value of <code>y</code> when <code>x</code> is equal to <code>0</code>.</li>
          </ul>


        <h3 id="Relationship-of-regression-lines">Relationship of regression lines<a class="anchor-link" href="#Relationship-of-regression-lines">&#182;</a></h3>
          <ul>
            <li>A linear line showing the relationship between the dependent and independent variables is called a regression line. </li>
            <li>A regression line can show two types of relationship:</li>
          </ul>
          <ol>
            <li><strong>Positive Linear Relationship:</strong> If the dependent variable increases on the Y-axis and independent variable increases on X-axis, then such a relationship is termed as a Positive linear relationship.</li>
            <li><strong>Negative Linear Relationship:</strong> If the dependent variable decreases on the Y-axis and independent variable increases on the X-axis, then such a relationship is called a negative linear relationship.</li>
          </ol>
          <img src="assets/img/data-engineering/line-slope.png" alt="" style="max-width: 70%; max-height: 70%;">

        <h3 id="Types-of-Linear-Regression">Types of Linear Regression<a class="anchor-link" href="#Types-of-Linear-Regression">&#182;</a></h3>
          <p>Linear regression can be further divided into two types of the algorithm:</p>
            <ol>
              <li><strong>Simple Linear Regression:</strong> If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.</li>
              <li><strong>Multiple Linear regression:</strong> If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression.</li>
            </ol>
      </section>


      <section id="Assumption-of-LR">
        <h3>Assumptions of Linear Regression</h3>
        <ol>
          <li><strong>Linearity of residuals: </strong>The relationship between the independent variables and the dependent variable is assumed to be linear. This means that the change in the dependent variable is directly proportional to the change in the independent variables.
            <figure>
              <img src="assets/img/data-engineering/Linearity.png" alt="" style="max-width: 70%; max-height: 70%;">
              <figcaption></figcaption>
            </figure>
          </li>
          <li><strong>Independence: </strong>The observations in the dataset are assumed to be independent of each other. There should be no correlation or dependence between the residuals (the differences between the actual and predicted values) of the dependent variable for different observations.
            <figure>
              <img src="assets/img/data-engineering/independence.png" alt="" style="max-width: 70%; max-height: 70%;">
              <figcaption></figcaption>
            </figure>
          </li>
          <li><strong>Normal distribution of residuals: </strong>The mean of residuals should follow a normal distribution with a mean equal to zero or close to zero. This is done in order to check whether the selected line is actually the line of best fit or not. If the error terms are non-normally distributed, suggests that there are a few unusual data points that must be studied closely to make a better model.
            <figure>
              <img src="assets/img/data-engineering/Disti-assumption.png" alt="" style="max-width: 70%; max-height: 70%;">
              <figcaption></figcaption>
            </figure>
          </li>
          <li><strong>The equal variance of residuals: </strong>The error terms must have constant variance. This phenomenon is known as Homoscedasticity. The presence of non-constant variance in the error terms is referred to as Heteroscedasticity. Generally, non-constant variance arises in the presence of outliers or extreme leverage values.
            <figure>
              <img src="assets/img/data-engineering/eual-variance-assumption.png" alt="" style="max-width: 70%; max-height: 70%;">
              <figcaption></figcaption>
            </figure>
          </li>
        </ol>
      </section>

      
            

      <section id="Mathematical-1">
        <h3 id="Mathematical-1">Mathematical Explanation:</h3>
        <p>There are parameters <code>β<sub>0</sub></code>, <code>β<sub>1</sub></code>, and ϵ, such that for any fixed value of the independent variable <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>x</mi> </math>, the dependent variable is a random variable related to <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>x</mi> </math> through the model equation:</p>
        
          $$y=\beta_0 + \beta_1 x +\epsilon$$

        <p>where</p>
        <ul>
          <li><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math> = Dependent Variable (Target Variable)</li>
          <li><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math> = Independent Variable (predictor Variable)</li>
          <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>0</mn> </msub> </math> = intercept of the line (Gives an additional degree of freedom)</li>
          <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>1</mn> </msub> </math> = Linear regression coefficient (scale factor to each input value).</li>
          <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>&#x03F5;<!-- ϵ --></mi> </math> = random error.</li>
        </ul>
        
        <p>The goal of linear regression is to estimate the values of the regression coefficients</p>
          <img src="assets/img/data-engineering/Multi-lin-reg.png" alt="" style="max-width: 60%; max-height: 60%;">
        
          <p>This algorithm explains the linear relationship between the dependent(output) variable <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math>
            and the independent(predictor) variable <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math> using a straight line 
            <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>y</mi> <mo>=</mo> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>1</mn> </msub> <mi>x</mi> </math></p>
        
        
        <h5>Goal</h5>
        <ul>
          <li>The goal of the linear regression algorithm is to get the best values for <math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>0</mn> </msub> </math>
              and <math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>1</mn> </msub> </math> to find the best fit line. </li>
          <li>The best fit line is a line that has the least error which means the error between predicted values and actual values should be minimum.</li>
          <li><p>For a datset with <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>n</mi> </math> observation <math xmlns="http://www.w3.org/1998/Math/MathML"> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo>,</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo stretchy="false">)</mo> </math>, 
            where <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mn>3....</mn> <mo>,</mo> <mi>n</mi> </math> the above function can be written as follows</p>
          
            <p><math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>=</mo> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>1</mn> </msub> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo>+</mo> <msub> <mi>&#x03F5;<!-- ϵ --></mi> <mi>i</mi> </msub> </math></p>

          <p>where <math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>y</mi> <mi>i</mi> </msub> </math> is the value of the observation of the dependent variable (outcome variable) in the smaple, <math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>x</mi> <mi>i</mi> </msub> </math> is the value of <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>i</mi> <mi>t</mi> <mi>h</mi> </math> observation 
            of the independent variable or feature in the sample, <math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>&#x03F5;<!-- ϵ --></mi> <mi>i</mi> </msub> </math> is the random error (also known as residuals) in predicting the value of <math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>y</mi> <mi>i</mi> </msub> </math>, 
            <math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>0</mn> </msub> </math> and <math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>i</mn> </msub> </math> are the regression parameters (or regression coefficients or feature weights).</p>
          </li>
        </ul>

        In simple linear regression, there is only one independent variable (<math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>x</mi> </math>) and one dependent variable (<math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>y</mi> </math>). 
        The parameters (coefficients) in simple linear regression can be calculated using the method of <strong>ordinary least squares (OLS)</strong> or <strong>gradient descent method</strong>.
        <p>The estimated parameters provide the values of the intercept and slope that best fit the data according to the simple linear regression model.</p> 
        
        


        
        <h3 id="OLS">Ordinary least squares method</h3>
          
        The equations and formulas involved in calculating the parameters are as follows:</p>
        <p><strong>Model Representation:</strong></p>
        <p>Let's suppose that predicted values of y after fitting the linear equation is:
          $$h_\theta (x) = \theta_0 + \theta_1 x + \epsilon$$
        </p>
          <p>Therefore, we can write error function as: \(\epsilon = y_i - h_\theta(x_i) \)</p>
        <ol>
          <li><p><strong>Cost Function or mean squared error (MSE):</strong></p>
            <p>The MSE, measures the average squared difference between the predicted values (<math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow class="MJX-TeXAtom-ORD"> <mover> <mi>y</mi> <mo stretchy="false">&#x005E;<!-- ^ --></mo> </mover> </mrow> </math>) and the actual values of the dependent variable (<math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>y</mi> </math>). It is given by:</p>
            
            $$MSE = \frac{1}{n} \sum (y_i - h_\theta(x_i))^2$$
            
            <p>Where:</p>
            
            <ul>
              <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>n</mi> </math> is the number of data points.</li>
              <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>y</mi> <mi>i</mi> </msub> </math> is the actual value of the dependent variable for the i-th data point.</li>
              <li>\(h_\theta(x_i)\) is the predicted value of the dependent variable for the i-th data point.</li>
            </ul>
          </li>

          <li><p><strong>Minimization of the Cost Function:</strong></p>
            <p>The parameters <math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>0</mn> </msub> </math> and <math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>1</mn> </msub> </math> are estimated by minimizing the cost function. The formulas for calculating the parameter estimates are derived from the derivative of the cost function with respect to each parameter.</p>
            <p>The parameter estimates are given by:</p>
            <ul>
              <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow class="MJX-TeXAtom-ORD"> <mover> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>1</mn> </msub> <mo stretchy="false">&#x005E;<!-- ^ --></mo> </mover> </mrow> <mo>=</mo> <mfrac> <mrow> <mtext>Cov</mtext> <mo stretchy="false">(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> </mrow> <mrow> <mi>V</mi> <mi>a</mi> <mi>r</mi> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo> </mrow> </mfrac> </math><math xmlns="http://www.w3.org/1998/Math/MathML"> <mo stretchy="false">&#x21D2;<!-- ⇒ --></mo> <menclose notation="box"> <mrow class="MJX-TeXAtom-ORD"> <mstyle displaystyle="true" scriptlevel="0"> <mrow class="MJX-TeXAtom-ORD"> <msub> <mrow class="MJX-TeXAtom-ORD"> <mover> <mi>&#x03B2;<!-- β --></mi> <mo stretchy="false">&#x005E;<!-- ^ --></mo> </mover> </mrow> <mn>1</mn> </msub> <mo>=</mo> <mfrac> <mrow> <mo>&#x2211;<!-- ∑ --></mo> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo>&#x2212;<!-- − --></mo> <mrow class="MJX-TeXAtom-ORD"> <mover> <mi>x</mi> <mo stretchy="false">&#x00AF;<!-- ¯ --></mo> </mover> </mrow> <mo stretchy="false">)</mo> <mo stretchy="false">(</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>&#x2212;<!-- − --></mo> <mrow class="MJX-TeXAtom-ORD"> <mover> <mi>y</mi> <mo stretchy="false">&#x00AF;<!-- ¯ --></mo> </mover> </mrow> <mo stretchy="false">)</mo> </mrow> <mrow> <mo>&#x2211;<!-- ∑ --></mo> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo>&#x2212;<!-- − --></mo> <mrow class="MJX-TeXAtom-ORD"> <mover> <mi>x</mi> <mo stretchy="false">&#x00AF;<!-- ¯ --></mo> </mover> </mrow> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> </mrow> </mfrac> </mrow> </mstyle> </mrow> </menclose> </math>$$</li>
              <li><p><math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow class="MJX-TeXAtom-ORD"> <mover> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>0</mn> </msub> <mo stretchy="false">&#x005E;<!-- ^ --></mo> </mover> </mrow> <mo>=</mo> <mtext>y</mtext> <mo>&#x2212;<!-- − --></mo> <mrow class="MJX-TeXAtom-ORD"> <mover> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>1</mn> </msub> <mo stretchy="false">&#x005E;<!-- ^ --></mo> </mover> </mrow> <mo>&#x00D7;<!-- × --></mo> <mtext>mean</mtext> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo> </math></p></li>
              <p>Where:</p>
              <ul>
                <li><p><math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow class="MJX-TeXAtom-ORD"> <mover> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>0</mn> </msub> <mo stretchy="false">&#x005E;<!-- ^ --></mo> </mover> </mrow> </math> is the estimated <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>y</mi> </math>-intercept.</p>
                </li>
                <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow class="MJX-TeXAtom-ORD"> <mover> <msub> <mi>&#x03B2;<!-- β --></mi> <mn>1</mn> </msub> <mo stretchy="false">&#x005E;<!-- ^ --></mo> </mover> </mrow> </math> is the estimated slope.</li>
                <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mtext>Cov</mtext> <mo stretchy="false">(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> </math> is the covariance between <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>x</mi> </math> and <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>y</mi> </math>.</li>
                <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mtext>Var</mtext> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo> </math> is the variance of <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>x</mi> </math>.</li>
                <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mtext>mean</mtext> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo> </math> is the mean of <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>x</mi> </math>.</li>
                <li><p><math xmlns="http://www.w3.org/1998/Math/MathML"> <mtext>mean</mtext> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo> </math> is the mean of <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>y</mi> </math>.</p>
              </ul>
              </li>
            </ol>
      
          <!----------------------------------->
          <h3 id ="gradient-decent">Gradient Descent method for Linear Regression:</h3>
            <figure>
              <img src="assets/img/machine-ln/gradient-discent.png" alt="" style="max-width: 70%; max-height: 70%;">
              <figcaption style="text-align: center;">Gradient descent is the process of taking steps to find the minimum of the loss surface. Image Credit: <a href="https://www.researchgate.net/figure/Non-convex-optimization-We-utilize-stochastic-gradient-descent-to-find-a-local-optimum_fig1_325142728" target="_blank">Alexander Amini</a></figcaption>
            </figure>
            <ul>
              <li>A regression model optimizes the gradient descent algorithm to update the coefficients of the line by reducing the cost function by randomly selecting coefficient values and then iteratively updating the values to reach the minimum cost function.</li>
              <li>Gradient Descent is an iterative optimization algorithm commonly used in machine learning to find the optimal parameters in a model. It can also be applied to linear regression to estimate the parameters (coefficients) that minimize the cost function.</li>
              <li>The steps involved in using Gradient Descent for Linear Regression are as follows:
                  <ol>
                    <li><strong>Define the Cost Function: </strong>The cost function for linear regression is the Mean Squared Error (MSE), which measures the average squared difference between the predicted values \(h_\theta(x^{(i)})\) and the actual values \(y^{(i)}\) of the dependent variable.</li>
                    
                    $$\text{MSE} = \frac{1}{2n} \left(y^{(i)} - h_\theta(x^{(i)})\right)^2$$ 
                    
                    <p>Where:</p>
                    <ul>
                      <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>n</mi> </math> is the number of data points.</li>
                      <li>\(y^{(i)}\) is the actual value of the dependent variable for the i-th data point.</li>
                      <li>\(h_\theta(x^{(i)})\) is the predicted value of the dependent variable for the i-th data point.</li>
                    </ul>
                    <li><strong>Initialize the Parameters: </strong>Start by initializing the parameters (coefficients) with random values. Typically, they are initialized as zero or small random values.</li>
                    <li><strong>Calculate the Gradient: </strong>Compute the gradient of the cost function with respect to each parameter. The gradient represents the direction of steepest ascent in the cost function space (for more details, see <a href="Linear-Parameter-estimation.html">Calculation of the equations</a>).
                    $$\frac{\partial (MSE)}{\partial \theta_0} = \frac{1}{n}\sum (h_\theta(x^{(i)}) - y^{(i)})$$
                    $$\frac{\partial (MSE)}{\partial \theta_i} = \frac{1}{n}\sum (h_\theta(x^{(i)}) - y^{(i)})\times x^{(i)}$$
                    <p>Where:</p>
                    <ul>
                      <li>\(\frac{\partial (MSE)}{\partial \theta_0}\) is the gradient with respect to the y-intercept parameter (\(\theta_0\)).</li>
                      <li>\(\frac{\partial (MSE)}{\partial \theta_1}\) is the gradient with respect to the slope parameter (\(\theta_1\)).</li>
                      <li>\(h_\theta(x^{(i)})\) is the predicted value of the dependent variable for the i-th data point.</li>
                      <li>\(y^{(i)}\) is the actual value of the dependent variable for the i-th data point.</li>
                      <li>\(x^{(i)}\) is the value of the independent variable for the i-th data point.</li>
                    </ul>
                    </li>
                    <li><strong>Update the Parameters: </strong> Update the parameters using the gradient and a learning rate (
                      α), which determines the step size in each iteration.
                      $$\theta_0 = \theta_0 - \alpha \times \frac{\partial (MSE)}{\partial \theta_0}$$
                      $$\theta_1 = \theta_1 - \alpha \times \frac{\partial (MSE)}{\partial \theta_1}$$
                      <p>Repeat this update process for a specified number of iterations or until the change in the cost function becomes sufficiently small.</p>
                    </li>
                    <li><strong>Predict: </strong>Once the parameters have converged or reached the desired number of iterations, use the final parameter values to make predictions on new data.
                      $$h_\theta (x_i)= \theta_0 +\theta_1 x$$
                      
                        <p>Gradient Descent iteratively adjusts the parameters by updating them in the direction of the negative gradient until it reaches a minimum point in the cost function. This process allows for the estimation of optimal parameters in linear regression, enabling the model to make accurate predictions on unseen data.</p>
                          <figure>
                            <img src="assets/img/data-engineering/optimal-reg2.png" alt="" style="max-width: 80%; max-height: auto;">
                            <figcaption></figcaption>
                          </figure>
                        <p>Let’s take an example to understand this. If we want to go from top left point of the shape to bottom of the pit, a discrete number of steps can be taken to reach the bottom.</p>
                          <ul>
                            <li>If you decide to take larger steps each time, you may achieve the bottom sooner but, there’s a probability that you could overshoot the bottom of the pit and not even near the bottom.</li>
                            <li>In the gradient descent algorithm, the number of steps you’re taking can be considered as the learning rate, and this decides how fast the algorithm converges to the minima.</li>
                          </ul>
                        <p><strong>In the gradient descent algorithm, the number of steps you’re taking can be considered as the learning rate i.e. 
                          α, and this decides how fast the algorithm converges to the minima.</strong></p>
                    </li>
                  </ol>
              </li>
            </ul>

        <br>

        <section id="evaluation-metrics-for-LR">
          <h3>Model Evaluation</h3>
          <p>To train an accurate linear regression model, we need a way to quantify how good (or bad) our model performs. In machine learning, we call such performance-measuring functions loss functions. Several popular loss functions exist for regression problems.
             To measure our model's performance, we'll use one of the most popular: mean-squared error (MSE). Here are some commonly used evaluation metrics: </p>
             
            <ol>
              <li><strong>Mean Squared Error (MSE): </strong>MSE quantifies how close a predicted value is to the true value, so we'll use it to quantify how close a regression line is to a set of points.
                The Mean Squared Error measures the average squared difference between the predicted values and the actual values of the dependent variable. It is calculated by taking the average of the squared residuals.
              $$\boxed{\text{MSE} = \frac{1}{n} \sum \left(y^{(i)} - h_\theta(x^{(i)})\right)^2}$$
              where:
              <ul>
                <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>n</mi> </math> is the number of data points.</li>
                <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mi>y</mi> <mi>i</mi> </msub> </math> is the actual value of the dependent variable for the $i-th$ data point.</li>
                <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <msub> <mrow data-mjx-texclass="ORD"> <mover> <mi>y</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>i</mi> </msub> </math> is the predicted value of the dependent variable for the i-th data point.</li>
                </ul>
                <p>A lower MSE value indicates better model performance, with zero being the best possible value.</p>
              </li>
              <li><strong>Root Mean Squared Error (RMSE): </strong>The Root Mean Squared Error is the square root of the MSE and provides a more interpretable measure of the average prediction error.
              $$\boxed{\text{RMSE} = \sqrt{\text{MSE}}}$$
              <p>Like the MSE, a lower RMSE value indicates better model performance.</p>
              </li>
              <li><strong>Mean Absolute Error (MAE): </strong>
                The Mean Absolute Error measures the average absolute difference between the predicted values and the actual values of the dependent variable. It is less sensitive to outliers compared to MSE.
                $$\boxed{\text{MAE} = \frac{1}{n} \sum |y^{(i)} - h_\theta(x^{(i)})|}$$
                <p>A lower MAE value indicates better model performance.</p>
              </li>
              <li><strong>R-squared (<math xmlns="http://www.w3.org/1998/Math/MathML"> <msup> <mi>R</mi> <mn>2</mn> </msup> </math>) Coefficient of Determination</strong>
                The R-squared value represents the proportion of the variance in the dependent variable that is explained by the independent variables. It ranges from <code>0</code> to <code>1</code>, 
                where <code>1</code> indicates that the model perfectly predicts the dependent variable. A negative <math xmlns="http://www.w3.org/1998/Math/MathML"> <msup> <mi>R</mi> <mn>2</mn> </msup> </math> means that our model is doing worse.
                $$\boxed{R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}}$$
                where:
                <ul>
                  <li><p>Residual sum of Squares (RSS) is defined as the sum of squares of the residual for each data point in the plot/data. It is the measure of the difference between the expected and the actual observed output.</p>
                    $$\text{RSS} = \sum_{i=1}^{n} \left(y^{(i)} - h_\theta(x^{(i)})\right)^2$$
                  </li>
                  <li><p>Total Sum of Squares (TSS) is defined as the sum of errors of the data points from the mean of the response variable. Mathematically TSS is</p>
                    $$\text{TSS} = \sum \left( y^{(i)}- \bar{y}\right)^2$$
                    <p>where:</p>
                    <ul>
                      <li><code>n</code> = is the number of data points.</li>
                      <li> \(h_\theta(x^{(i)})\) is the predicted value of the dependent variable from the regression model </li>
                      <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow data-mjx-texclass="ORD"> <mover> <mi>y</mi> <mo stretchy="false">¯</mo> </mover> </mrow> </math> 
                        is the mean of the observed values of the dependent variable.</li>
                    </ul>
                  </li>
                </ul>
                <p>A higher <math xmlns="http://www.w3.org/1998/Math/MathML"> <msup> <mi>R</mi> <mn>2</mn> </msup> </math>
                  value indicates a better fit of the model to the data. <math xmlns="http://www.w3.org/1998/Math/MathML"> <msup> <mi>R</mi> <mn>2</mn> </msup> </math> 
                  is commonly interpreted as the percentage of the variation in the dependent variable that is explained by the independent variables. 
                  However, it is important to note that <math xmlns="http://www.w3.org/1998/Math/MathML"> <msup> <mi>R</mi> <mn>2</mn> </msup> </math> 
                  does not determine the causal relationship between the independent and dependent variables. It is solely a measure of how well the model fits the data.</p>
                  <div style="background-color: #f2f2f2; padding: 15px;">
                    <p>
                      <strong>Note: </strong> A higher R-squared value indicates a better fit of the model to the data. However, it's essential to consider other factors and use 
                      R-squared in conjunction with other evaluation metrics to fully assess the model's performance. R-squared has limitations, especially in the case of overfitting, 
                      where a model may fit the training data very well but perform poorly on new, unseen data.
                    </p>
                  </div>
                  
              </li>
              <li><strong>Adjusted R-squared: </strong>
                <p>The Adjusted R-squared accounts for the number of independent variables in the model. It penalizes the inclusion of irrelevant variables and rewards the inclusion of relevant variables.</p>
                $$\boxed{\text{Adjusted}~ R^2 = 1-\left[\frac{(1 - R²) * (n - 1)}{(n - p - 1)}\right]}$$
                <p>Where:</p>
                <ul>
                  <li>n is the number of data points.</li>
                  <li>p is the number of independent variables.</li>
                </ul>
                <p>A higher Adjusted R-squared value indicates a better fit of the model while considering the complexity of the model.</p>
                <p>These evaluation metrics help assess the performance of a linear regression model by quantifying the accuracy of the predictions and the extent to which the independent variables explain the 
                  dependent variable. It is important to consider multiple metrics to gain a comprehensive understanding of the model's performance.</p>
              </li>
            </ol>
  
            <p></p>
            <strong>Selecting An Evaluation Metric:</strong>
            <p>Many methods exist for evaluating regression models, each with different concerns around interpretability, theory, and usability. The evaluation metric should reflect whatever it is you actually 
              care about when making predictions. For example, when we use MSE, we are implicitly saying that we think the cost of our prediction error should reflect the quadratic (squared) distance between 
              what we predicted and what is correct. This may work well if we want to punish outliers or if our data is minimized by the mean, but this comes at the cost of interpretability: we output our error 
              in squared units (though this may be fixed with RMSE). If instead we wanted our error to reflect the linear distance between what we predicted and what is correct, or we wanted our data minimized by 
              the median, we could try something like Mean Abosulte Error (MAE). Whatever the case, you should be thinking of your evaluation metric as part of your modeling process, and select the best metric based 
              on the specific concerns of your use-case.</p>
            <br>
            <strong>Are Our Coefficients Valid?: </strong>
                <p>In research publications and statistical software, coefficients of regression models are often presented with associated p-values. These p-values come from traditional null hypothesis statistical tests: t-tests are used to measure whether a given cofficient is significantly different than zero (the null hypothesis that a particular coefficient 
                  β<sub>i</sub> equals zero), while F tests are used to measure whether any of the terms in a regression model are significantly different from zero. Different opinions exist on the utility of such tests.</p>
        </section>
        
        <!----------------->
        <div class="box">
          <h5 id="Maximum-likelihood-estimation"><strong>Maximum Likelihood Estimation (MLE)</strong></h5>
          Maximum Likelihood Estimation (MLE) is a statistical method used to estimate the parameters of a statistical model. The fundamental idea behind MLE is to find the values of the model parameters that maximize the likelihood of observing the given data. In other words, MLE seeks the parameter values that make the observed data most probable under the assumed statistical model.

          <p><strong>Key concepts of MLE:</strong></p>
          <ol>
            <li><strong>Likelihood Function:</strong>
              <ul>
                <li>The likelihood function, denoted as \(L(\theta)\), is a measure of how well the model explains the observed data for a given set of parameter \(\theta\).</li>
                <li>It is defined as the joint probability (or probability density) of the observed data given the parameters:
                  $$L(\theta) = P(D | \theta),$$
                  where \(D\) is the observed data.
                </li>
              </ul>
            </li>
            <li><strong>Log-Likelihood Function: </strong>
              <ul>
                <li>In practice, it is common to work with the log-likelihood function \(l(\theta)\), which is the natural logrithm of the likelihood function.</li>
                <li>Maximizing the log-likekihood is equivalent to Maximizing the likelihood, and it simplifies computations.</li>
              </ul>
            </li>
            <li><strong>Parameter Estimation: </strong>
              <ul>
                <li>The goal of MLE is to find the values of the model parameters that maximize the likelihood (for log-likelihood) function.</li>
                <li>Mathematically, this is expressed as 
                  $$\hat{\theta}_{\text{MLE}} = \text{arg max}_\theta ~~L(\theta),$$ 
                  or 
                  $$\hat{\theta}_{\text{MLE}} = \text{arg max}_\theta ~~l(\theta).$$
                </li>
              </ul>
            </li>
            <li><strong>Optimization: </strong>
              <ul>
                <li>Typically, optimization algorithms (such as gradient descent, Newton's method, or other numerical optimization techniques) are employed to find the maximum of the likelihood function.</li>
                <li>In some cases, analytical solutions exist, making it possible to find the values of the parameters directly.</li>
              </ul>
            </li>
          </ol>

          <strong>Example:</strong> Consider a simple example of coin flipping, where we want to estimate the probability of getting heads. If we observe a sequence of coin flips (data),
          the likelihood function would be the product of the probabilities of obtaining the observed outcomes (head or tails) under the assumed 
          probability parameter \(\theta\) that maximizes this likelihood.
          <br>
          <strong>Mathematical Notation:</strong>
          <p>Likelihood FUnction: \(L(\theta) = P(D | \theta)\)</p>
          <p>Log-Likelihood FUnction: \(l(\theta) = \text{log}L(\theta)\).</p>
          
          In summary, Maximum Likelihood Estimation is a powerful method for estimating the parameters of a statistical model by maximizing the likelihood (or log-likelihood) of the observed data. It is widely used in various fields, including statistics, machine learning, and econometrics.

          <p><strong>Likelihood function for linear regression</strong></p>
          The likelihood function for linear regression is based on the assumption that the errors (\(\epsilon\)) are normally distributed. Therefore, the likelihood funstion is expressed as the product of the probabilities 
          of observing the actual data points given the parameters \(\theta_0, \theta_1,..., \theta_n\). 
          $$L(\theta) = \Pi_{i=1}^n \frac{1}{\sqrt{\pi \sigma^2}} \text{exp}\left(-\frac{y^{(i)} - h_\theta(x^{(i)})}{2\sigma^2}\right)$$
        
          where:
          <ul>
            <li>n is the number of data points</li>
            <li>\((x_i, y^{(i)})\) are the observed data pairs</li>
            <li>\(\sigma^2\) is the variance of the error term.</li>
          </ul>

          <p><strong>Log-Likelihood Function:</strong></p>
          It is defined as log of the likelihood function defined above. 

          <p><strong>MLE Objectives:</strong> The objective of MLE is to find the values of the coefficients that maximize the log-likelihood function:</p>
          $$\hat{\theta}_0, \hat{\theta}_1, ..., \hat{\theta}_n = \text{arg}~\text{max}_{\theta_0, \theta_1, ..., \theta_n} l(\theta_0, \theta_1, ..., \theta_n)$$
        
        <p><strong>Optimization:</strong> Optimization techniques, such as gradient descent or analytical solutions (set partial derivatives to zero), are used to find the values of the \(\hat{\theta}_0, \hat{\theta}_1, ..., \hat{\theta}_n\)
        that mazimize the log-likelihood. 
        </p>

        <p>In practice, the MLE estimates for the \(\theta_0, \theta_1, ..., \theta_n\) are often equivalent to the least squares estimates obtained through the minimizing the sum of squaresd residuals.</p>
        
        
        </div>
        <br>
        <div class="box">
          For more details on overfitting and underfitting go to following link: <a href="Ridge-lasso-elasticnet.html">Ridge lasso and Elastic net algorithms</a>. 
          On this page, you'll find a comprehensive guide on selecting the most relevant variables when dealing with multiple factors. The detailed description outlines strategies to identify the best variables, 
          ensuring that a specific algorithm aids in constructing a model that guards against overfitting.
        </div>
      </section>
     

      <!-----------Example ------------->
      <section id="Example-simple-linear">
        <h3><a href="https://github.com/arunp77/Machine-Learning/tree/main/Projects-ML/Reg-models" target="_blank">Example on Simple lienar method</a></h3>
        Lets consider a dataset of person with their Weight and Height. he data set can be found at: 
        <a href="https://github.com/arunp77/Machine-Learning/blob/main/Projects-ML/Reg-models/Weight_height.csv" target="_blank">Dataset</a>.
        Now we want to do the linear regression curve fitting, where we will see on the basis of model training, what would be height of a person with a given weight.
        This can be done through the gradient decent method or OLS method. We will first look at gradient decent method.
        <ul>
          <li>Lets first import the import libraries:
            <pre><Code>
              import pandas as pd 
              import matplotlib.pyplot as plt 
              import numpy as np 
              import seaborn as sns 
              %matplotlib inline 
            </Code></pre>
          </li>
          <li>Next we can load the dataset using:
            <pre><code>df = pd.read_csv('Weight_height.csv')</code></pre>
            the table will have two colmns: <pre>df.head()</pre>
            <table>
              <thead>
                  <tr>
                      <th>Weight</th>
                      <th>Height</th>
                  </tr>
              </thead>
              <tbody>
                  <tr>
                      <td>45</td>
                      <td>120</td>
                  </tr>
                  <tr>
                      <td>58</td>
                      <td>125</td>
                  </tr>
                  <tr>
                      <td>48</td>
                      <td>123</td>
                  </tr>
                  <tr>
                      <td>60</td>
                      <td>145</td>
                  </tr>
                  <tr>
                      <td>70</td>
                      <td>160</td>
                  </tr>
              </tbody>
          </table>
          </li>
          <li>The scattered and pair plot can be obtained using the '<code>seaborn</code>' pairplot: <code>sns.pairplot(df)</code>
            <figure>
              <img src="assets/img/machine-ln/pair-plot.png" alt="" style="max-width: 70%; max-height: 70%;">
              <figcaption></figcaption>
            </figure>
          </li>
          <li>Now created the independent and dependent features as:
            <pre><code>
# Independe and dependent features
X = df[['Weight']] ## Always remember that independent features should be dataframe or 2 dimesnional array
y=df['Height']   ## this variable can be a series or 1D array.
            </code></pre>
          </li>
          <li>Now deviding the dataset into the training an test dataset.
            <pre><code>
## Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)              
            </code></pre>
          </li>
          <li>It is best to standardize the training and test datasets.
            <pre><code>
              ## Standardization 
              from sklearn.preprocessing import StandardScaler
              scaler = StandardScaler()
              X_train = scaler.fit_transform(X_train)      
              X_test = scaler.transform(X_test)        
            </code></pre>
            It is to be noted here that only transfor is used in the case of X_test. This is due to the following fact.
            <code>scaler = StandardScaler()</code> this basically make norm of calulating the mean and standard deiation for the dataset. 
            However when we provide the <code>X_train</code> to the <code>scaler.fit_transform</code>, it basically calculate the mean and the standard deviation
            and then transform the <code>X_train</code>. Next when we use the same scaler function, we don't need to recalculate the mean and standard deviation.
          </li>
          <li>Now we fit the train datasets, using:
            <pre><code>
              ## Apply linear regression
              from sklearn.linear_model import LinearRegression
              regression = LinearRegression(n_jobs=-1)
              
              regression.fit(X_train, y_train) 
              print(f"Coefficient or slop is: {regression.coef_}")
              print(f"Intercept is: {regression.intercept_}")             
            </code></pre>
            The output will be
            <pre>
              Coefficient or slop is: [17.81386924]
              Intercept is: 155.88235294117646              
            </pre>
          </li>
          <li>We can see the fitted line:
            <pre><code>
              ## plot Training data plot and best fit line

              plt.scatter(X_train, y_train)
              plt.plot(X_train, regression.predict(X_train))              
            </code></pre>
          </li>
          <figure>
            <img src="assets/img/machine-ln/fit-line.png" alt="" style="max-width: 70%; max-height: 70%;">
            <figcaption></figcaption>
          </figure>
          <li>Predicteion of the test data:
            <ul>
              <li>predicted height output = intercept + coef_(Weight)</li>
              <li>y_pred_test = regression.intercept_ + regression.coef_ * X_test</li>
            </ul>
            <pre><code>
              ## Prediction for the test data
              y_pred = regression.predict(X_test)
            </code></pre>
          </li>
          <li><strong>Model testing:</strong>
          <pre><code>
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mse)

            print(f"The Mean squared error is: {mse}")
            print(f"The mean absolute error is: {mae}")
            print(f"The root mean squared error is: {rmse}")
          </code></pre>
          The output is:
          <pre>
            The Mean squared error is: 119.78211426580515
            The mean absolute error is: 9.751561944430335
            The root mean squared error is: 10.944501554013556
          </pre>
          Similalry R-squared can be caluclated as:
          <pre><code>
            from sklearn.metrics import r2_score

            score = r2_score(y_test, y_pred)
            print(f"The r-squared value for the model is= {score}")            
          </code></pre>
          and the output is: <code>The r-squared value for the model is= 0.724726708358188</code>
          Similalry adjusted R-squared can be obtained as:
          $$ \text{Adjusted R-squared} = 1- \frac{(1-R^2)(n-1)}{n-k-1}$$
          where 
          <ul>
            <li>R^2 = is the R-squared value of the model,</li>
            <li>n = The number of observations</li>
            <li>k = The number of predictor variables</li>
          </ul>
          <pre><code>
            ## displaying adjusted r squared value 
            1 - (1-score)*(len(y_test) - 1) / (len(y_test) - X_test.shape[1]-1)
          </code></pre>  
          and the adjusted r-squared value is: 0.655908385447735
        </li>
        <li>Prediction of height for a person of weight 72 Kgs.
          <pre><code>
            ### Prediction for net data
            new_weight = [[72]]
            scaled_new_weight = scaler.transform(new_weight) # since we have standardized the training data, we mush use the standardize the weight
            
            # Make the prediction
            prediction_new_height = regression.predict(scaled_new_weight)
            
            print(f"Height for the weight 72 Kg is: {prediction_new_height}")
            </code></pre>
        </li>
        The predicted height is 155.37 cms.
       </ul>
       <strong>OLS method:</strong>
       We can also use the OLS method for the above example. 
       <pre><code>
        import statsmodels.api as sm 
        model = sm.OLS(y_train, X_train).fit()
        prediction = model.predict(X_test)
        print(prediction)
       </code></pre>
       This will give the prediction points for the X_test dataset. The model summary:
       <pre><code>
        print(model.summary())
       </code></pre>
       shows that the two methods discussed gave same coefficient and the intercept. 
       <figure>
          <img src="assets/img/machine-ln/ols-method.png" alt="" style="max-width: 70%; max-height: 70%;">
          <figcaption></figcaption>
        </figure>
      </section>

      <!-----------Example ------------->
      <section id="Example-multiple-regression">
        <h3>Example on multiple regression model</h3>
        This example generates synthetic data for advertising spend, number of salespeople, and sales. It then performs multiple 
        linear regression analysis using the <code>Advertising\_Spend</code> and <code>Num\_Salespeople</code> as independent variables to predict <code>Sales</code>. 
        The code also includes visualization to compare actual vs. predicted sales.
        <ul>
          <li><strong>Importing libraries</strong>
          <pre><code>
            import numpy as np
            import pandas as pd
            import statsmodels.api as sm
            import matplotlib.pyplot as plt            
          </code></pre>
          </li>
          <li><strong>Generating random data and creating dataframe: </strong>
          <pre><code>
            # Set a random seed for reproducibility
            np.random.seed(42)
            
            # Generate synthetic data
            num_samples = 200
            advertising_spend = np.random.uniform(50, 200, num_samples)
            num_salespeople = np.random.randint(3, 10, num_samples)
            error_term = np.random.normal(0, 20, num_samples)
            
            sales = 50 + 2 * advertising_spend + 5 * num_salespeople + error_term    
            
            # Create a DataFrame
            df = pd.DataFrame({'Advertising_Spend': advertising_spend, 'Num_Salespeople': num_salespeople, 'Sales': sales})
            df.head()
          </code></pre>
          <table>
            <thead>
                <tr>
                  <th></th>
                    <th>Advertising_Spend</th>
                    <th>Num_Salespeople</th>
                    <th>Sales</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                  <td>0</td>
                    <td>106.181018</td>
                    <td>6</td>
                    <td>282.863129</td>
                </tr>
                <tr>
                  <td>1</td>
                    <td>192.607146</td>
                    <td>5</td>
                    <td>447.147707</td>
                </tr>
                <tr>
                  <td>2</td>
                    <td>159.799091</td>
                    <td>3</td>
                    <td>419.907267</td>
                </tr>
                <tr>
                  <td>3</td>
                    <td>139.798773	</td>
                    <td>6</td>
                    <td>367.697179</td>
                </tr>
                <tr>
                  <td>4</td>
                    <td>73.402796</td>
                    <td>8</td>
                    <td>211.587913</td>
                </tr>
            </tbody>
          </table>
          The pair plot of this gives: <code>sns.pairplot(df)</code>
          <figure>
            <img src="assets/img/machine-ln/pair-plot.png" alt="" style="max-width: 70%; max-height: 70%;">
            <figcaption></figcaption>
          </figure>
          and the correlation of these are: <code>df.corr()</code> which gives as output following:
          <figure>
            <img src="assets/img/machine-ln/correlation-heatmap.png" alt="" style="max-width: 70%; max-height: 70%;">
            <figcaption></figcaption>
          </figure>
          <div class="box">
            <h5>Cross validation:</h5> 
            Here we are going to use the cross-validation method to assess the performance and 
            generalizability of predictive model. The primary goal of cross-validation is to ensure that a model trained on a particular dataset can generalize 
            well to new, unseen data. It helps in estimating how well the model perform on an independent dataset. For more details, you can see the <a href="https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right" target="_blank">document (well explained)</a>,
            or you can look at <code>sklearn</code> website: <a href="https://scikit-learn.org/stable/modules/cross_validation.html" target="_blank">Cross-validation: evaluating estimator performance</a>.
            <figure>
              <img src="assets/img/machine-ln/grid_search_cross_validation.png" alt="" style="max-width: 80%; max-height: auto;">
              <figcaption style="text-align: center;"><strong>k-fold</strong></figcaption>
            </figure>
            <p>In machine learning (ML), generalization usually refers to the ability of an algorithm to be effective across various inputs. It means that the ML model does not encounter performance degradation on the new inputs from the same distribution of the training data. </p>
            <p><strong>Definition: </strong>Cross-validation is a technique for evaluating a machine learning model and testing its performance. CV is commonly used in applied ML tasks. It helps to compare and select an appropriate model for the specific predictive modeling problem.</p>
            <p>There are a lot of different techniques that may be used to cross-validate a model. Still, all of them have a similar algorithm:</p>
            <ul>
              <li>Divide the dataset into two parts: one for training, other for testing</li>
              <li>Train the model on the training set</li>
              <li>Validate the model on the test set</li>
              <li>Repeat 1-3 steps a couple of times. This number depends on the CV method that you are using</li>
            </ul>
            <p>There are plenty of CV techniques of which some of them are:</p>
            <ul>
              <li>Hold-out</li>
              <li>K-folds</li>
              <li>Leave-one-out</li>
              <li>Leave-p-out</li>
              <li>Stratified K-folds</li>
              <li>Repeated K-folds</li>
              <li>Nested K-folds</li>
              <li>Time series CV</li>         
            </ul>
            For details, see: <a href="https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right" target="_blank">Cross-validation</a>.
          </div>
          </li>
          <li><strong>Feature selection: </strong>Now let's select the features.
            <pre><code>
              X= df.iloc[:,:-1]
              y = df.iloc[:,-1]              
            </code></pre>
          </li>
          <li><strong>Standardization: </strong>We standardize the data as:
          <pre><code>
            from sklearn .preprocessing import StandardScaler
            scaler =StandardScaler()
            X_train =scaler.fit_transform(X_train)
            X_test =scaler.fit_transform(X_test)                        
          </code></pre>
          </li>
          <li><strong>Regression model:</strong>
            <pre><code>
              from sklearn.linear_model import LinearRegression
              regression = LinearRegression()
              regression.fit(X_train, y_train)                            
            </code></pre>
            We can calculate the coefficient as:
            <pre><code>
              print(regression.coef_)              
            </code></pre>
            which gave: <code>[86.62120427  6.33750727]</code>.
          </li>
          <li><strong>Cross-validation method: </strong> Here we use cross-validation method. It is not mandetory but best practice. 
            <pre><code>
              from sklearn.model_selection import cross_val_score
              validation_score = cross_val_score(regression, X_train, y_train, scoring='neg_mean_squared_error', cv=3)
              validation_score
            </code></pre>
            The output is <code>validation_score = ray([-437.35690029, -363.86846439, -321.13612303])</code>, which infact represents the MSE each. 
            The mean of these can be found as: <code>np.mean(validation_score) = -374.1204959020976 </code>.
          </li>
          <li><strong>Prediction: </strong>Now we find the prediction as follows:
            <pre><code>
              ## prediction
              y_pred = regression.predict(X_test)              
            </code></pre>
          </li>
          <li><strong>Calculating the MSE, MAE:</strong>
          <pre><code>
            from sklearn.metrics import mean_absolute_error, mean_squared_error
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            
            print(f"The Mean squared error is: {mse}")
            print(f"The mean absolute error is: {mae}")
            print(f"The root mean squared error is: {rmse}")            
          </code></pre>
          which in term gave:
          <pre>
            The Mean squared error is: 508.99064011586074
            The mean absolute error is: 17.628284263814095
            The root mean squared error is: 22.560820909618087            
          </pre>
          </li>
          <li><strong>R-squared value: </strong>
          <pre><code>
            from sklearn.metrics import r2_score

            score = r2_score(y_test, y_pred)
            print(f"The r-squared value for the model is= {score}")            
          </code></pre>
          The r-squared value for the model is= 0.9326105266469662.
          </li>
          <li><strong>Calculating the residuals and seeing it's distribution:</strong>
          <pre><code>
            residuals = y_test-y_pred
            # plot residuals
            sns.displot(residuals, kind='kde')                        
          </code></pre>
          <figure>
            <img src="assets/img/machine-ln/distribution-residuals.png" alt="" style="max-width: 80%; max-height: auto;">
            <figcaption></figcaption>
          </figure>
          Defiently not a perfect gaussian distrinution but still resembles a gaussian distribution.
          </li>
        </ul>
        <strong>OLS method:</strong>
        We can alsi use OLS method for this. 
        <pre><code>
          import statsmodels.api as sm 
          model = sm.OLS(y_train, X_train).fit()      
          prediction = model.predict(X_test)      
          preint(model.summary())        
        </code></pre>
        <figure>
          <img src="assets/img/machine-ln/multi-ols.png" alt="" style="max-width: 80%; max-height: auto;">
          <figcaption></figcaption>
        </figure>
        So we can compare the two method and we can see that the coefficient calculated in the two methods are approximately same. 

      </section>


      <!-------Reference ------->
      <section id="reference">
        <h2>References</h2>
        <ul>
          <li>My github Repositories on Remote sensing <a href="https://github.com/arunp77/Machine-Learning/" target="_blank">Machine learning</a></li>
          <li><a href="https://mlu-explain.github.io/linear-regression/" target="_blank">A Visual Introduction To Linear regression</a> (Best reference for theory and visualization).</li>
          <li>Book on Regression model: <a href="https://avehtari.github.io/ROS-Examples/" target="_blank">Regression and Other Stories</a></li>
          <li>Book on Statistics: <a href="https://hastie.su.domains/Papers/ESLII.pdf" target="_blank">The Elements of Statistical Learning</a></li>
          <li>A nice mathematical description on Simple & Multiple Linear Regression Implementation From Basic To Advanced is available at <a href="https://www.youtube.com/watch?v=KZ1mWboXE6g&list=PLZoTAELRMXVPMbdMTjwolBI0cJcvASePD&index=8&ab_channel=KrishNaik" target="_blank">youtube video</a>.</li>
          <li>A nice mathematical description on Linear Regression,Cost Function,Convergence Algorithm is available at <a href="https://www.youtube.com/watch?v=a22OPjS-4Lc&list=PLZoTAELRMXVPMbdMTjwolBI0cJcvASePD&index=6&ab_channel=KrishNaik" target="_blank">youtube video</a>.</li>
          <li>A nice mathematical description on Regression Performance Metrics,MSE,MAE,RMSE, R squared is available at <a href="https://www.youtube.com/watch?v=KZ1mWboXE6g&list=PLZoTAELRMXVPMbdMTjwolBI0cJcvASePD&index=8&ab_channel=KrishNaik" target="_blank">youtube video</a>.</li>
        </ul>
      </section>

      <hr>
      
      <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

        <h3>Some other interesting things to know:</h3>
        <ul style="list-style-type: disc; margin-left: 30px;">
            <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
            <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
        </ul>
      </div>
      <p></p>

      <div class="navigation">
          <a href="index.html#portfolio" class="clickable-box">
              <span class="arrow-left">Portfolio section</span>
          </a>
          
          <a href="portfolio-details-1.html" class="clickable-box">
              <span class="arrow-right">Content</span>
          </a>
      </div>
  </div>
</div>
</section><!-- End Portfolio Details Section -->
</main><!-- End #main --

<!-- ======= Footer ======= -->
<footer id="footer">
  <div class="container">
    <div class="copyright">
      &copy; Copyright <strong><span>Arun</span></strong>
    </div>
  </div>
</footer><!-- End  Footer -->

<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/typed.js/typed.umd.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    hljs.initHighlightingOnLoad();
  });
</script>

</body>

</html>