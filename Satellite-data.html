<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Data processing</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>


  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
  

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha384-oGqqFhf3ELCpQk69FVb6jGrwPOTR5SO5FeECBbCFgrFJzVpXJFLHc06dL/iPzCBJe" crossorigin="anonymous">

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Arun</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="https://www.linkedin.com/in/arunp77/" target="_blank" class="linkedin"><i class="bx bxl-linkedin"></i></a>
          <a href="https://github.com/arunp77" target="_blank" class="github"><i class="bx bxl-github"></i></a>
          <a href="https://twitter.com/arunp77_" target="_blank" class="twitter"><i class="bx bxl-twitter"></i></a>
          <a href="https://www.instagram.com/arunp77/" target="_blank" class="instagram"><i class="bx bxl-instagram"></i></a>
          <a href="https://arunp77.medium.com/" target="_blank" class="medium"><i class="bx bxl-medium"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
          <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
          <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
          <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
          <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
          <li><a href="index.html#language" class="nav-link scrollto"><i class="bi bi-menu-up"></i> <span>Languages</span></a></li>
          <li><a href="index.html#awards" class="nav-link scrollto"><i class="bi bi-award-fill"></i> <span>Awards</span></a></li>
          <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
          <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
          <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
          <!-- <li><a href="#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li> -->
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

<main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Data processing</h2>
          <ol>
            <li><a href="Remote-sensing-content.html" class="clickable-box"><i class="fas fa-arrow-left"></i> Remote sensing content </i></a></li>
            <li><a href="index.html#portfolio" class="clickable-box"> Go to portfolio <i class="fas fa-arrow-right"></i></a></li>
          </ol>
        </div>

      </div>
    </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
        <div class="dropdown">
            <button class="dropbtn"><strong>Shortcuts:</strong></button>
            <div class="dropdown-content">
                <ul>
                    <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                    <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                    <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                    <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                    <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(19, 18, 18);"></i> Docker</a></li>
                    <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(17, 16, 16);"></i> Jupyter-nifi</a></li>
                    <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                    <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                    <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                    <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                    <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                    <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                    <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                      <!-- Add more subsections as needed -->
                  </ul>
            </div>
          </div>
      </div>


    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">
          <div class="row gy-4">
            <div style="background-color: rgb(115, 228, 243); color: rgb(29, 27, 27); padding: 10px;">
              <h1>Process levels</h1>
          </div>
          <div class="image">
            <figure style="text-align: center;">
                <img src="assets/img/remote-sensing/sentinal=EUMESAT.png" alt="" style="max-width: 80%; max-height: 80%;">
                <br><figcaption style="text-align: center;"><strong>Image credit:</strong><a href="https://www.eumetsat.int/" target="_blank"> EUMESAT</a></figcaption>
            </figure>            
          </div>

          <section id="section-1">
            <h2>Various aspects of Satellite data collection </h2>
            The process of remote sensing, from data collection to final image processing, involves several stages. Here's a detailed description of each stage:
            <!-- <figure style="text-align: center;">
              <img src="assets/img/remote-sensing/remote-dataprocessing.png" alt="" style="max-width: 70%; max-height: 70%;">
              <figcaption style="text-align: center;"><strong>Image credit:</strong><a href="https://www.dlr.de/eoc/en/PortalData/60/Resources/dokumente/2_dfd/Final_DFD_Statusreport_2013-2021.pdf" target="_blank"> EO Data Management Systems</a></figcaption>
            </figure>   -->
            <ol>
              <li><strong>Platform and Sensor Selection: </strong>First step is to choose appropriate satellite, aircraft, or unmanned aerial Vehicle (UAV)
              platform and hence the sensors onboard on the specific objectives of the remote sensing mission. These sensors then captures data in relevat 
              spectral bands for the desired applications (example optical, infrared, radar).</li>
              <li><strong>Mission Planning and Scheduling: </strong>Next step would be to choose the orbits to cover the target area after 
                considering the lighting conditions, weather and other mission constrains.</li>
              <li><strong>Data Acquisition: </strong>Then the sensors onboards acquire remote sensing data through capturning the electromagnetic radiation in various wavelengths. When utilizing optical sensors, the collection of data is accomplished by depending on sunlight reflection and absorption. In contrast, with radar sensors, the sensors emit microwave signals, and the captured data comes from the signals that are backscattered. </li>
              <li><strong>Telemetry and Data Transmission: </strong>In the next step, the acquired data is transmitted to ground stations using telemetry systems. Telemetry data includes details such as the satellite's health, its orientation in space, power levels, and sensor status.</li>
              <li><strong> Data Preprocessing: </strong> In this step, various data corrections (radiometric and geometric distortions introduced during data acquisition) are done. Atmospheric correction are also done in this step. The normalization and standardization of pixel values are also done for consistency.</li>
              <li><strong>Image Registration:</strong> Next multiple images from different times or sensors are aligned to a common coordiante system. Also geometric distortions are corrected to enable accurate comparison and analysis.</li>
              <li><strong>Image Enhancement: </strong>Image emhancement is done to improve the visual quality and highlight specific features using techniques like contrast adjustment and histogram equalization. If required, specific spectral bands are enhanced to emphaseze certain infromation.</li>
              <li><strong>Image Classification: </strong>Pixels or regions are assigned within the images to predefined classes or land cover types. These are done by using classification algorithms based on spectral, textural and contextual features. </li>
              <li><strong>Feature Extraction:</strong>Finally relevant features are identified and extracted from the images, such as vegitation indices, water bodeis, or urban areas. Texture analysis and spatially filtering are used for this purpose. </li>
              <li><strong>Change Detection: </strong>After going through all the above steps, multiple images over times are compared to idnetify changes in land cover or other features. The algorithm is then trained using a training set which includes images from different geographical locations with similar characteristics. </li>
            </ol>


          <!--------------------------->
          <h2 id="basic-principle">Satellite Data Processing: From Raw Measurements to Calibrated Insights</h2>
          Let's delve into the details of how satellite data is processed, starting from the raw Level-0 data received by the sensors and then transforming it to Level-1 data.
          <!---------------------------->
          <h4>Satellite Data levels</h4>
          <p>Within remote sensing and its applications, there are a series of levels that are used to define the amount of processing that has been performed to provide a given dataset. Satellite data is collected at various levels, each representing a different stage of processing and refinement. Here are the common levels of satellite data (<a href="https://classroom.eumetsat.int/mod/book/tool/print/index.php?id=13919#ch621" target="_blank">reference 1</a> and <a href="https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_processing_levels.php" target="_blank">reference 2</a>):</p>

          <ol>
            <li><strong>Level 0 (L0): Raw Data:</strong> Level-0 data refers to the raw data received directly from the satellite sensors. This data is essentially the raw measurements taken by the sensors and detectors onboard the satellite. It is typically in a compressed or encoded format. The first step is to decompress or decode the raw data into a usable format for further processing. It includes unprocessed digital counts or voltage measurements. L0 data is transmitted to ground stations. It is unlikely that we will work with this level of data, especially for more modern sensors, as this data lacks information such as geo-referencing and time-referencing ancillary information. Let's understand the basic concepts here:
              <ul>
                <li><b>Electromagnetic Signals: </b>
                  <ul>
                    <li>Satellite sensors detect electromagnetic signals reflected or emitted from the Earth's surface.</li>
                    <li>These signals cover various wavelengths depending on the sensor's design (e.g., visible light, infrared, microwave).</li>
                  </ul>
                </li>
                <li><b>Detection and Digitization: </b>
                  <ul>
                    <li>The sensors convert these electromagnetic signals into electrical signals. For example, photodetectors in optical sensors generate a voltage or current proportional to the intensity of the incoming light.</li>
                    <li>These analog signals are then digitized by onboard <a href="https://www.electronics-tutorials.ws/combination/analogue-to-digital-converter.html" target="_blank">analog-to-digital converters (ADCs)</a>. The digitization process samples the analog signal at discrete intervals and quantizes the signal into digital counts, which are integers representing the signal’s amplitude.</li>
                  </ul>
                </li>
                <li><b>Pixel Format: </b>
                  <ul>
                    <li>The digitized signals are organized into a grid of pixels, where each pixel represents a specific geographical area on the Earth's surface.</li>
                    <li>Each pixel contains a digital number (DN), which is a quantized value corresponding to the detected signal's intensity.</li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><strong>Level 1 (L1):</strong> Level 1 is further divided into A and B. Level 1A is the full resolution sensor data with time-referencing, ancillary information including radiometric and geometric calibration coefficients and georeferencing parameters computed and added to the file.  Level 1B is the stage following the application of the parameters appended to the files at L1A (such as instrument calibration coefficients). This level also includes quality and classification flags.
              <ul>
                <li><b>Level 1A (Reconstructed Unprocessed Instrument Data): </b>
                  <ul>
                    <li>Decompress or decode the raw satellite data received from the sensor.
                      Apply initial sensor-specific calibration algorithms to the raw data.</li>
                    <li>Perform time-referencing and append ancillary information, including radiometric and geometric calibration coefficients and georeferencing parameters.</li>
                    <li>Conduct quality checks to identify and flag any missing or corrupted data.</li>
                    <li>Organize and store the data in a standard hierarchical format, such as HDF or NetCDF.</li>
                  </ul>
                </li>
                <li><b>Level 1B (Sensor Data Record): </b>
                <ul>
                  <li>Apply radiometric calibration to convert the raw sensor counts into physical units of radiance or reflectance.</li>
                  <li>Perform geometric corrections and geolocation to assign geographic coordinates (latitude, longitude) to each pixel.</li>
                  <li>Apply sensor-specific corrections for various effects (e.g., detector nonlinearity, spectral response, stray light).</li>
                  <li>Implement cloud detection and masking algorithms.</li>
                </ul>
                </li>
              </ul>
              <p><strong><u>Example</u>:</strong> for ocean colour this would be often referred to as the “<em><b>top of atmosphere (TOA)</b></em>” radiance [mW.m-2.sr-1.nm-1] or a meteorologist might use Level 1b visible wavelength imgery to monitor cloud formation associated with a tropical cyclone. </p> 
              <figure style="text-align: center;">
                <img src="assets/img/remote-sensing/goes-img.png" alt="" style="max-width: 50%; max-height: 50%;">
                <figcaption style="text-align: center;">GOES-East ABI Band 2 (visible) Level 1b imagery of Hurricane Ida’s landfall on August 29, 2021. (<strong>Image credit: © </strong><a href="https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_processing_levels.php" target="_blank"> NOAA</a>)</figcaption>
              </figure>
            </li>
            <li><strong>Level 2 (Environmental Data Record): </strong> Level 2 refers to derived geophysical variables.  This will have required processing to remove the atmospheric component of the signal, as well as the application of algorithms to measurements to generate other products. This is the level at which many users will use the data, particularly if they are interested in event scale processes that require the highest time and space resolution available from the data stream. 
              <ul>
                <li>Perform atmospheric correction to remove the effects of atmospheric scattering and absorption.</li>
                <li>Apply retrieval algorithms to derive geophysical parameters (e.g., land surface temperature, ocean color, aerosol optical depth) from the calibrated and atmospherically corrected radiances or reflectances.</li>
                <li>Implement quality control checks and flagging of retrieved parameters based on predefined thresholds and conditions.</li>
                <li>Apply additional corrections or adjustments specific to the derived parameter (e.g., bidirectional reflectance distribution function (BRDF) correction for land surface products).</li>
              </ul>
              <p><strong><u>Example</u>:</strong> in the case of ocean colour, the core level 2 product is the water-leaving <b><em>reflectance</em></b>, whilst chlorophyll a is the commonly derived product. For OLCI these are available both at full resolution (300m pixel size) and reduced resolution (1km pixel size). </p>
              <figure style="text-align: center;">
                <img src="assets/img/remote-sensing/level2-img.png" alt="" style="max-width: 50%; max-height: 50%;">
                <figcaption style="text-align: center;">Level 2 SNPP & NOAA-20 VIIRS AOD imagery of volcanic aerosols over eastern Australia and the Coral Sea, transported from the eruption of Hunga Tonga-Hunga Ha‘apai on January 15, 2022. (<strong>Image credit: © </strong><a href="https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_processing_levels.php" target="_blank"> NOAA</a>)</figcaption>
              </figure>
            </li>
            <li><strong>Level 3 & 4 :</strong> Levels 3 and 4 refer to binned and or merged/filled versions of the level 2 data for a given spatial or temporal resolution. 
              <ul>
                <li><strong>Level 3 (Geophysical Data Record): </strong>Level 3 data are Level 2 data that have been mapped on a uniform space-time grid, and thus have been averaged over space and/or time.
                  <ul>
                    <li>Spatially and temporally aggregate or grid the Level 2 products into regular grids or maps.</li>
                    <li>Apply additional filtering, smoothing, or gap-filling techniques to enhance the data quality and continuity.</li>
                    <li>Implement compositing or mosaicking algorithms to create multi-day, multi-sensor, or multi-resolution products.</li>
                    <li>Perform additional quality control and flagging based on the aggregated data.</li>
                  </ul>
                  <p><strong>Example: </strong> the VIIRS Level 3 AOD product is a gridded composite at 0.1° or 0.25° resolution, available daily or as a monthly average; it is derived from the daily 750 m resolution Level 2 AOD data.</p>
                  <figure style="text-align: center;">
                    <img src="assets/img/remote-sensing/level3.png" alt="" style="max-width: 50%; max-height: 50%;">
                    <figcaption style="text-align: center;">Level 3 NOAA-20 VIIRS AOD imagery at 0.1° resolution for January 3, 2020. (<strong>Image credit: © </strong><a href="https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_processing_levels.php" target="_blank"> NOAA</a>)</figcaption>
                  </figure>
                </li>
                <li><strong>Level 4 (Model Output or Results): </strong>
                  <ul>
                    <li>Assimilate the Level 2 or Level 3 products into numerical models or data assimilation systems.</li>
                    <li>Perform model simulations or forecasts using the satellite data as input or constraints.</li>
                    <li>Derive higher-level products or analyses by combining satellite data with other data sources (e.g., in-situ measurements, model outputs).</li>
                    <li>Implement validation and uncertainty quantification procedures for the model outputs or derived products.</li>
                  </ul>
                </li>
              </ul>
              <p><b>Example:</b> The level 2 data from the <a href="https://marine.copernicus.eu/" target="_blank">Copernicus Marine Data Stream (CMDS)</a> feeds in to a further component of Copernicus (the Copernicus Marine Environmental Monitoring Service - <a href="https://insitu.copernicus.eu/FactSheets/CMEMS/" target="_blank">CMEMS</a>). This data can be useful for those wanting better spatial coverage, and those looking at marine processes that happen over longer time periods. In particular merged data can be used to generate long time series from multiple sensors, which is essential for climate studies</p>
            </li>
          </ol>
          
          <p>It's important to note that not all satellite data goes through all these levels. The processing levels that data undergoes depend on the specific requirements of the remote sensing application 
            and the type of data needed for analysis. Researchers and analysts choose the appropriate level of data based on the goals and objectives of their studies.</p>

          <figure style="text-align: center;">
            <img src="assets/img/remote-sensing/Level-0-4-stages.png" alt="" style="max-width: 90%; max-height: 90%;">
            <figcaption style="text-align: center;">Level-0 to Level-1 remote sensing data transformation. (<strong>Image credit: © </strong><a href="https://arunp77.github.io/Arun-Kumar-Pandey/" target="_blank"> Arun Kumar Pandey</a>)</figcaption>
          </figure>
          <div class="important-box">
            <h5>Note on Cal/Val (calibration/validation) activities:</h5>
            So before moving to the main pre-processing step, we may also need to do following calibration:
            <ol>
              <li><b>Vicarious calibration:</b> Vicarious calibration is a technique used to calibrate satellite sensors by comparing their measurements with well-characterized ground-based or airborne measurements over specific calibration sites. These sites are carefully selected and characterized for their stable and well-known surface properties, such as reflectance or radiance. The process typically involves:
                <ul>
                  <li>Collecting simultaneous ground-based or airborne measurements over the calibration site at the time of satellite overpass.</li>
                  <li>Using radiative transfer models to simulate the at-sensor radiance or reflectance based on the ground-based measurements and atmospheric conditions.</li>
                  <li>Comparing the simulated and satellite-measured radiances/reflectances to derive calibration coefficients or adjustments for the satellite sensor.</li>
                </ul>
              </li>
              <li><b>Cross-calibration with other satellites:</b> Cross-calibration involves comparing the measurements from one satellite sensor with those from another well-calibrated satellite sensor, typically operating in similar spectral bands. This helps ensure consistency between different satellite missions and maintains long-term data records. The process involves:
                <ul>
                  <li>Identifying simultaneous <a href="http://gsics.atmos.umd.edu/bin/view/Development/SimultaneousNadirOverpassMethod" target="_blank">nadir overpasses (SNOs)</a> or <a href="https://digitalenvironment.org/nerc-embedded-digital-researcher-highlighting-the-potential-of-coincident-satellite-data/" target="_blank">coincident observations</a> between the two satellites over common surfaces or targets.</li>
                  <li>Applying appropriate corrections for spectral band differences, viewing geometry, and atmospheric conditions.</li>
                  <li>Comparing the radiance or reflectance measurements from the two sensors over the common targets.</li>
                  <li>Deriving cross-calibration coefficients or adjustments to harmonize the measurements between the two sensors.</li>
                </ul>
              </li> 
              <li><b>Product validation using ground-truth data: </b>
                This activity involves comparing satellite-derived geophysical products (e.g., atmospheric, land, or ocean products) with ground-based or in-situ measurements to assess their accuracy and validate the retrieval algorithms. The process typically includes:
                <ul>
                  <li>Collecting ground-truth measurements of the geophysical parameter of interest using various instruments and techniques (e.g., weather balloons, buoys, field campaigns).</li>
                  <li>Co-locating and matching the ground-based measurements with the satellite observations in space and time.</li>
                  <li>Applying appropriate corrections or adjustments to account for spatial and temporal differences between the satellite and ground-based measurements.</li>
                  <li>Statistically analyzing the differences between the satellite products and ground-truth data to quantify uncertainties, biases, and other performance metrics.</li>
                  <li>Using the validation results to refine and improve the satellite retrieval algorithms or flag potential issues with the satellite products.</li>
                </ul>
              </li> 
            </ol>
          </div>
        </section>

          <section id="Data-Transformation-required">
          <h3 id="Data-Transformation-required">Data Processing</h3> 
          <p>Level 0 (L0) data represents the raw, unprocessed data directly received from a satellite's sensors. Transforming Level 0 data into Level 1 and higher levels involves several essential steps to convert the raw sensor measurements into physically meaningful units. Here's a step-by-step process for this transformation:</p>
            <figure style="text-align: center;">
              <img src="assets/img/remote-sensing/Level-0-4.png" alt="" style="max-width: 90%; max-height: 90%;">
              <figcaption style="text-align: center;"><strong>Image credit:</strong><a href="https://link.springer.com/referenceworkentry/10.1007/978-0-387-36699-9_36">Processing Levels, Ron Weaver </a></figcaption>
            </figure>
            
            <ul>
              <li><strong>Level-0 to Level-1 Transformation: </strong>
                <p>The transformation from level-0 to level-1 involves converting the raw sensor data into calibrated and corrected data. This process typically includes the following steps:</p>
                <ol>
                  <li><strong>Radiometric Calibration: </strong>This step converts the digital counts or voltages recorded by the sensor into physical units of radiance or reflectance. This is done by applying calibration coefficients that are determined during sensor calibration. The calibration coefficients account for the sensor's sensitivity and response to different wavelengths of radiation. The digital counts (DN), recorded by sensors are converted to radiance using the following equation:

                    $$L = (\text{DN}- \text{Offset})\times \text{Gain}$$

                  <br>
                    where 
                    <ul>
                      <li>\(L\) is radiance (TOA spectral radiance) and is typically in units of \(W/(m^2 ~\text{srad}~ \mu \text{m})\),</li>
                      <li><code>DN</code> is a dimensionless integer and known as digital counts or Digital Number or raw sensor count (DN). It has a value in the range of <code>0</code> to <code>65535</code> for a 16-bit sensor,</li>
                      <li><code>Offset</code> (an additive calibration coefficient that accounts for the sensor's dark current or instrument offset) and,</li>
                      <li>Gain = A multiplicative calibration coefficient that accounts for the sensor's gain or sensitivity and units depends on the calibrated quantity (radiance or reflectance).
                        <ul>
                          <li><b>For radiance: </b> \((W/(m^2 ~\text{sr}~ \mu \text{m})) / \text{count}\)</li>
                          <li><b>For reflectance: </b>Unitless (reflectance values are inherently dimensionless)</li>
                        </ul>
                      </li>
                    </ul>
                    It's important to note that the exact form of the calibration equation and the units of the coefficients (offset and gain) may vary depending on the specific satellite sensor and calibration methodology used. The provided equation is a simplified example, and more complex calibration procedures may involve additional terms or different unit conventions.
                  </li>
                  <li><strong>Sensor Correction: </strong>This step corrects for sensor-specific errors and non-linearities. This may involve removing sensor bias, correcting for non-linear responses, and applying gain corrections. The goal is to ensure that the sensor data accurately represents the incoming radiation.</li>
                  <li><strong>Geometric Correction: </strong>This step corrects for distortions caused by the sensor's optics and the earth's curvature. This is done by applying a geometric transformation to the data to align it with a reference map or coordinate system. The transformation accounts for factors such as sensor orientation, lens distortion, and earth's projection.
                    <div class="box">
                      <p>Geometric correction is performed using a variety of techniques, such as polynomial fitting, map projection, and terrain correction. The goal is to remove distortions caused by the sensor's optics and the earth's curvature and to align the data with a reference map or coordinate system. For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                      </p>
                    </div>
                  </li>
                  <li><strong>Geometric Registration: </strong>This step aligns multiple level-1 data sets to a common coordinate system. This is necessary for multi-temporal or multi-sensor analyses, where data from different sources or time periods needs to be compared or overlaid.
                    <div class="box">
                      It is performed by assigning geographic coordinates (latitude and longitude) to each pixel in the image. This is done by matching the image to a reference map or coordinate system. For more details see
                      <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                    </div>
                  </li>
                </ol>
              </li>
              <li><strong>Level-1 to Level-2 Transformation: </strong>
                <p>The transformation from level-1 to level-2 involves deriving physical properties of the earth's surface and atmosphere from the calibrated and corrected data. This process typically includes the following steps:</p>
                <ol>
                  <li><strong>Atmospheric Correction: </strong>This step removes the effects of the atmosphere on the level-1 data. This is done by modeling the interaction of radiation with the atmosphere, including scattering, absorption, and emission. The goal is to obtain a more accurate representation of the earth's surface reflectance.
                  <div class="box">
                    Atmospheric correction is performed using a variety of techniques, such as the Sen2Cor algorithm or the ACOLITE algorithm. The goal is to remove the effects of the atmosphere on the level-1 data to obtain a more accurate representation of the earth's surface reflectance.
                    For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                  </div>
                  </li>
                  <li><strong>Atmospheric Parameter Retrieval: </strong>This step retrieves atmospheric parameters, such as aerosol concentration, ozone concentration, and water vapor content, from the level-1 data. This is done using algorithms that analyze the spectral characteristics of the radiation.</li>
                  <li><strong>Surface Property Retrieval: </strong>This step derives physical properties of the earth's surface, such as land cover, vegetation cover, and water content, from the level-2 data. This is done using algorithms that analyze the spectral reflectance of the surface.
                    <div class="box">
                      Surface property retrieval is performed using a variety of algorithms, such as the normalized difference vegetation index (NDVI) algorithm or the land surface temperature (LST) retrieval algorithm. The goal is to derive physical properties of the earth's surface, such as land cover, vegetation cover, and water content, from the level-2 data.
                      For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                    </div>
                  </li>
                  <li><strong>Data Validation: </strong>This step ensures the quality and accuracy of the level-2 data. This involves checking for outliers, artifacts, and inconsistencies in the data.</li>
                </ol>
              </li>
              <li><strong>Level-2 to Level-3 Transformation: </strong>
                <p>The transformation from level-2 to level-3 involves integrating level-2 data into thematic maps or geophysical products. This process typically includes the following steps:</p>
                <ol>
                  <li><strong>Mosaicking: </strong>This step combines multiple level-2 data sets into a seamless mosaic. This is necessary for large-scale analyses, where data from different regions or time periods needs to be combined.
                    <div class="box">
                      Mosaicking is performed by combining multiple level-2 data sets into a seamless mosaic. This is necessary for large-scale analyses, where data from different regions or time periods needs to be combined.
                      For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                    </div>
                  </li>
                  <li><strong>Geolocation and Projection: </strong>This step assigns geographic coordinates (latitude and longitude) to the level-3 data and projects it onto a standard map projection. This allows the data to be accurately displayed and analyzed in a geographic context.
                    <div class="box">
                      Geolocation and projection are performed by assigning geographic coordinates and projecting the data onto a standard map projection. This allows the data to be accurately displayed and analyzed in a geographic context.
                      For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                    </div>
                  </li>
                  <li><strong>Thematic Mapping: </strong>This step generates thematic maps that represent specific themes or characteristics of the earth's surface. This may involve classifying land cover, identifying vegetation types, or mapping surface water bodies.
                    <div class="box">
                      Thematic mapping is performed by classifying each pixel in the image based on its spectral characteristics. This may involve classifying land cover, identifying vegetation types, or mapping surface water bodies.
                      For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                    </div>
                  </li>
                  <li><strong>Geophysical Product Generation: </strong>This step generates geophysical products, such as digital elevation models (DEMs) or land surface temperature maps. These products provide quantitative information about the earth's surface and atmosphere.</li>
                </ol>
              </li>
              <li><strong>Level-3 to Level-4 Transformation: </strong>
                <p>The transformation from Level-3 to Level-4 involves deriving higher-level information and insights from the thematic maps and geophysical products generated at Level-3. This process typically includes the following steps:</p>
                <ol>
                  <li><strong>Data Integration and Analysis: </strong>This step integrates multiple Level-3 data sets and other relevant information sources, such as ancillary data, climate models, and socio-economic data. This integration allows for a more comprehensive understanding of the Earth's system and its interactions.
                    <div class="box">
                      Data integration and analysis involves integrating multiple Level-3 data sets and other relevant information sources, such as ancillary data, climate models, and socio-economic data. This integration allows for a more comprehensive understanding of the Earth's system and its interactions
                      For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                    </div>
                  </li>
                  <li><strong>Change Detection and Characterization: </strong>This step identifies and characterizes changes in the Earth's system over time. This may involve analyzing trends in land cover, vegetation cover, or water resources, or detecting and monitoring deforestation, urbanization, or natural disasters.
                    <div class="box">
                      Change detection and characterization involves identifying and characterizing changes in the Earth's system over time. This may involve analyzing trends in land cover, vegetation cover, or water resources, or detecting and monitoring deforestation, urbanization, or natural disasters.
                      For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                    </div>
                  </li>
                  <li><strong>Modeling and Simulation: </strong>This step develops models to simulate the behavior of the Earth's system and predict future trends. This may involve using climate models, land use models, or hydrological models to understand and anticipate changes in the environment.
                    <div class="box">
                      Modeling and simulation involves developing models to simulate the behavior of the Earth's system and predict future trends. This may involve using climate models, land use models, or hydrological models to understand and anticipate changes in the environment.
                      For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                    </div>
                  </li>
                  <li><strong>Uncertainty Assessment: </strong>This step quantifies the uncertainty associated with Level-3 and Level-4 data products. This is important for understanding the limitations and reliability of the information and for making informed decisions based on the data.
                    <div class="box">
                      Uncertainty assessment involves quantifying the uncertainty associated with Level-3 and Level-4 data products. This is important for understanding the limitations and reliability of the information and for making informed decisions based on the data.
                      For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                    </div>
                  </li>
                  <li><strong>Decision Support: </strong>This step provides decision-makers with actionable insights derived from Level-3 and Level-4 data. This may involve developing recommendations for sustainable land use practices, disaster preparedness strategies, or climate change adaptation plans.
                    <div class="box">
                      Decision support involves providing decision-makers with actionable insights derived from Level-3 and Level-4 data. This may involve developing recommendations for sustainable land use practices, disaster preparedness strategies, or climate change adaptation plans.
                      For more details see
                        <a href="Remote-sesning-image-processing.html">Remote sensing image processing page</a>.
                    </div>
                  </li>
                </ol>
              </li>
              <p>The Level-3 to Level-4 transformation represents a critical step in converting raw remote sensing data into actionable information that can be used to address environmental challenges, support sustainable development, and inform decision-making processes.</p>
              <p>The transformation of level-0 data to level-4 data is a complex process that involves a series of mathematical operations and algorithms. The specific equations and techniques used will depend on the type of sensor, the earth's surface features, and the atmospheric conditions. However, the general principles of calibration, correction, geolocation, atmospheric correction, surface property retrieval, thematic mapping, and geophysical product generation are applicable to most remote sensing data processing workflows.</p>
            </ul>
          </section>

            <section id="Voltage-values-in-Level-0-data">
            <h3>Voltage values in Level-0 data</h3>
              <p>In the context of collecting raw data from satellite sensors, "voltage values" refer to one of the ways that certain types of sensors record measurements. 
                When a sensor detects a physical quantity, such as light intensity or temperature, it often generates an electrical signal proportional to that quantity. 
                This electrical signal is typically in the form of voltage.</p>
              <p>Here's a more detailed explanation:</p>
              <ol>
                <li><p><strong>Sensor Output:</strong> Satellite sensors are designed to capture various types of data, such as imagery, temperature, or radiation levels. 
                  When these sensors interact with the environment, they produce an electrical signal that corresponds to the measured quantity.</p>
                </li>
                <li><p><strong>Voltage as a Signal:</strong> In many cases, this electrical signal is in the form of voltage. The magnitude of the voltage signal is directly 
                  related to the quantity being measured. For example, in an optical sensor, the amount of light detected can be translated into a voltage signal where higher 
                  light intensity corresponds to higher voltage values.</p>
                </li>
                <li><p><strong>Analog-to-Digital Conversion:</strong> Before transmitting the data, the analog voltage signal is often converted into a digital representation. 
                  This process is called analog-to-digital conversion (ADC). In ADC, the continuous analog voltage is sampled at discrete intervals and assigned digital values 
                  (digital counts).</p>
                </li>
                <li><p><strong>Digital Counts:</strong> These digital values, referred to as "digital counts" or simply "counts," are then transmitted as part of the raw data. 
                  Each count represents a specific voltage level recorded by the sensor during its measurement.</p>
                </li>
              </ol>
              <p>For instance, if you have an optical sensor on a satellite that measures sunlight intensity, it might generate voltage values as it detects varying levels 
                of sunlight. These voltage values are then digitized and transmitted as part of the raw data to ground stations or receivers for further processing and analysis.</p>
              <p>In summary, "voltage values" in this context refer to the electrical signals generated by satellite sensors to represent the physical quantities they are 
                designed to measure. These signals are converted into digital counts for transmission and subsequent data processing.</p>

                <h3>Reflectance</h3>

                <figure style="text-align: center;">
                  <img src="assets/img/portfolio/spectrum-1.png" alt="" style="max-width: 90%; max-height: 90%;">
                </figure>

              Reflectance in remote sensing is calculated by measuring the amount of electromagnetic radiation (light) that is reflected from a surface. The reflectance values are 
              typically expressed as a percentage or a unitless fraction. The formula for calculating reflectance is as follows:
              $$\text{Reflectance} = \frac{\text{Reflected Radiance}}{\text{Incident Radiance}} \times 100\%$$

              where:
              <ul>
                <li><strong>Reflectance: </strong>The percentage of light that is reflected by a surface. It indicates how much of the incident light is returned.</li>
                <li><strong>Reflected Radiance: </strong>The amount of electromagnetic radiation (light) that is reflected by the surface and measured by the sensor. It is often 
                  represented in radiance units. </li>
                <li><strong>Incident Radiance: </strong>The amount of electromagnetic radiation (light) that reaches the surface. It is the incoming light before interacting with the surface.</li>
              </ul>
              <p>Reflectance values range from 0% (no reflection) to 100% (complete reflection). A reflectance of 0% indicates that the surface absorbs all incident light, while a 
                reflectance of 100% means that the surface reflects all incident light.</p>
              <p>In practice, the reflectance values are often corrected for atmospheric effects and sensor characteristics. The reflectance calculation is an essential step in 
                converting raw remote sensing data into meaningful information that can be used for various applications, such as land cover classification, vegetation health 
                assessment, and environmental monitoring.</p>
              <p>The accuracy of reflectance calculations depends on factors such as sensor calibration, atmospheric correction, and the spectral characteristics of the surface 
                being observed. Remote sensing instruments equipped with different spectral bands capture the reflected radiance at specific wavelengths, allowing scientists to 
                analyze and interpret the Earth's surface properties.</p>

              <h3><strong>Spectral Indices in Remote Sensing</strong></h3>
              <p>Spectral indices in remote sensing are mathematical combinations of reflectance values from different spectral bands. These indices help highlight specific 
                features or characteristics of the Earth's surface, such as vegetation health, water content, or soil moisture.
                <figure style="text-align: center;">
                  <img src="assets/img/remote-sensing/spectral-indexes-1.png" alt="" style="max-width: 80%; max-height: 80%;">
                  <figcaption style="text-align: center;"> Spectral indices mini cubes computed from Sentinel-2 displaying a 2.56 km radius around the DE-Hai
                    site (<a href="https://www.nature.com/articles/s41597-023-02096-0" target="_blank"> <strong>Image credit: </strong> Reference paper by David Montero et. al.</a>)</figcaption>
                </figure>

                Here are a few commonly used spectral indices:</p>
              <ol>
                <li><strong>Normalized Difference Vegetation Index (NDVI):</strong><br>
                  (For reference, please see the link <a href="https://www.sciencedirect.com/topics/earth-and-planetary-sciences/normalized-difference-vegetation-index" target="_blank">Normalized Difference Vegetation Index</a>)
                  <figure style="text-align: center;">
                    <img src="assets/img/remote-sensing/NDVI.png" alt="" style="max-width: 90%; max-height: 90%;">
                  </figure>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{NDVI} = \frac{(\text{NIR} - \text{Red})}{(\text{NIR}+\text{Red})}$$
                    <li><strong>Purpose: </strong> NDVI is widely used to assess vegetation health and density. Healthy vegetation reflects more near-infrared (NIR) light and 
                      absorbs more red light.s</li>
                    <li><b>NIR (Near-Infrared):</b> Reflectance in the near-infrared region of the electromagnetic spectrum.</li>
                    <li><b>R i.e. Red:</b> Reflectance in the red region of the electromagnetic spectrum.</li>
                  </ul>
                </li>
                <li><strong>Enhanced Vegetation Index (EVI):</strong>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{EVI} = G\times \frac{(\text{NIR} - \text{Red})}{(\text{NIR}+C_1 \times \text{Red}-C_2 \times \text{Blue}+L)}$$
                    </li>
                    <li><strong>Purpose: </strong>EVI is an enhanced version of NDVI, designed to minimize atmospheric influences and improve sensitivity to vegetation.</li>
                    <li><b>G (Gain Factor):</b> A gain factor to optimize the sensitivity of the index.</li>
                    <li><b>C1, C2 (Coefficients):</b> Coefficients to correct for aerosol influences.</li>
                    <li><b>L (Canopy background adjustment):</b> Canopy background adjustment to minimize soil background influences.</li>
                  </ul>
                </li>
                <li><strong>Normalized Difference Water Index (NDWI):</strong>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{NDWI} = \frac{(\text{Green} - \text{NIR})}{(\text{Green}+\text{NIR})}$$
                    </li>
                    <li><strong>Purpose: </strong>NDWI is used to detect the presence of water. Water absorbs more in the near-infrared region, resulting in a lower reflectance value.</li>
                    <li><b>Green:</b> Reflectance in the green region of the electromagnetic spectrum.</li>
                  </ul>
                </li>
                <li><strong>Soil Adjusted Vegetation Index (SAVI):</strong>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{SAVI} = (1+L)\times \frac{(\text{NIR} - \text{Red})}{(\text{NIR}+\text{Red}+L)}$$
                    </li>
                    <li><strong>Purpose: </strong>SAVI is designed to reduce the influence of soil brightness on vegetation indices, making it useful in areas with varying soil conditions.</li>
                    <li>L (Soil adjustment factor): A soil adjustment factor to reduce the influence of soil brightness.</li>
                  </ul>
                </li>
                <li><strong>Normalized Burn Ratio (NBR):</strong>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{NBR} = \frac{(\text{NIR} - \text{SWIR})}{(\text{NIR}+\text{SWIR})}$$
                    </li>
                    <li><strong>Purpose: </strong>NBR is used for post-fire assessment, as it highlights changes in vegetation cover and health after a fire.</li>
                    <li>SWIR (Shortwave Infrared): Reflectance in the shortwave infrared region of the electromagnetic spectrum.</li>
                  </ul>
                </li>
                <li><strong>Moisture Stress Index (MSI):</strong>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{NBR} = \frac{(\text{NIR} - \text{SWIR})}{(\text{NIR}+\text{SWIR})}$$
                    </li>
                    <li><strong>Purpose:</strong> MSI helps identify areas experiencing moisture stress by leveraging differences in water absorption in the near-infrared and shortwave infrared regions.</li>
                  </ul>
                </li>
              </ol>
              <p>These indices are calculated using the reflectance values from specific spectral bands, often captured by satellite or airborne sensors. 
              By applying these indices, remote sensing analysts can extract valuable information about the environment, monitor changes, and gain insights into various ecological and agricultural parameters.</p><br>

              <h3>Remote Sensing – Spectral Indices – Applications</h3>
              <p>Remote sensing and spectral indices have various applications, especially in the realm of data science and analytics. Spectral indices are mathematical calculations applied to remote sensing data to extract meaningful information. Some applications include:</p>
              <ol>
                <li>Vegetation Monitoring: Spectral indices like NDVI (Normalized Difference Vegetation Index) are widely used to assess vegetation health and monitor changes over time.</li>
                <li>Crop Health Assessment: Remote sensing helps in evaluating crop conditions, detecting diseases, and optimizing agricultural practices using indices such as NDVI or EVI (Enhanced Vegetation Index).</li>
                <li>Land Cover Classification: Spectral indices contribute to accurate land cover mapping, aiding in urban planning, environmental monitoring, and resource management.</li>
                <li>Water Quality Monitoring: Spectral indices can be employed to assess water quality in lakes, rivers, and oceans by detecting changes in water properties.</li>
                <li>Forest Management: Remote sensing assists in forest inventory, monitoring deforestation, and assessing overall forest health using indices tailored for vegetation analysis.</li>
                <li>Climate Change Studies: Spectral indices contribute to monitoring environmental changes, such as shifts in temperature, precipitation, and vegetation patterns, supporting climate change research.</li>
                <li>Disaster Management: Remote sensing helps in assessing and managing natural disasters like floods, wildfires, and earthquakes by providing real-time data for decision-making.</li>
              </ol>

<p>In the context of your interests in data science, analytics, and data engineering, integrating and analyzing remote sensing data using the mentioned tools and technologies can enhance insights and support decision-making in these domains.</p>


              <h3><span style="color:red"><strong>Radiance or Reflectance:</strong></span></h3>
              <p>Radiance and reflectance are two fundamental radiometric quantities used in remote sensing and satellite imagery to describe the properties of the reflected 
                or emitted electromagnetic radiation from surfaces on Earth. These quantities are used to quantify the amount of light or radiation observed by remote sensing 
                sensors. Here's an explanation of each, along with their formulas:</p>
              <ol>
                <li><strong>Radiance (L):</strong>
                  <ul>
                    <li><strong>Definition:</strong> Radiance measures the amount of electromagnetic radiation per unit 
                      area, per unit solid angle, and per unit wavelength interval. It describes the radiative energy 
                      received by a sensor from a particular direction and wavelength.</li>
                    <li><strong>Units:</strong> Radiance is typically expressed in watts per square meter per 
                      steradian per micrometer (W/(m²·sr·μm)).</li>
                    <li><strong>Formula:</strong> The formula for radiance is given by:
                    
                      \[L(\lambda) = \frac{\pi \cdot R(\lambda)}{E(\lambda)}\]
                      
                      <p>where:</p>
                        <ul>
                          <li>L(λ) is the radiance at wavelength λ.</li>
                          <li>R(λ) is the radiance received by the sensor from the target.</li>
                          <li>E(λ) is the effective spectral radiance of the sensor.</li>
                        </ul>
                    </li>
                  </ul><br>
                  <div style="background-color: #66a8a56b; padding: 15px; border-radius: 5px; border: 1px solid #000;">
                      <p>In the case of satellite measurments, the objective of ocean sensors is to retreive the spectral distribution of upwelling radiance 
                        just above the sea surface, which is termed the water leaving radiance (L<sub>w</sub>). However, the sensors actually measure the Top 
                        of Atmoshphere (TOA) radiance L<sub>t</sub> and so the contribution resulting from processes such as the atmosphere such as the 
                        atmoshphere's scattering and absorption needs to be accounted for -termed Atmospheric  Correction (AC).

                    $$L_t(\lambda) = L_r(\lambda)+ L_a(\lambda)+L_{ra}(\lambda)+t(\lambda)L_{wc}(\lambda)+T(\lambda)L_{g}(\lambda)+t(\lambda)t_0(\lambda)\text{cos}(\theta_0)
                    L_{wn}(\lambda)$$

                    where</p>
                    
                    <ul>
                    <li>L<sub>r</sub> due to Rayleigh scattering</li>
                    <li>L<sub>a</sub> due to aerosol scattering</li>
                    <li>L<sub>ra</sub> due to interaction aerosols and molecules</li>
                    <li>L<sub>wc</sub> due to interaction between white caps</li>
                    <li>L<sub>g</sub> due to  interaction between  glint.</li>
                    <li>t and t<sub>0</sub> are diffusive transmmitances of the atmohsphere from the surface to the senor and from the sun to the surface.</li>
                    <li>T is the direct transmittance from surface to sensor</li>
                    <li> θ<sub>0</sub> is the solar zenith angle </li>
                    <li>L<sub>wn</sub>(λ) is the normalized water leaving radiance.</li>
                    </ul>
                  </div><br>

                <li><strong>Reflectance (ρ) or Reflectance Factor (RF):</strong>
                  <ul>
                    <li><strong>Definition:</strong> Reflectance measures the ratio of reflected light from a surface to the incident light upon it. It quantifies how much of 
                      the incoming radiation is reflected by the surface. Reflectance is usually expressed as a dimensionless value between 0 and 1, but it can be multiplied 
                      by 100 to express it as a percentage.</li>
                    <li><strong>Units:</strong> Reflectance is a dimensionless quantity or percentage.</li>
                    <li><strong>Formula:</strong> The formula for reflectance is given by:
                    $$
                    \rho(\lambda) = \frac{L_{\rm reflected(\lambda)}}{L_{\rm incident(\lambda)}}
                    $$
                    where:
                    <ul>
                      <li>ρ(λ) is the reflectance at wavelength λ.</li>
                      <li>L<sub>reflected</sub>(λ) is the radiance of the reflected light from the target.</li>
                      <li>L<sub>incident</sub>(λ) is the radiance of the incident light on the target.</li>
                    </ul>
                  </ul>
              </ol>
              <p>The calculation of reflectance often involves radiometric calibration to convert sensor radiance measurements to physical units and account for atmospheric 
                effects. Reflectance is an important quantity in remote sensing because it allows for the comparison of data collected by different sensors or at different 
                times, making it a valuable tool for monitoring changes in land cover, vegetation health, and other Earth surface properties.</p>
              <p>Note that the specific calculation of reflectance can be more complex in practice, taking into account various factors like atmospheric correction, sensor 
                characteristics, and surface properties. The formula provided here is a simplified representation, and in practice, detailed algorithms and corrections may 
                be applied to obtain accurate reflectance values from remote sensing data.</p>
              <p>(Reference: <a href="https://training.eumetsat.int/mod/book/tool/print/index.php?id=11832">https://training.eumetsat.int/mod/book/tool/print/index.php?id=11832</a>)</p>
            </section>

              <h2>Calculation of physical parameters from the Radiance:</h2>

              <p>Once you have preprocessed raw data received from a remote sensing satellite and calculated radiance, you can use this radiance data to derive several 
                important physical parameters and information about the Earth's surface and atmosphere. The specific parameters you can calculate depend on the type of 
                remote sensing data, the spectral bands used, and the sensors' characteristics. Here are some common physical parameters that can be derived from radiance data:</p>

              <ol>
                  <li><strong>Surface Temperature:</strong>
                      <ul>
                          <li>Radiance data in thermal infrared bands can be used to calculate the surface temperature of the Earth's features. This is essential for 
                            applications like land surface temperature monitoring, agriculture, and urban heat island analysis.</li>
                      </ul>
                  </li>
                  
                  <li><strong>Vegetation Indices:</strong>
                      <ul>
                          <li>Radiance data in visible and near-infrared bands can be used to calculate vegetation indices such as the Normalized Difference Vegetation 
                            Index (NDVI) or Enhanced Vegetation Index (EVI). These indices provide information about vegetation health, density, and vigor.</li>
                      </ul>
                  </li>

                  <li><strong>Ocean Color Parameters:</strong>
                      <ul>
                          <li>Radiance data over water bodies can be used to calculate ocean color parameters such as chlorophyll-a concentration, water turbidity, and 
                            suspended particulate matter. These parameters are crucial for marine and coastal studies.</li>
                      </ul>
                  </li>

                  <li><strong>Aerosol Optical Depth (AOD):</strong>
                      <ul>
                          <li>Radiance data can be used to estimate the AOD, which quantifies the amount of aerosols (dust, smoke, pollution) in the atmosphere. AOD is 
                            vital for studying air quality, climate modeling, and visibility assessments.</li>
                      </ul>
                  </li>

                  <li><strong>Atmospheric Profiles:</strong>
                      <ul>
                          <li>Radiance data can be used in conjunction with radiative transfer models to derive vertical profiles of atmospheric parameters, including 
                            temperature, humidity, and aerosol content. These profiles are valuable for atmospheric research and weather forecasting.</li>
                      </ul>
                  </li>

                  <li><strong>Snow Cover and Albedo:</strong>
                      <ul>
                          <li>Radiance data can be used to estimate snow cover extent and calculate surface albedo, which measures the reflectivity of the Earth's surface. 
                            These parameters are essential for climate studies and snowmelt modeling.</li>
                      </ul>
                  </li>

                  <li><strong>Land Surface Emissivity:</strong>
                      <ul>
                          <li>For thermal infrared data, you can calculate land surface emissivity, which is essential for accurate surface temperature retrieval and energy 
                            balance studies.</li>
                      </ul>
                  </li>

                  <li><strong>Land Cover Classification:</strong>
                      <ul>
                          <li>Radiance data can be used for land cover classification and land use mapping, providing information on the types of land features present in a 
                            particular area.</li>
                      </ul>
                  </li>

                  <li><strong>Cloud Properties:</strong>
                      <ul>
                          <li>Radiance data can be used to retrieve cloud properties such as cloud height, cloud type, and cloud cover fraction. This information is vital 
                            for weather forecasting and climate studies.</li>
                      </ul>
                  </li>

                  <li><strong>Surface Roughness:</strong>
                      <ul>
                          <li>Radiance data can be used to estimate surface roughness, which is useful for applications like agriculture, hydrology, and soil moisture monitoring.</li>
                      </ul>
                  </li>
              </ol>

              <p>It's important to note that the process of deriving these physical parameters often involves complex radiative transfer models, atmospheric correction, and 
                calibration procedures. Additionally, the availability of specific bands and sensors on the remote sensing satellite can impact the types of parameters that 
                can be calculated. Therefore, the choice of satellite and sensor should align with the specific objectives of your remote sensing analysis.</p>

              <h3>Calculation of surface temperature:</h3>

                <p>Surface temperature is often calculated using radiance data acquired in the thermal infrared (TIR) spectral range. The process involves several steps, 
                  including atmospheric correction and radiative transfer modeling. Here's an overview of how surface temperature is calculated from radiance data:</p>

                <ol>
                    <li><strong>Radiance Measurement:</strong>
                        <ul>
                            <li>Radiance data is collected by remote sensing instruments operating in the TIR spectral range. These sensors capture the thermal radiation 
                              emitted by the Earth's surface in the form of radiance values.</li>
                        </ul>
                    </li>
                    
                    <li><strong>Atmospheric Correction:</strong>
                        <ul>
                            <li>Atmospheric correction is a critical step because the atmosphere absorbs and scatters thermal radiation. To calculate surface temperature 
                              accurately, you need to remove the atmospheric effects. This is typically done using radiative transfer models or atmospheric profile data.</li>
                        </ul>
                    </li>

                    <li><strong>Emissivity Correction:</strong>
                        <ul>
                            <li>Surface emissivity is a measure of how efficiently a surface emits thermal radiation. It can vary depending on the material and the wavelength 
                              of observation. To calculate surface temperature, you need to know or estimate the emissivity of the surface in the TIR band. Emissivity values 
                              are typically assigned based on land cover classes or measured in the field.</li>
                        </ul>
                    </li>

                    <li><strong>Planck's Law:</strong>
                        <ul>
                            <li>Planck's law describes the relationship between radiance, temperature, wavelength, and emissivity. It is used to relate the radiance measured 
                              by the sensor to the surface temperature and emissivity. The formula for Planck's law is:</li>
                        </ul>
                        <img src="https://latex.codecogs.com/svg.latex?L%28%5Clambda%2C%20T%29%20%3D%20%5Cfrac%7B2%20%5Cpi%20h%20c%5E2%7D%7B%5Clambda%5E5%7D%20%5Cfrac%7B1%7D%7Be%5E%7B%5Cfrac%7Bhc%7D%7B%5Clambda%20k%20T%7D%7D%20-1%7D">
                    </li>

                    <li><strong>Iterative Optimization:</strong>
                        <ul>
                            <li>Calculating surface temperature from radiance data involves an iterative optimization process. Given the measured radiance, known or estimated 
                              emissivity, and the Planck's law equation, an iterative optimization algorithm is used to find the temperature that best fits the observed radiance. 
                              This is typically done for each pixel in the image.</li>
                        </ul>
                    </li>

                    <li><strong>Output:</strong>
                        <ul>
                            <li>The result of this process is a surface temperature map, where each pixel represents the estimated temperature of the corresponding area on 
                              the Earth's surface.</li>
                        </ul>
                    </li>
                </ol>

                <p>It's important to note that accurate surface temperature retrieval can be influenced by various factors, including atmospheric conditions, sensor 
                  characteristics, and emissivity assumptions. Therefore, careful calibration, validation, and consideration of these factors are essential to ensure 
                  the reliability of the calculated surface temperatures in remote sensing applications.</p>


                <h4>Note:</h4>

                <ul>
                    <li><strong>Power/Radiation Flux:</strong> The rate at which energy is transferred per time t is known as power: 
                      $$P = \frac{dQ}{dt} ~~~~[J s^{-1}] = W$$.
                        <ul>
                            <li>When speaking about radiant power that is emitted by, passing through, or incident on a particular surface, the term flux ϕ is more commonly used.</li>
                        </ul>
                    </li>
                
                    <li><strong>Flux Density:</strong> The term flux density E refers to the spatial density of radiant power. It is defined as the infinitesimal amount 
                      of radiant power 
                      <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mi>&#x03C6;</mi><mo>(</mo><mi>&#x1D431;</mi><mo>,</mo><mi>&#x1D714;</mi><mo>)</mo></mrow></math>
                      passing through an infinitesimal surface element dA that is aligned normal to a direction θ
                      and located at a position <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi>r</mi><mo>&rarr;</mo></mover></math> of interest.
                        
                            $$dE(\vec{r}, \vec{\theta}) = \frac{d\phi(\vec{r}, \vec{\theta})}{dA}~~~ [W m^{-2}]$$

                    </li>
                
                    <li><strong>Radiance:</strong> The radiance, i.e., a measure of the radiant intensity that originates from a small unit area <d>dA</d><sup>&perp;</sup>
                      (aligned normal to the direction θ of interest) rather than from a single point:
                        
                        $$L(\vec{r}, \vec{\theta}) = \frac{d\phi^2(\vec{r}, \vec{\theta})}{dA^\perp d\Omega}$$
                        
                    </li>
                </ul>
                
                <img src="assets/img/portfolio/image-8.png" alt="Your Image" style="max-width: 500px;"/>
                
                <p>The quantity radiance is a quantity that is characteristic for a specific point <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi>r</mi><mo>&rarr;</mo></mover></math>
                   and a direction θ. It is defined as the amount of flux passing through a unit area <d>A</d><sup>&perp;</sup> (centered at <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi>r</mi><mo>&rarr;</mo></mover></math> and aligned normal to θ) into a solid angle Ω around the direction of θ.</p>
                
                <p>(Reference: <a href="https://www.physics-in-a-nutshell.com/article/22/local-properties-of-radiation">https://www.physics-in-a-nutshell.com/article/22/local-properties-of-radiation</a>)</p>
                
              <!----------------  Reference ------------>
              <h3>Reference:</h3>
                <ul style="margin-left: 30px;">
                  <li><a href="https://www.mdpi.com/2072-4292/12/16/2597" target="_blank">Evaluation Analysis of Landsat Level-1 and Level-2 Data Products Using In Situ Measurements, Cibele Teixeira Pinto, Xin Jing, Larry Leigh.</a></li>
                  <li><a href="https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing">Earthdata NASA - Remote Sensing</a></li>
                  <li><a href="https://en.wikipedia.org/wiki/Remote_sensing">Wikipedia - Remote Sensing</a></li>
                  <li><a href="https://giovanni.gsfc.nasa.gov/giovanni/#service=DiArAvTs&starttime=2022-01-01T00:00:00Z&endtime=2023-01-31T23:59:59Z&bbox=68.1152,5.8337,96.416,37.4744&data=OMAERUVd_003_FinalAerosolAbsOpticalDepth388%2COMAERUVd_003_FinalAerosolAbsOpticalDepth500&variableFacets=dataProductObservation%3AModel%2CObservation%3B">Giovanni NASA</a></li>
                  <li><a href="https://appliedsciences.nasa.gov/get-involved/training/english/arset-satellite-data-air-quality-environmental-justice-and-equity">NASA Applied Sciences - Satellite Data</a></li>
                  <li><a href="https://en.wikipedia.org/wiki/European_Organisation_for_the_Exploitation_of_Meteorological_Satellites">Wikipedia - European Organisation for the Exploitation of Meteorological Satellites</a></li>
                  <li><a href="https://www.atmospheremooc.org/">Atmosphere MOOC</a></li>
                  <li><a href="https://www.eumetsat.int/online-learning">EUMETSAT Online Learning</a></li>
                  <li><a href="https://www.eumetsat.int/data-and-user-support/training">EUMETSAT Data and User Support - Training</a></li>
                  <li><a href="https://login.ltpy.adamplatform.eu/authentication/login/?next=/">ADAM Platform Login</a></li>
                  <li><a href="https://www.mdpi.com/1424-8220/19/20/4453" target="_blank">Multispectral Sensor Calibration and Characterization for sUAS Remote Sensing, by Baabak Mamaghani and Carl Salvaggio </a></li>
                  <li><a href="https://www.mdpi.com/2072-4292/12/16/2597" target="_blank">Evaluation Analysis of Landsat Level-1 and Level-2 Data Products Using In Situ Measurements</a></li>
                </ul>

                <div class="navigation">
                    <a href="index.html" class="clickable-box">
                        <span class="arrow-left">Go home</span>
                    </a>
                    <a href="Remote-sesning-image-processing.html" class="clickable-box">
                        <span class="arrow-right">Image processing</span>
                    </a>
                </div>
            </div>
        </div>
    </section><!-- End Portfolio Details Section -->

</main><!-- End #main -->

<!-- ======= Footer ======= -->
<footer id="footer">
  <div class="container">
    <div class="copyright">
      &copy; Copyright <strong><span>Arun</span></strong>
    </div>
  </div>
</footer><!-- End  Footer -->

<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/typed.js/typed.umd.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    hljs.initHighlightingOnLoad();
  });
</script>

</body>

</html>