<!DOCTYPE html>
<html lang="en">

<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1.0" name="viewport">

<title>KNN</title>
<meta content="" name="description">
<meta content="" name="keywords">

<!-- Favicons -->
<link href="assets/img/Favicon-1.png" rel="icon">
<link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

<!-- Google Fonts -->
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

<!-- Vendor CSS Files -->
<link href="assets/vendor/aos/aos.css" rel="stylesheet">
<link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
<link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
<link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
<link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
<link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
<!-- Creating a python code section-->
<link rel="stylesheet" href="assets/css/prism.css">
<script src="assets/js/prism.js"></script>

<!-- Template Main CSS File -->
<link href="assets/css/style.css" rel="stylesheet">

<!-- To set the icon, visit https://fontawesome.com/account-->
<script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
<!-- end of icon-->

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
======================================================== -->
</head>

<body>

<!-- ======= Mobile nav toggle button ======= -->
<i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

<!-- ======= Header ======= -->
<header id="header">
<div class="d-flex flex-column">

    <div class="profile">
    <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
    <h1 class="text-light"><a href="index.html">Arun</a></h1>
    <div class="social-links mt-3 text-center">
        <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
        <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
        <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
        <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
        <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
    </div>
    </div>

    <nav id="navbar" class="nav-menu navbar">
    <ul>
        <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
        <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
        <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
        <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
        <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
        <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
        <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
        <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
        <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
        <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
    </ul>
    </nav><!-- .nav-menu -->
</div>
</header><!-- End Header -->

<main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
    <div class="container">

    <div class="d-flex justify-content-between align-items-center">
        <h2>Machine Learning</h2>
        <ol>
        <li><a href="portfolio-details-1.html" class="clickable-box">Content section</a></li>
        <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
        </ol>
    </div>

    </div>
    </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
    <div class="dropdown">
        <button class="dropbtn"><strong>Shortcuts:</strong></button>
        <div class="dropdown-content">
            <ul>
                <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                <!-- Add more subsections as needed -->
            </ul>
        </div>
    </div>
    </div>

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
    <div class="container">
    <div class="row gy-4">
        <h1>K-Nearest Neighbors (KNN): Classification methods</h1>
        <div class="col-lg-8">
        <div class="portfolio-details-slider swiper">
            <div class="swiper-wrapper align-items-center">
            <div class="swiper-slide">
                <figure>
                    <img src="assets/img/machine-ln/classfication-knn1.png" alt="" style="max-width: 90%; max-height: auto;">
                    <figcaption style="text-align: center;"></figcaption>
                </figure>
            </div>
            </div>
        </div>
    </div>

    <div class="col-lg-4 grey-box">
        
        <div class="section-title">
        <h3>Content</h3>
        <ol>
            <li><a href="#introduction">Introduction</a></li>
            <ul>
            <li><a href="#importance">Importance of Logistic Regression</a></li>
            <li><a href="#need-of-LR">Need of Logistic Regression</a></li>
            <li><a href="#advantages">Advantages of Logistic Regression</a></li>
            <li><a href="#applications">Scope of Applications</a></li>
            </ul>
            <li><a href="#Binary">Binary Logistic Regression</a></li>
            <ul>
                <li><a href="#how-sigma">How Sigmoid function works?</a></li>
                <li><a href="#cost-func1">Cost function for binary Logistic Regression</a></li>
            </ul>
            <li><a href="#multi-class">Multi-Class Logistic Cost function</a></li>
            <ul>
                <li><a href="#one-vs-rest">One-vs-Rest (OvR) Logistic Regression</a></li>
                <li><a href="#multi-lr">Multinomial Logistic Regression (Softmax Regression)</a></li>
            </ul>
            <li><a href="#gdr">Gradient Descent Rule</a></li>
            <li><a href="#mle">Maximum Likelihood Estimation (MLE)</a></li>
            <li><a href="#con-mat">Confusion matrix </a></li>
            <li><a href="#reference">Reference</a></li>  
        </ol>
        </div>
    </div>
    </div>

    <section>
    <!-------------------- Introduction ---------------------->
    <h2 id="introdction">Introduction</h2>
    <ul>
        <li>K-Nearest Neighbors (KNN) is a simple yet powerful supervised machine learning algorithm used for classification and regression tasks.</li>
        <li>It is a non-parametric and instance-based learning algorithm, meaning it doesn't make any underlying assumptions about the distribution of data and learns directly from the training instances.</li>
        <li>The main objective of the KNN algorithm is to predict the classification of a new sample point based on data points that are separated into several individual classes. It is used in text mining, agriculture, finance, and healthcare.</li>
    </ul>
    <p></p>
    
    <!-------------------- Principle ---------------------->
    <h2 id="principle">Principle of KNN</h2>
    <p>The principle behind KNN is straightforward: objects are classified based on the majority class among their K nearest neighbors. In other words, the class label of an unseen data point is determined by the class labels of its K nearest neighbors in the feature space.</p>
    <ul>
        <li>The central idea behind KNN is to classify a new data point based on the majority class of its nearest neighbors.</li>
        <li>It assumes that similar things are close to each other. Hence, if most of the nearest neighbors of a data point belong to a certain class, the new data point is likely to belong to that class as well</li>
        <li>Let use suppose that we have a set of training observations \((x,y)\) and they capture the relationship between \(x\) and \(y\). Now our goal is to make a model and then predict the values on the basis of new datasets.</li>
        <li>In the context of \(K\)-Nearest Neighbors (KNN), our goal is to learn a function \(h : X \rightarrow Y\) that can predict the output \(y\) for an unseen observation \(x\) based on the relationships between the input features \(X\) and the corresponding output labels \(Y\) in a labeled dataset.</li>
        <li>In the K-Nearest Neighbors (KNN) algorithm, \(K\) represents the number of nearest neighbors to consider when making predictions for a new data point.</li>
        <li>When a new data point is to be classified, KNN calculate the distances between the point and all other points in the dataset. It then selects the \(K\) nearest neighbors based on these distances. The class label or value assigned to the new data point is typically determined by the majority class or avergae value among these \(K\) nearest neighbors.</li>
        <li>The choice of \(K\) is crucial in KNN, as it significantly impacts the performance of the algorithm. A smaller \(K\) value may lead to more flexible decision boundaries, potentially capturing noise in the data but being more sensitive to outliers. On the other hand, a larger \(K\) value may provide smoother decision boundaries but might not capture local patterns effectively.</li>
        <li>Selecting an appropriate \(K\) value often involves experimentation and validation using techniques such as cross-validation to ensure optimal performance for the specific dataset and problem at hand.</li>
    </ul>

    <!---------------------------------->
    <h4 id="distance">Distance metrics for k-nearest neighbours</h4>
    <p>Distance metrics are crucial in the k-Nearest Neighbours (KNN) algorithm, as they determine how similarity or dissimilarity between data points is measured. The choice of distance metric can significantly impact the performance of the KNN algorithm. Here are some standard distance metrics used in KNN:</p>
    <ol>
        <li><strong>Euclidean Distance: </strong>The most commonly used distance metric in KNN. Calculates the straight-line distance between two points in the feature space. Suitable for continuous numerical features.
        <p><strong>Formula: </strong></p>
        $$d(\vec{p},\vec{q}) = \sqrt{\sum_{i=1}^n (p_i - q_i)^2}$$
        <p>The Euclidean distance measures the straight-line distance between two points in a Euclidean space and the formula calculates the square root of the sum of squared differences between corresponding coordinates of two points.</p>
        </li>
        <li><strong> Manhattan Distance (Taxicab or City Block Distance): </strong>Calculates the distance between two points by summing the absolute differences between their coordinates along each dimension. Practical when dealing with features measured in different units.
        <p><strong>Formula: </strong></p>
        $$d(\vec{p},\vec{q}) = \sum_{i=1}^n |p_i - q_i|$$
        <p>Manhattan distance calculates the distance between two points by summing the absolute differences of their coordinates. It represents the distance a taxicab would travel to reach the destination by moving along the grid-like city blocks.</p>
        </li>
        <li><strong>Minkowski Distance: </strong>Minkowski distance is a generalization of both Euclidean and Manhattan distances.
            <p><strong>Formula: </strong></p>
            $$d(\vec{p},\vec{q}) = \left(\sum_{i=1}^n |p_i - q_i|^{r}\right)^{1/r}$$
            <p>where The parameter 'r' controls the degree of the distance metric. When \(r=1\), it reduces to the Manhattan distance, and when \(r=2\), it becomes the Euclidean distance.</p>
        </li>
        <li><strong>Chebyshev Distance (Maximum Metric): </strong>
            Chebyshev distance calculates the maximum absolute difference between the coordinates of two points.
            <p><strong>Formula:</strong></p>
            $$d(\vec{p},\vec{q}) = \max_{i}(|p_i - q_i|)$$
            <p>It represents the distance a king would travel on a chessboard to move between two squares.</p>
        </li>
        <li><strong>Hamming Distance (for categorical data):</strong> Hamming distance measures the number of positions at which the corresponding symbols are different between two strings of equal length.
            <p><strong>Formula:</strong></p>
            $$d(\vec{p},\vec{q}) = \sum_{i=1}^n \delta(p_i \neq q_i)$$
            It's commonly used for categorical variables or binary data.
        </li>
        <li><strong>Cosine Similarity: </strong> The Cosine Similarity is a metric used to measure the similarity between two vectors in a multidimensional space. It calculates the cosine of the angle between the two vectors, providing a measure of how closely the vectors align in direction, irrespective of their magnitude. The Cosine Similarity ranges from -1 to 1.
            <p><strong>Formula:</strong></p>
            $$\cos(\theta) =  \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \cdot \|\mathbf{b}\|}$$
            where:
            <ul>
                <li>\(\mathbf{a} \cdot \mathbf{b}\) represents the dot product of the two vectors.</li>
                <li>\(\|\mathbf{a}\|\) and \(\cdot \|\mathbf{b}\|\)  represent the magnitudes (or norms) of the vectors \(a\) and \(b\), respectively.</li>
            </ul>
            <p>The Cosine Similarity is widely used in text analysis, document clustering, recommendation systems, and many other applications where vector representations are used to measure similarity between entities.</p>
        </li>
        <li><strong></strong>
            <p><strong>Formula:</strong></p>
            $$d(\vec{p},\vec{q}) =$$
        </li>
    </ol>


    <!---------------------------------->
    <h4 id="when">When Do We Use the KNN Algorithm?</h4>
    <p>The K-Nearest Neighbors (KNN) algorithm is used in various scenarios where its characteristics align well with the requirements of the problem. Here are some common situations where KNN is often used:</p>
    <ul>
        <li>Data is labeled</li>
        <li>Data is noise-free</li>
        <li>Dataset is small, as KNN is a lazy learner</li>
    </ul>
    

    <!---------------------------------->
    <h4 id="why">Why Do We Need the KNN Algorithm?</h4>
    The K-Nearest Neighbors (KNN) algorithm is valuable in data science and analytics for several reasons:
    <ul>
        <li><strong>Flexibility in Data Patterns:</strong> KNN is effective in recognizing patterns in data that might not follow a linear or parametric model. It can handle complex relationships between features and target classes, making it suitable for a wide range of datasets.</li>
        <li><strong>Non-parametric Approach:</strong> Unlike some other algorithms that make assumptions about the underlying data distribution, KNN is non-parametric. It doesn't require the data to be normally distributed or have a specific shape, making it versatile and applicable in various domains.</li>
        <li><strong>Handling Multi-Class Classification:</strong> KNN can handle multi-class classification problems with ease. By considering the majority class among the nearest neighbors, it can assign a class label to a new data point based on the distribution of classes in its vicinity.</li>
        <li><strong>Adaptability to New Data:</strong> KNN is well-suited for online learning scenarios where new data points need to be incorporated into the existing model without retraining. Since KNN doesn't have a training phase and simply memorizes the training data, it can quickly adapt to changes in the dataset.</li>
        <li><strong>Interpretability:</strong> KNN predictions are intuitive and easy to interpret. The class assigned to a new data point is based on the classes of its nearest neighbors, providing insights into why a particular prediction was made.</li>
        <li><strong>Robustness to Noisy Data:</strong> KNN can handle noisy data and outliers reasonably well. Since it relies on the majority class among the nearest neighbors, outliers are less likely to significantly impact the classification results compared to some other algorithms.</li>
        <li><strong>Simple Implementation:</strong> KNN is relatively easy to implement and understand, making it accessible to beginners in machine learning and pattern recognition. Its simplicity also allows for quick experimentation and prototyping.</li>
    </ul>

    <!---------------------------------->
    <h4 id="pros">Pros of using KNN</h4>
    <ul>
        <li><strong>Simplicity:</strong> KNN is easy to understand and implement, making it accessible to beginners in machine learning and data science.</li>
        <li><strong>No Training Phase:</strong> KNN is a lazy learning algorithm, meaning it doesn't require a training phase. Instead, it memorizes the entire training dataset, making it efficient for online learning scenarios where new data points need to be incorporated without retraining.</li>
        <li><strong>Versatility:</strong> KNN can be applied to both classification and regression tasks. It can handle complex relationships between features and target variables, making it suitable for a wide range of datasets.</li>
        <li><strong>Non-parametric:</strong> KNN makes no assumptions about the underlying data distribution, making it robust to different types of data. It can capture complex patterns in the data without relying on predefined models.</li>
        <li><strong>Interpretability:</strong> Predictions made by KNN are intuitive and easy to interpret. The class label or regression value assigned to a new data point is based on the majority class or average value of its nearest neighbors.</li>
    </ul>

    <!-------------------------------------->
    <h4 id="cons">Cons of Using KNN</h4>
    <ol>
        <li><strong>Computational Complexity:</strong> As the size of the training dataset grows, the computational cost of KNN increases significantly. Calculating distances between the new data point and all existing data points in the training set can be time-consuming for large datasets.</li>

        <li><strong>Memory Usage:</strong> Since KNN memorizes the entire training dataset, it requires storing all training instances in memory. This can be problematic for datasets with a large number of features or a high-dimensional feature space, leading to high memory usage.</li>
        
        <li><strong>Prediction Time:</strong> KNN incurs a high prediction time during inference. For each new data point, KNN needs to calculate distances to all training instances and determine the nearest neighbors, which can be slow for real-time or latency-sensitive applications.</li>
        
        <li><strong>Sensitivity to Irrelevant Features:</strong> KNN considers all features equally when calculating distances between data points. Irrelevant or noisy features can negatively impact the algorithm's performance and lead to suboptimal results.</li>
        
        <li><strong>Need for Optimal K Value:</strong> The choice of the hyperparameter K significantly influences the performance of KNN. Selecting an inappropriate value of K can lead to overfitting or underfitting of the model, requiring careful tuning and validation.</li>
        
        <li><strong>Curse of Dimensionality:</strong> KNN's performance can degrade in high-dimensional feature spaces due to the curse of dimensionality. As the number of dimensions increases, the Euclidean distance between data points becomes less meaningful, making it challenging to define nearest neighbors accurately.</li>
    </ol>


    <!--------------------------------->
    <h4 id="math">How Does a KNN Algorithm Work?</h4>
    <p>The K-NN working can be explained on the basis of the below algorithm:</p>
    <ul>
        <li><strong>Step-1:</strong> Select the number K of the neighbors</li>
        <li><strong>Step-2:</strong> Calculate the Euclidean distance of K number of neighbors</li>
        <li><strong>Step-3:</strong> Take the K nearest neighbors as per the calculated Euclidean distance.</li>
        <li><strong>Step-4:</strong> Among these k neighbors, count the number of the data points in each category.</li>
        <li><strong>Step-5:</strong> Assign the new data points to that category for which the number of the neighbor is maximum.</li>
        <li><strong>Step-6:</strong> Our model is ready.</li>
    </ul>

    
    




    
    </section>

    <!----------- Reference ----------->
    <section id="reference">
    <h2>References</h2>
    <ul>
        <li>My github Repositories on Remote sensing <a href="https://github.com/arunp77/Machine-Learning/" target="_blank">Machine learning</a></li>
        <li><a href="https://mlu-explain.github.io/linear-regression/" target="_blank">A Visual Introduction To Linear regression</a> (Best reference for theory and visualization).</li>
        <li>Book on Regression model: <a href="https://avehtari.github.io/ROS-Examples/" target="_blank">Regression and Other Stories</a></li>
        <li>Book on Statistics: <a href="https://hastie.su.domains/Papers/ESLII.pdf" target="_blank">The Elements of Statistical Learning</a></li>
        <li><a href="https://www.javatpoint.com/machine-learning-naive-bayes-classifier" target="_blank">Naïve Bayes Classifier Algorithm, JAVAPoint.com</a></li>
        <li><a href="https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf">https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf</a></li>
        <li><a href="https://datahacker.rs/002-machine-learning-linear-regression-model/" target="_blank">One of the best description on Linear regression</a>.</li>
    </ul>
    </section>

    <hr>
    
    <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

    <h3>Some other interesting things to know:</h3>
    <ul style="list-style-type: disc; margin-left: 30px;">
        <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
        <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
    </ul>
    </div>
    <p></p>

    <div class="navigation">
        <a href="index.html#portfolio" class="clickable-box">
            <span class="arrow-left">Portfolio section</span>
        </a>
        
        <a href="portfolio-details-1.html" class="clickable-box">
            <span class="arrow-right">Content</span>
        </a>
    </div>
</div>
</div>
</section><!-- End Portfolio Details Section -->
</main><!-- End #main --

<!-- ======= Footer ======= -->
<footer id="footer">
  <div class="container">
    <div class="copyright">
      &copy; Copyright <strong><span>Arun</span></strong>
    </div>
  </div>
</footer><!-- End  Footer -->

<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/typed.js/typed.umd.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    hljs.initHighlightingOnLoad();
  });
</script>

</body>

</html>