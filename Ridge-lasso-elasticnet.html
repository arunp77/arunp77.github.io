<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Ridge-Lasso</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Arun</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
          <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
          <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
          <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
          <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
          <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
          <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
          <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
          <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
          <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
          <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
          <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
          <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
          <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

<main id="main">

        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs"> 
          <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
              <h2>Machine learning</h2>
              <ol>
                <li><a href="portfolio-details-1.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
              </ol>
            </div>
    
          </div>
        </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
      <div class="dropdown">
          <button class="dropbtn"><strong>Shortcuts:</strong></button>
          <div class="dropdown-content">
              <ul>
                  <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                  <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                  <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                  <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                  <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                  <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                  <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                  <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                  <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                  <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                  <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                  <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                  <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                    <!-- Add more subsections as needed -->
                </ul>
          </div>
        </div>
    </div>

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">
        <div class="row gy-4">
          <h1>Ridge Lasso and Elasticnet Machine learning algorithms</h1>
          <div class="col-lg-8">
            <div class="portfolio-details-slider swiper">
              <div class="swiper-wrapper align-items-center"> 
                <figure>
                  <img src="assets/img/machine-ln/ridge-lasso.png" alt="" style="max-width: 60%; max-height: auto;">
                  <figcaption style="text-align: center;"></figcaption>
                </figure>
              </div>
          </div>
        </div>

        <div class="col-lg-4 grey-box">
          
          <div class="section-title">
            <h3>Content</h3>
            <ol>
              <li><a href="#introduction">Introduction</a></li>
              <li><a href="#overfit-goodfit-underfit">Overfitting, Good Fit, and Underfitting in Machine Learning</a></li>
              <li><a href="#ridge">Ridge Regression</a></li>
              <li><a href="#lasso">Lasso Regression</a></li>
              <li><a href="#elasticnet">Elasticnet</a></li>
              <li><a href="Example">Example</a></li>
              <li><a href="#reference">Reference</a></li>
            </ol>
          </div>
        </div>
      </div>

      <section id="introduction">
        <h2>Introduction</h2>
        Ridge and Lasso regression are regularization techniques used in linear regression to prevent overfitting and improve the model's generalization performance. Both methods introduce a penalty term to the linear regression cost function.
        
        <section id="overfit-goodfit-underfit">
            <h3><a href="https://github.com/arunp77/Machine-Learning/blob/main/ML-Fundamental/Overfitting-underfitting.ipynb" target="_blank"> Understanding and Addressing Fitting Issues in Machine Learning Models</a></h3>
            <p>Overfitting and underfitting are two common problems encountered in machine learning. They occur when a machine learning model fails to generalize well to new data.</p> 
            <figure>
              <img src="assets/img/data-engineering/overfitting.png" alt="" style="max-width: 70%; max-height: 70%;">
              <figcaption></figcaption>
            </figure>
            <ol>
              <li><strong>Overfitting: </strong>
                <ul>
                  <li><strong>Description: </strong>Overfitting occurs when a machine learning model learns the training data too well, including the noise and irrelevant patterns. As a result, the model becomes too complex and fails to capture the underlying relationships in the data. This leads to poor performance on unseen data.</li>
                  <li><strong>Signs of overfitting: </strong> 
                    <ul>
                      <li>The model performs well on the training data but poorly on unseen data.</li>
                      <li>The model is complex and has a large number of parameters.</li>
                    </ul>
                  </li>
                  <li><strong>Causes: </strong>Too complex model, excessive training time, or insufficient regularization.</li>
                </ul>
              </li>
              
      
              <li><strong>Underfitting</strong></li> 
              <ul>
                <li><strong>Description: </strong>Underfitting occurs when a machine learning model is too simple and does not capture the underlying relationships in the data. This results in poor performance on both the training data and unseen data.</li>
                <li><strong>Signs of underfitting: </strong>
                  <ul>
                    <li>The model performs poorly on both the training data and unseen data.</li>
                    <li>The model is simple and has a small number of parameters.</li>
                  </ul>
                </li>
                <li><strong>Causes: </strong>Model complexity is too low, insufficient training, or inadequate feature representation.</li>
              </ul>
      
              <li><strong>Bias (Systematic Error):</strong></li>
              <ul>
                  <li><strong>Description:</strong> The model consistently makes predictions that deviate from the true values.</li>
                  <li><strong>Symptoms:</strong> Consistent errors in predictions across different datasets.</li>
                  <li><strong>Causes:</strong> Insufficiently complex model, inadequate feature representation, or biased training data.</li>
              </ul>
      
              <li><strong>Variance (Random Error):</strong></li>
              <ul>
                  <li><strong>Description:</strong> The model's predictions are highly sensitive to variations in the training data.</li>
                  <li><strong>Symptoms:</strong> High variability in predictions when trained on different subsets of the data.</li>
                  <li><strong>Causes:</strong> Too complex model, small dataset, or noisy training data.</li>
              </ul>
      
              <li><strong>Data Leakage:</strong></li>
              <ul>
                  <li><strong>Description:</strong> Information from the validation or test set inadvertently influences the model during training.</li>
                  <li><strong>Symptoms:</strong> Overly optimistic evaluation metrics, unrealistic performance.</li>
                  <li><strong>Causes:</strong> Improper splitting of data, using future information during training.</li>
              </ul>
      
              <li><strong>Model Instability:</strong></li>
              <ul>
                  <li><strong>Description:</strong> Small changes in the input data lead to significant changes in model predictions.</li>
                  <li><strong>Symptoms:</strong> Lack of robustness in the model's performance.</li>
                  <li><strong>Causes:</strong> Sensitivity to outliers, highly nonlinear relationships.</li>
              </ul>
      
              <li><strong>Multicollinearity:</strong></li>
              <ul>
                  <li><strong>Description:</strong> High correlation among independent variables in regression models.</li>
                  <li><strong>Symptoms:</strong> Unstable coefficient estimates, difficulty in isolating the effect of individual variables.</li>
                  <li><strong>Causes:</strong> Redundant or highly correlated features.</li>
              </ul>
      
              <li><strong>Imbalanced Data:</strong></li>
              <ul>
                  <li><strong>Description:</strong> A disproportionate distribution of classes in classification problems.</li>
                  <li><strong>Symptoms:</strong> Biased models toward the majority class, poor performance on minority classes.</li>
                  <li><strong>Causes:</strong> Inadequate representation of minority class, biased sampling.</li>
              </ul>
            </ol>
            
    
            <!-------- Preventing Overfitting    ------->
            <h4><strong>Preventing Overfitting and Underfitting</strong></h4> 
            There are a number of techniques that can be used to prevent overfitting and underfitting. These include:
            <figure>
              <img src="assets/img/data-engineering/overfitting-preventaion.png" alt="" style="max-width: 70%; max-height: 70%;">
              <figcaption></figcaption>
            </figure>
            <ul>
              <li><strong>Regularization:</strong> Regularization is a technique that penalizes complex models. This helps to prevent the model from learning the noise and irrelevant patterns in the training data. Common regularization techniques include L1 regularization, L2 regularization, and dropout.</li>
              <li><strong>Early stopping:</strong> Early stopping is a technique that stops training the model when it starts to overfit on the validation data. The validation data is a subset of the training data that is held out during training and used to evaluate the model's performance.</li>
              <li><strong>Cross-validation:</strong> Cross-validation is a technique that divides the training data into multiple folds. The model is trained on a subset of the folds and evaluated on the remaining folds. This process is repeated multiple times so that the model is evaluated on all of the data. Cross-validation can be used to select the best hyperparameters for the model.</li>
              <li><strong>Model selection:</strong> Model selection is a technique that compares different models and selects the one that performs best on the validation data. This can be done using a variety of techniques, such as k-fold cross-validation or Akaike Information Criterion (AIC).</li>
            </ul>
            <figure>
              <img src="assets/img/data-engineering/overfitting-preventaion1.png" alt="" style="max-width: 70%; max-height: 70%;">
              <figcaption></figcaption>
            </figure>
    
          </section>
        
        <!----------------------------->
        <h3 id="ridge">Ridge Regression (reduce overfitting)</h3>
        Ridge regression, also known as Tikhonov regularization or L2 regularization, adds the squared sum of the coefficients to the cost function. This regularization method is used to reduce overfitting.
         The regularization term is proportional to the square of the
        L2 norm of the coefficients:
        $$\text{Ridge Cost Function} = \frac{1}{2m}\sum_{i=1}^m\left(h_\beta(x^{(i)}) - y^{(i)}\right)^2 + \alpha \sum_{i=1}^n \beta_i^2$$
        where:
        <ul>
            <li>\(m\) is the number of training examples.</li>
            <li>\(h_\beta(x^{(i)})\) is the predicted value for the \(i-\)th example.</li>
            <li>\(y^{(i)}\) is the actual output for the \(i-\)th example.</li>
            <li>\(\beta_i\) is the coefficient associated with the \(i-\)th feature.</li>
        </ul>
        here <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>α</mi></math> is the regularization strength. Ridge regression tends to shrink the coefficients towards zero but does not lead to exact zero coefficients.
        The firat term in the above equation is nothing but 'Least Squares Cost Function' used in the gradient decent method. 
        <p><strong>Ridge optimization:</strong></p>
        The goal is to find the value of \(\beta\) that minmize the Ridge cost function. The optimization problem can be stated as:
        $$\underset{\beta}{min} \left(\frac{1}{2m}\sum_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}\right)^2 +\alpha \sum_{i=1}^n \beta_i^2\right)$$
        
        
        <figure>
            <img src="assets/img/machine-ln/lasso-jtheta.png" alt="" style="max-width: 30%; max-height: auto;">
            <figcaption style="text-align: center;">Ridge Cost function. Here \(\lambda =0\) correspond to the gradient dicent cost function. It is clear that, as \(\lambda\) increases, 
            global minima also decreases, but it will never reach 0.
            </figcaption>
          </figure>
          In simpler terms, without the last term in the Lasso cost function, the model might overfit the data by assigning too much importance to all features. However, with the addition of the last term, Lasso introduces a mechanism that can shrink some coefficients to exactly zero. This results in a more parsimonious model by effectively eliminating the influence of certain features.
          <p>Let's illustrate with an example:</p>
          <ul>
            <li>Suppose our linear regression model without Lasso regularization is:</li>
             $$h_\theta(x) = \theta_0 +\theta_1 x_1 +\theta_2 x_2+\theta_3 x_3 = 0.34+ 0.48 x_1 + 0.52 x_2 + 0.24 x_3$$
             <li>Now, when we apply Lasso regularization, it can reduce the coefficients, leading to a simpler model:</li>
             $$h_\theta(x) =  0.34+ 0.32 x_1 + 0.40 x_2 + 0.12 x_3$$
             <li>This means Lasso automatically diminishes the reliance on certain features, in this case, reducing the dependence on the last feature (\(x_3\)) which was already deemed insignificant. The regularization process aids in feature selection, promoting a more robust and interpretable model.</li>
          </ul>

        <p>In Scikit-learn, Ridge regression is implemented in the `<code>Ridge</code>` class.</p>


        <!----------------------------->
        <h3 id="lasso">Lasso Regression (feature selection)</h3>
        Lasso regression, or L1 regularization, adds the sum of the absolute values of the coefficients to the cost function. The regularization term is proportional to the L1 norm of the coefficients: 
        
        $$\text{Lasso Cost Function} = \frac{1}{2m}\sum_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}\right)^2+ \alpha \sum_{i=1}^n |\beta_i|.$$
        
        Similar to Ridge regression, <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>α</mi></math> is the regularization strength. 
        Lasso regression has the property of producing sparse models by setting some coefficients to exact zero. What it means is that the features that are not that important will automatically get deleted
        and features that are very very important will be considered.

        <h5>Lasso Optimization</h5>
        The goal is to find the value of \(\beta\) that minmize the Lasso cost function. The optimization problem can be stated as:
        $$\underset{\beta}{min} \left(\frac{1}{2m}\sum_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}\right)^2 +\alpha \sum_{i=1}^n |\beta_i|\right)$$
        
        <h5>Example:</h5>
        When you apply the Lasso method to the linear regression model:
        $$h_\theta(x) = \theta_0 +\theta_1 x_1 +\theta_2 x_2+\theta_3 x_3 = 0.34+ 0.48 x_1 + 0.52 x_2 + 0.24 x_3$$
        Lasso introduces a penalty term to the cost function that contains the absolute values of the coefficients \(\propto \alpha \sum_{i=1}^n |\beta_i|\), where
        \(\alpha\) is the regularization strength. When using Lasso, it tends to shrink the coefficients towards zero, and it may even set some coefficients exactly to zero. The process of setting Some
        coefficients to zero is what makes Lasso particulalry useful for feature selection.

        <p>In simpler terms, Lasso may lead to a model with fewer features by reducing the impact of less important features, possibly making the equation look like:</p>
        $$h_\theta(x)= 0.34+ 0.32 x_1 + 0.40 x_2 + 0.12 x_3$$
        Notice how some coefficients have been reduced, and it might indicate that the Lasso method has identified and downplayed less influential features, making the model more parsimonious and potentially improving its generalization to new data.

        <h5>Difference between the Ridge and Lasso</h5>
        <ul>
            <li><strong>Ridge:</strong> Ridge might shrink the coefficients, but it won't set them exactly to zero. It will encourage smaller values fo \(\theta_i\)</li>
            <li><strong>Lasso:</strong> Lasso might not only shrink the coefficients but could also set some of them exactly to zero. It tends to prefer sparsity in the model, making it useful for feature selection.</li>
        </ul>
        In summary, Ridge and Lasso provide different regularization techniques with Ridge being more continuous in its shrinking effect, while Lasso introduces a sparsity element by potentially eliminating some features entirely. The choice between them depends on the specific characteristics of your data and the desired properties of your model.

        <p>In Scikit-learn, Lasso regression is implemented in the '<code>Lasso</code>' class.</p>

        <p><strong>Usage in scikit-learn:</strong></p>
        <pre class="language-python"><code>
            from sklearn.linear_model import Ridge, Lasso
            from sklearn.model_selection import train_test_split
            from sklearn.preprocessing import StandardScaler
            
            # Assuming X_train, X_test, y_train, y_test are defined
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Ridge Regression
            ridge_reg = Ridge(alpha=1.0)  # Adjust alpha as needed
            ridge_reg.fit(X_train_scaled, y_train)
            ridge_score = ridge_reg.score(X_test_scaled, y_test)
            
            # Lasso Regression
            lasso_reg = Lasso(alpha=1.0)  # Adjust alpha as needed
            lasso_reg.fit(X_train_scaled, y_train)
            lasso_score = lasso_reg.score(X_test_scaled, y_test)            
        </code></pre>

        <!----------------------------->
        <h3 id="elasticnet">Elastic Net</h3>
        Elastic Net is a regularization technique that combines both L1 (Lasso) and L2 (Ridge) regularization terms in the linear regression cost function. 
        It is particularly useful when dealing with datasets that have a large number of features, and some of these features are correlated. 
        The Elastic Net cost function is a combination of the L1 and L2 regularization terms:
        
        $$\text{Elastic Net Cost Function} = \frac{1}{2m}\sum_{i=1}^m\left(h_\beta(x^{(i)}) - y^{(i)}\right)^2 + \alpha \left(\rho \sum_{i=1}^n |\beta_i| +\frac{1-\rho}{2} \sum_{i=1}^n \beta_i^2 \right)$$

        where:
        <ul>
            <li>Least Squares Cost Function is the standard least squares cost function, which measures the difference between predicted and actual values.</li>
            <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>α</mi></math> is the total regularization strength, controlling the overall amount of regularization applied.</li>
            <li><math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>ρ </mi></math> is the mixing parameter that determines the balance between the L1 and L2 regularization. It basically control the trade-off betwee the L1 and L2 regularization.
                When:
                <ul>
                    <li>\(\rho= 0 \Rightarrow\) Elastic Net is equivalent to Ridge (L2 regularization), </li>
                    <li>\(\rho= 1 \Rightarrow\) it is equivalent to Lasso (L1 regularization only). </li>
                </ul>
            </li>
        </ul>
        The terms \(\sum_{i=1}^n |\beta_i|\) and  \(\sum_{i=1}^n \beta_i^2\) represents the L1 and L2 regularization penalties applied to the coefficients \(\beta_i\) respectively. 
        The coefficients \(\beta_i\) are the parameters being optimized during the training process. 

        <p>In mathematical terms, if \(X\) is the feature matrix, \(\beta\) is the vector of coefficients, and \(y\) is the target variables, the Elastic Net cost function can be written as:</p>

        $$J(\beta) = \frac{1}{2m} \sum_{i=1}^m \left(y_i - X_i \beta\right)^2 +\alpha \left(\rho \sum_{i=1}^n |\beta_i| +\frac{1-\rho}{2}\sum_{i=1}^n \beta_i^2\right)$$

        where 
        <ul>
            <li>\(m\) = is the number of samples</li>
            <li>\(n\) is the number of features</li>
            <li>\(y_i\) is the target values</li>
            <li>\(X_i\) is the featur vectors </li>
        </ul>

        <h5>When to Use Elastic Net</h5>
        <ul>
            <li>Use Elastic Net when you suspect that there is multicollinearity among your features and you also want the benefit of feature selection.</li>
            <li>It is a more flexible regularization method as it allows you to tune the balance between L1 and L2 penalties.</li>
        </ul>

        In summary, Elastic Net provides a balanced regularization approach that incorporates the advantages of both Lasso and Ridge regression, offering flexibility in handling different types of datasets.
      </section>

      <Section id="example">
        <h3>Example project: Algerian forest fire data analysis</h3>
        In this project, we have considered the dataset on the ALgerian Forest Fires, which can be obtained from the: <a href="https://archive.ics.uci.edu/dataset/547/algerian+forest+fires+dataset" target="_blank">website</a>.
        <h5>About the dataset:</h5>
        The dataset includes 244 instances that regroup a data of two regions of ALgeria, namely the Bejaia region located in the norteast of Algeria and the Sidi Bel-abbes region located in the northwest of Algeria. 

        122 instances for each region.

        The period from June 2012 to Septermber 2012. The datset includes attributes and 1 output attribute (class). The 244 instances have been classified into fire (138 classes) and not fire (106 classes) classes.

        <h5>Github repository containg the code:</h5>
        You can also find the Jupyter notebook from <a href="https://github.com/arunp77/Machine-Learning/tree/main/Projects-ML/Reg-models" target="_blank">my GitHub repository</a>.
        <ol>
            <li>Let's first start with importing the libraries and then the dataset:
            <pre class="language-python"><code>
                import pandas as pd 
                import matplotlib.pyplot as plt 
                import numpy as np 
                import seaborn as sns 
                %matplotlib inline 

                df = pd.read_csv('Algerian_forest_fires_dataset_UPDATE.csv', header=1)
            </code></pre>
                I have kept the data file in same directory where the Jupyter notebook is present. This dataset contains two section of data on the basis of the two regions: 'Bejaia region' and 'Sidi Bel-abbes region'. We have created a combined dataset with a column added for the two regions with number 0 for first one and 1 for second region. 
                <pre class="language-python"><code>
                    # Extract data for the Bejaia region
                    bejaia_data = df.iloc[bejaia_header_row :sidi_bel_abbes_header_row].copy()
                    
                    # Extract data for the Sidi-Bel Abbes region
                    sidi_bel_abbes_data = df.iloc[sidi_bel_abbes_header_row:].copy()
                    
                    # Add a new column 'Region' with values 0 for the Bejaia region and 1 for the Sidi-Bel Abbes region
                    bejaia_data['Region'] = 0
                    sidi_bel_abbes_data['Region'] = 1         
                    # Concatenate the two DataFrames back together
                    final_df = pd.concat([bejaia_data, sidi_bel_abbes_data])
                    
                    # Reset the index of the final DataFrame
                    final_df.reset_index(drop=True, inplace=True)                               
                </code></pre>
                After doing the data engineering we will have our final data frame (for more details on it, you can look at <a href="https://github.com/arunp77/Machine-Learning/blob/main/Projects-ML/Reg-models/Algerian-fire-EDA.ipynb" target="_blank">Jupyter notebook</a>).
            </li>
            <li><strong>Extrapolatory data analysis:</strong>
                <ul>
                    <li><strong>Histogram plots:</strong>
                        <pre class="language-python"><code>
                            ## Density plot for all features
                            df_copy.hist(bins=50, figsize=(20,15), grid=False)
                            
                            plt.show()
                        </code></pre>
                        <figure>
                            <img src="assets/img/machine-ln/histogram-plot.png" alt="" style="max-width: 60%; max-height: auto;">
                            <figcaption style="text-align: center;"></figcaption>
                        </figure>
                    </li>
                    <li><strong>Pie chart for the classes:</strong>
                    <pre class="language-python"><code>
                        ## PErcentage for category
                        percentage = df_copy['Classes'].value_counts(normalize=True)*100
                                                
                        # Pie chart for the category
                        classlabels = ["Fire", "Not Fire"]
                        plt.figure(figsize=(12,7))  # Corrected the function name to plt.figure
                        plt.pie(percentage, labels=classlabels, autopct='%1.1f%%')
                        plt.title("Pie chart of classes")
                        plt.show()
                    </code></pre>
                    <figure>
                        <img src="assets/img/machine-ln/piechart.png" alt="" style="max-width: 30%; max-height: auto;">
                        <figcaption style="text-align: center;"></figcaption>
                      </figure>
                    </li>
                    <li><strong>Correlation plots:</strong>
                        <pre class="language-python"><code>
                            # Calculate the correlation matrix
                            correlation_matrix = df_copy.corr()
                            
                            # Create a mask to hide the upper triangle of the correlation matrix
                            mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)
                            
                            # Plotting the heatmap with the upper triangle masked
                            plt.figure(figsize=(12, 10))
                            sns.heatmap(correlation_matrix, annot=True, cmap='Greens', fmt=".2f", linewidths=.5, mask=mask)
                            plt.title('Trimmed Correlation Matrix Heatmap')
                            plt.show()
                        </code></pre>
                        <figure>
                            <img src="assets/img/machine-ln/correlationplot.png" alt="" style="max-width: 50%; max-height: auto;">
                            <figcaption style="text-align: center;"></figcaption>
                          </figure>
                    </li>
                    <li><strong>Checking the outliers in FWI column</strong>
                        <pre class="language-python"><code>
                            ## Box plot to see the outliers
                            sns.boxplot(df_copy['FWI'], color='y')
                            plt.show()
                        </code></pre>
                        <figure>
                            <img src="assets/img/machine-ln/outliersplot_FWI.png" alt="" style="max-width: 30%; max-height: auto;">
                            <figcaption style="text-align: center;"></figcaption>
                          </figure>
                    </li>
                    <li><strong>Monthly fire analysis:</strong>
                        <pre class="language-python"><code>
                            dftemp = final_df.loc[final_df['Region'] == 1] # here for second region it should be 0. 
                            plt.subplots(figsize=(13,6))
                            sns.countplot(x='month', hue='Classes', data=final_df)
                            plt.ylabel('Number of FIres', weight='bold')
                            plt.xlabel('Months', weight='bold')
                            plt.title("Fire analysis of Sidi-Bel Abbes region", weight='bold')
                            plt.show()
                        </code></pre>
                    </li>
                    The combined plot for the two regions can be seen here:
                    <figure>
                        <img src="assets/img/machine-ln/fire-analysis.png" alt="" style="max-width: 100%; max-height: auto;">
                        <figcaption style="text-align: center;"></figcaption>
                      </figure>
                </ul>
            </li>
            <li><strong>Ridge lasso analysis:</strong>
            In this part, we will work on ridge lasso analysis part. The original code can be found at my <a href="https://github.com/arunp77/Machine-Learning/blob/main/Projects-ML/Reg-models/Model-training.ipynb" target="_blank">github repository</a>
                <ul>
                    <li><strong>Feature scaling or standardization: </strong>
                        <pre class="language-python"><code>
                            from sklearn.preprocessing import StandardScaler
                            scaler = StandardScaler()
                            X_train_scaled =scaler.fit_transform(X_train)
                            X_test_scaled = scaler.transform(X_test)
                            plt.subplots(figsize=(15,5))
                            plt.subplot(1,2,1)
                            sns.boxplot(data = X_train)
                            plt.title("X_train before scaling")
                            plt.subplot(1,2,2)
                            sns.boxplot(data = X_train_scaled)
                            plt.title("X_train After scaling")
                            plt.show()
                        </code></pre>
                        <figure>
                            <img src="assets/img/machine-ln/box-plot-train-scaled.png" alt="" style="max-width: 100%; max-height: auto;">
                            <figcaption style="text-align: center;"></figcaption>
                        </figure>
                    </li>
                    <li><strong>Linear-regression method:</strong>
                        Here we will start with linear regression method. 
                        <pre class="language-python"><code>
                            from sklearn.linear_model import LinearRegression
                            from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
                            linreg = LinearRegression()
                            linreg.fit(X_train_scaled, y_train)
                            y_pred =linreg.predict(X_test_scaled)
                            
                            
                            mae = mean_absolute_error(y_test, y_pred)
                            mse  = mean_squared_error(y_test, y_pred)
                            score = r2_score(y_test, y_pred)
                            print(f'Mean absolute error = {mae}')
                            print(f'Mean squared error = {mse}')
                            print(f'R-squared value = {score}')
                        </code></pre>
                        Output of this is:
                        <pre>
                            Mean absolute error = 0.5468236465249976
                            Mean squared error = 0.6742766873791581
                            R-squared value = 0.9847657384266951
                        </pre>
                    </li>
                    <li><strong>Lasso Regression:</strong>
                        <pre class="language-python"><code>
                            from sklearn.linear_model import Lasso

                            from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
                            lasso = Lasso()
                            lasso.fit(X_train_scaled, y_train)
                            y_pred =lasso.predict(X_test_scaled)
                            
                            
                            mae = mean_absolute_error(y_test, y_pred)
                            mse  = mean_squared_error(y_test, y_pred)
                            score = r2_score(y_test, y_pred)
                            print(f'Mean absolute error = {mae}')
                            print(f'Mean squared error = {mse}')
                            print(f'R-squared value = {score}')
                        </code></pre>
                        The output is:
                        <pre>
                            Mean absolute error = 1.1331759949144085
                            Mean squared error = 2.2483458918974746
                            R-squared value = 0.9492020263112388   
                        </pre>
                        <strong>Lasso Cross validation:</strong> 
                        <pre class="language-python"><code>
                            from sklearn.linear_model import LassoCV
                            lassocv = LassoCV(cv=5)
                            lassocv.fit(X_train_scaled, y_train)
                            y_pred = lassocv.predict(X_test_scaled)
                        </code></pre>
                        Now we can see the LassoCV the MSE with respect to alpha 
                        <pre class="language-python"><code>
                            # Plot MSE path
                            plt.figure(figsize=(10, 6))
                            plt.plot(lassocv.alphas_, lassocv.mse_path_, ':')
                            plt.plot(lassocv.alphas_, lassocv.mse_path_.mean(axis=-1), 'k', label='Average MSE')
                            plt.axvline(lassocv.alpha_, linestyle='--', color='r', label='Selected Alpha')
                            plt.legend()
                            plt.xscale('log')
                            plt.xlabel('Alpha (Regularization Strength)')
                            plt.ylabel('Mean Squared Error (MSE)')
                            plt.title('LassoCV Mean Squared Error Path')
                            plt.show()
                        </code></pre>
                        <figure>
                            <img src="assets/img/machine-ln/lasso-cv.png" alt="" style="max-width: 100%; max-height: auto;">
                            <figcaption style="text-align: center;"></figcaption>
                        </figure>
                    </li>
                    <li><strong>Ridge regression model:</strong>
                    <pre class="language-python"><code>
                        from sklearn.linear_model import Ridge

                        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
                        ridge = Ridge()
                        ridge.fit(X_train_scaled, y_train)
                        y_pred =ridge.predict(X_test_scaled)
                        
                        
                        mae = mean_absolute_error(y_test, y_pred)
                        mse  = mean_squared_error(y_test, y_pred)
                        score = r2_score(y_test, y_pred)
                        print(f'Mean absolute error = {mae}')
                        print(f'Mean squared error = {mse}')
                        print(f'R-squared value = {score}')
                        </code></pre>
                        The output of this is:
                        <pre>
                            Mean absolute error = 0.5642305340105693
                            Mean squared error = 0.6949198918152067
                            R-squared value = 0.9842993364555513
                        </pre>
                        <strong>Ridge cross validation:</strong>
                        <pre class="language-python"><code>
                            from sklearn.linear_model import RidgeCV 
                            ridgecv = RidgeCV(cv=5)
                            ridgecv.fit(X_train_scaled, y_train)
                            y_pred = ridgecv.predict(X_test_scaled)

                            mae = mean_absolute_error(y_test, y_pred)
                            score = r2_score(y_test, y_pred)
                            print(f"Mean absolute error={mae}")
                            print(f"R-squared value = {score}")
                        </code></pre>
                        The output of this:
                        <pre>
                            Mean absolute error=0.5642305340105693
                            R-squared value = 0.9842993364555513
                        </pre>
                    </li>
                    <li><strong>Elastic Net regression:</strong>
                        <pre class="language-python"><code>
                            from sklearn.linear_model import ElasticNet

                            from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
                            elastic = ElasticNet()
                            elastic.fit(X_train_scaled, y_train)
                            y_pred =elastic.predict(X_test_scaled)
                            
                            
                            mae = mean_absolute_error(y_test, y_pred)
                            mse  = mean_squared_error(y_test, y_pred)
                            score = r2_score(y_test, y_pred)
                            print(f'Mean absolute error = {mae}')
                            print(f'Mean squared error = {mse}')
                            print(f'R-squared value = {score}')
                        </code></pre>
                        The output of this is:
                        <pre>
                            Mean absolute error = 1.8822353634896
                            Mean squared error = 5.517251101025224
                            R-squared value = 0.8753460589519703
                        </pre>
                        <strong>Elasticnet cross validation:</strong>
                        <pre class="language-python"><code>
                            from sklearn.linear_model import ElasticNetCV

                            from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
                            elasticcv = ElasticNetCV()
                            elasticcv.fit(X_train_scaled, y_train)
                            y_pred =elasticcv.predict(X_test_scaled)

                            mae = mean_absolute_error(y_test, y_pred)
                            mse  = mean_squared_error(y_test, y_pred)
                            score = r2_score(y_test, y_pred)
                            print(f'Mean absolute error = {mae}')
                            print(f'Mean squared error = {mse}')
                            print(f'R-squared value = {score}')
                        </code></pre>
                        The output is:
                        <pre>
                            Mean absolute error = 0.6575946731430904
                            Mean squared error = 0.8222830416276272
                            R-squared value = 0.9814217587854941
                        </pre>
                        Variation of Mean squared error with respect to various values of alphas can be seen here:
                        <pre class="language-python"><code>
                            # Plot MSE path
                            plt.figure(figsize=(10, 6))
                            plt.plot(lassocv.alphas_, lassocv.mse_path_, ':')
                            plt.plot(elasticcv.alphas_, elasticcv.mse_path_.mean(axis=-1), 'k', label='Average MSE')
                            plt.axvline(lassocv.alpha_, linestyle='--', color='r', label='Selected Alpha')
                            plt.legend()
                            plt.xscale('log')
                            plt.xlabel('Alpha (Regularization Strength)')
                            plt.ylabel('Mean Squared Error (MSE)')
                            plt.title('Elasticnet Cross validation Mean Squared Error Path')
                            plt.show()
                        </code></pre>
                        <figure>
                            <img src="assets/img/machine-ln/elasticnet-cv.png" alt="" style="max-width: 100%; max-height: auto;">
                            <figcaption style="text-align: center;"></figcaption>
                        </figure>
                    </li>
                </ul>
            </li>
        </ol>
      </Section>
      <!-------Reference ------->
      <section id="reference">
        <h2>References</h2>
        <ul>
          <li>My github Repositories on <a href="https://github.com/arunp77/Machine-Learning/" target="_blank">Machine learning</a>.</li>
          <li>A Brief description with all individual concpet can be found in my <a href="https://github.com/arunp77/Machine-Learning/blob/main/Projects-ML/Reg-models/Project-2_Advertising.ipynb" target="_blank">Jupyter notebook</a> (Best reference for theory and visualization).</li>
          <li><a href="https://thaddeus-segura.com/lasso-ridge/" target="_blank">Lasso & Ridge Regression in 200 words</a>.</li>
          <li><a href="https://mlu-explain.github.io/linear-regression/" target="_blank">A Visual Introduction To Linear regression</a> (Good reference).</li> 
          <li>Book on Regression model: <a href="https://avehtari.github.io/ROS-Examples/" target="_blank">Regression and Other Stories</a></li>
          <li>Book on Statistics: <a href="https://hastie.su.domains/Papers/ESLII.pdf" target="_blank">The Elements of Statistical Learning</a></li>
          <li>A nice mathematical description is available at <a href="https://www.youtube.com/watch?v=lv5IEOItgWM&t=1278s&ab_channel=KrishNaik" target="_blank">youtube video</a>.</li>
          <li><a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html" target="_blank"> Lasso model selection and cross validation</a></li>
        </ul>
      </section>

      <hr>
      
      <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

        <h3>Some other interesting things to know:</h3>
        <ul style="list-style-type: disc; margin-left: 30px;">
            <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
            <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
        </ul>
      </div>
      <p></p>

      <div class="navigation">
          <a href="index.html#portfolio" class="clickable-box">
              <span class="arrow-left">Portfolio section</span>
          </a>
          
          <a href="portfolio-details-1.html" class="clickable-box">
              <span class="arrow-right">Content</span>
          </a>
      </div>
  </div>
</div>
</section><!-- End Portfolio Details Section -->
</main><!-- End #main --

<!-- ======= Footer ======= -->
<footer id="footer">
  <div class="container">
    <div class="copyright">
      &copy; Copyright <strong><span>Arun</span></strong>
    </div>
  </div>
</footer>
<!-- End  Footer -->

<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/typed.js/typed.umd.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    hljs.initHighlightingOnLoad();
  });
</script>

</body>

</html>