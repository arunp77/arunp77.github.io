<!DOCTYPE html>
<html lang="en">

<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1.0" name="viewport">

<title>Logistic Regression</title>
<meta content="" name="description">
<meta content="" name="keywords">

<!-- Favicons -->
<link href="assets/img/Favicon-1.png" rel="icon">
<link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

<!-- Google Fonts -->
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

<!-- Vendor CSS Files -->
<link href="assets/vendor/aos/aos.css" rel="stylesheet">
<link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
<link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
<link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
<link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
<link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
<!-- Creating a python code section-->
<link rel="stylesheet" href="assets/css/prism.css">
<script src="assets/js/prism.js"></script>

<!-- Template Main CSS File -->
<link href="assets/css/style.css" rel="stylesheet">

<!-- To set the icon, visit https://fontawesome.com/account-->
<script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
<!-- end of icon-->

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
======================================================== -->
</head>

<body>

<!-- ======= Mobile nav toggle button ======= -->
<i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

<!-- ======= Header ======= -->
<header id="header">
<div class="d-flex flex-column">

    <div class="profile">
    <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
    <h1 class="text-light"><a href="index.html">Arun</a></h1>
    <div class="social-links mt-3 text-center">
        <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
        <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
        <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
        <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
        <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
    </div>
    </div>

    <nav id="navbar" class="nav-menu navbar">
    <ul>
        <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
        <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
        <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
        <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
        <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
        <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
        <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
        <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
        <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
        <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
    </ul>
    </nav><!-- .nav-menu -->
</div>
</header><!-- End Header -->

<main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
    <div class="container">

    <div class="d-flex justify-content-between align-items-center">
        <h2>Machine Learning</h2>
        <ol>
        <li><a href="portfolio-details-1.html" class="clickable-box">Content section</a></li>
        <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
        </ol>
    </div>

    </div>
    </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
    <div class="dropdown">
        <button class="dropbtn"><strong>Shortcuts:</strong></button>
        <div class="dropdown-content">
            <ul>
                <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                <!-- Add more subsections as needed -->
            </ul>
        </div>
    </div>
    </div>

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
    <div class="container">
    <div class="row gy-4">
        <h1>Logistic Regression: Classification methods</h1>
        <div class="col-lg-8">
        <div class="portfolio-details-slider swiper">
            <div class="swiper-wrapper align-items-center">
            <div class="swiper-slide">
                <figure>
                    <img src="assets/img/machine-ln/logistic-fun-3d.jpg" alt="" style="max-width: 70%; max-height: auto;">
                    <figcaption style="text-align: center;"></figcaption>
                </figure>
            </div>
            </div>
        </div>
    </div>

    <div class="col-lg-4 grey-box">
        
        <div class="section-title">
        <h3>Content</h3>
        <ol>
            <li><a href="#introduction">Introduction</a></li>
            <ul>
            <li><a href="#importance">Importance of Logistic Regression</a></li>
            <li><a href="#need-of-LR">Need of Logistic Regression</a></li>
            <li><a href="#advantages">Advantages of Logistic Regression</a></li>
            <li><a href="#applications">Scope of Applications</a></li>
            </ul>
            <li><a href="#Binary">Binary Logistic Regression</a></li>
            <ul>
                <li><a href="#how-sigma">How Sigmoid function works?</a></li>
                <li><a href="#cost-func1">Cost function for binary Logistic Regression</a></li>
            </ul>
            <li><a href="#multi-class">Multi-Class Logistic Cost function</a></li>
            <ul>
                <li><a href="#one-vs-rest">One-vs-Rest (OvR) Logistic Regression</a></li>
                <li><a href="#multi-lr">Multinomial Logistic Regression (Softmax Regression)</a></li>
            </ul>
            <li><a href="#gdr">Gradient Descent Rule</a></li>
            <li><a href="#mle">Maximum Likelihood Estimation (MLE)</a></li>
            <li><a href="#con-mat">Confusion matrix </a></li>
            <li><a href="#reference">Reference</a></li>
        </ol>
        </div>
    </div>
    </div>

    <section>
    <h2 id="introdction">Introduction</h2>
    In the realm of machine learning, classification problems involve the task of categorizing input data into distinct classes or categories based on certain features or attributes. This fundamental problem finds widespread 
    applications across various domains, ranging from medical diagnosis and sentiment analysis to image recognition and spam detection.
    <p>Among the plethora of algorithms available for classification tasks, logistic regression stands out as a versatile and widely-used method. Despite its name, logistic regression is not used for regression problems but 
        rather for binary classification, where the goal is to predict the probability of an observation belonging to a particular class.</p>
    
    <!-------------------------->
    <h4 id="importance">Importance of Logistic Regression</h4>
    <p>Logistic regression holds significant importance in the field of machine learning for several reasons. Firstly, it provides a simple yet powerful framework for modeling the relationship between input features and the 
        likelihood of belonging to a particular class. This makes it particularly suitable for scenarios where interpretability and explanatory power are crucial.</p>
    <p>Secondly, logistic regression offers robustness and efficiency, making it well-suited for both small and large-scale classification tasks. Its computational simplicity and ability to handle high-dimensional data make 
        it a popular choice for real-world applications.</p>

    <!-------------------->
    <h4 id="need-of-LR">Need of Logistic Regression</h4>
    <p>Linear regression and logistic regression are both widely used techniques in machine learning, but they serve different purposes and are suited for different types of problems. </p>
    <ul>
        <li><strong>Linear Regression:</strong> Linear regression is used for predicting continuous outcomes. It models the relationship between a dependent variable and one or more independent variables by fitting a 
            linear equation to the observed data points. The output of linear regression is a continuous value, making it suitable for regression tasks where the target variable is numeric.</li>
        <li><strong>Logistic Regression:</strong> Logistic regression, on the other hand, is specifically designed for binary classification tasks, where the outcome variable is categorical and has two possible 
            classes (e.g., yes/no, spam/not spam). Unlike linear regression, logistic regression models the probability that a given input belongs to a particular class. It uses the logistic function (also known as the 
            sigmoid function) to map input features to a probability value between 0 and 1, making it suitable for classification problems.</li>
    </ul>

    <!------------------------>
    <h4 id="advantages">Advantages of Logistic Regression</h4>
    Logistic regression offers several advantages that make it a popular choice for binary classification tasks:
    <ul>
        <li><strong>Simple and Interpretable: </strong>Logistic regression models are relatively simple and easy to interpret compared to more complex algorithms like neural networks. The coefficients of logistic regression can 
            be directly interpreted in terms of the impact of each feature on the predicted probability of the target class.</li>
        <li><strong>Efficient Training and Inference: </strong>Logistic regression models are computationally efficient to train and make predictions with, especially for large datasets. They require less computational resources 
            compared to more complex models, making them suitable for real-time applications.</li>
        <li><strong>Probabilistic Interpretation: </strong>Logistic regression provides probabilistic outputs, allowing users to understand the uncertainty associated with each prediction. This probabilistic interpretation is useful 
            for decision-making and risk assessment in various domains.</li>
    </ul>

    <!------------------------------->
    <h4 id="applications">Scope of Applications</h4>
    Logistic regression finds applications across various fields due to its simplicity, interpretability, and effectiveness in binary classification tasks. Some common applications include:
    <ul>
        <li><strong>Medical Diagnosis:</strong> Predicting the likelihood of disease occurrence based on patient symptoms and medical history.</li>
        <li><strong>Credit Risk Assessment:</strong> Assessing the risk of default for loan applicants based on financial and demographic factors.</li>
        <li><strong>Marketing Analytics:</strong> Predicting customer churn or likelihood of response to marketing campaigns based on customer behavior and demographic information.</li>
        <li><strong>Fraud Detection:</strong> Identifying fraudulent transactions based on patterns and anomalies in financial data.</li>
        <li><strong>Sentiment Analysis:</strong> Classifying text data (e.g., customer reviews, social media posts) as positive or negative sentiment.</li>
    </ul>
    <hr>

    <!----------------------->
    <h3 id="Binary">Binary Logistic Regression</h3>
    Logistic regression is a statistical model in which the response variable takes a discrete value and the 
    explanatory variables can either be continuous or discrete. If the outcome variable takes only two values, 
    then the model is called binary logistic regression model. 

    <p>Logistic regression is statistical method used to model the probability of a outcome (i.e., outcome that can take on one of two values, such as 0 or 1, yes or no, etc.) based on one or more predictor variables. Mathematically, 
        logistic regression uses the logistic function, also known as the sigmoid function, to model the probability of the outcome.</p>
        
    <p>The logistic function is defined as:</p>

    $$P(Y=1 | X) =  \sigma(z) = \frac{1}{1+e^{-z}}$$

    Here \(z\) is defined as:

    $$z = \theta_0 + \theta_1 x_1 + \theta_2 x_2+ ... \theta_n x_n = \theta^T x$$

    and
    <ul>
        <li>\(P(Y=1 | X)\) is the probability of the outcome,</li>
        <li>\(X = (x_1, x_2, ..., x_n)\) represents the input features variables.</li>
        <li>\(\theta_0\) is the intercept, \(\theta_1, \theta_2, ..., \theta_n\) are the coefficients for the  input features variables.</li>
        <li>\(z\) is the linear combination of the input features.</li>
        <li>\(\theta^T\) represents the transpose of the parameter vector.</li>
        <li>'\(e\)' is the base of the natural logarithm (approximately equal to 2.71828).</li>
    </ul>

    <div class="box">
        Sometimes the logistic function is also represented in more compact form as \(h_\theta(x)\) and \(\sigma(z)\) and known as sigmoid function also known as 
        '<strong><em>activation function</em></strong>' (for more details, see various activation functions <a href="optimization.html">Optimization functions</a>):
        $$h_\theta(x) = \frac{1}{1+e^{-\theta^T x}}$$
        where:
        <ul>
        <li>\(h_\theta(x)\) is the predicted probability that \(y = 1\).</li>
        <li>\(\theta^T\) is the transpose of the parameter vectors</li>
        <li>\(x\) is the input feature vector</li>
        </ul>
    </div>
    
    <figure>
        <img src="assets/img/machine-ln/logistic-fun.png" alt="" style="max-width: 60%; max-height: auto;">
        <figcaption style="text-align: center;"></figcaption>
    </figure>
    
    <p>The logistic function has an S-shaped curve (thus also known as Sigmoid function).
    The logistic function maps any real-valued number to a value between 0 and 1, which can be interpreted as a probability. The goal of logistic regression is to find the values of the coefficients that maximize the likelihood of 
    observing the data, given the model. This is typically done using maximum likelihood estimation. Once the coefficients have been estimated, the logistic regression model can be used to predict the probability of the outcome 
    for new observations.</p>
    
    <div class="grey-box">
    <p><strong>Example: </strong>For example, if we have a new observation with predictor variables \(x_1=1, x_2=3\) and \(x_3=4\), and the estimated coefficients are \(\theta_0 = -1, \theta_1 =0.5, \theta_2 = 1\) and \(\theta_3 = 0.2\), 
        we can calculate the probability of the outcome as follows:</p>
    
    $$z = -1 +0.52 +13 +0.2 \times 4 = 3.3$$

    and hence,

    $$P(Y = 1 | X) = \frac{1}{1+e^{-3.3}} = 0.96$$

    Therefore, the probability of the outcome for this new observation is 0.96. It's important to note that logistic regression assumes a linear relationship between the predictor variables and the log odds of the outcome. This means that the logistic 
    regression model assumes that the relationship between the predictor variables and the probability of the outcome can be modeled using a linear equation.
    </div>
    <p>From above equation, we can re write following equations:</p>
    $$\text{ln}\left(\frac{P(Y=1 | X)}{1- P(Y=1 | X)}\right) = z = \theta_0 + \theta_1 x_1 + ... + \theta_n x_n . $$

    The right hand side of the equation is a linear function. Such models are called <strong>generalized linear models (GLM)</strong>. In GLM, the errors may not follow normal distribution and there exists a transformation function of the outcome variable that takes a linear 
    functional form.

    <br><br>
    <h4 id="how-sigma">How Sigmoid function works?</h4> 
    <p>Let's say we provide an image of a fruit to our model. The model analyzes the image's features and computes a raw output score. 
    This raw score is then passed through the sigmoid function, which converts it into a probability score. Based on the probability outcomes generated for a given image, we determine whether the fruit in the image is a mango or not.</p>
    <figure>
        <img src="assets/img/machine-ln/classification-binary.png" alt="" style="max-width: 90%; max-height: auto;">
        <figcaption style="text-align: center;">Here \(x\) represents out input image. \(y\) denotes the output and it can have two values, 1 or 0.if \(y=1 \), there is a mango in an image; else, if \(y=0 \), there is no mango in the image. (<strong>Image credit:</strong> Left image is credited to @ <a href="index.html">arunp77</a> and the
            right image is taken from <a href="https://datahacker.rs/004-machine-learning-logistic-regression-model/" target="_blank">datahacker.rs</a> (a nice description))</figcaption>
    </figure>
    <p>In this image, it is clear that we first calculate the output of a linear function \(z\). This output \(z\) will be the input to the Sigmoid Function. </p>
    <p>Next, for calculated \(z \) we will produce prediction \(\hat{y} \) which will be determined by the \(z \). Then, if \(z \) is large positive value, the \(\hat{y} \) will be close to 1. On the other hand, if \(z \) is a large negative value, the \(\hat{y} \) will be close to 0. Therefore, the \(\hat{y} \) will always be in the range between 0 to 1.</p>
    <p>One simple way to classify the prediction \(\hat{y} \) is to use a threshold value of 0.5. So, if our prediction is greater than 0.5, we assume that \(y \) is 1. Otherwise, we will assume that \(y \) is 0. As the \(\hat{y} \) gets closer to 1, the probability that there is a cat in the image becomes higher. On the other hand, as the \(\hat{y} \) comes closer to 0, the probability that there is a cat in the image also becomes lower.</p>
    <p>To calculate the \(\hat{y} \), we will use the following equations. It is a very simple calculation wherein we will just plug in the Sigmoid Function formula into the linear model.</p>
    <ul>
        <li><strong>Linear Model:</strong></li>
        $$\hat{y}=w^{T}x+b $$
        <li><strong>Sigmoid Function: </strong></li>
        $$ \sigma(z)=\frac{1}{1+e^{-z}} $$
        <ul>
            <li>If \(z \) is a large positive number, then:
                $$\sigma (z)=\frac{1}{1+0}\approx 1 $$
            </li>
            <li>If \(z \) is a large negative number, then:</li>
            $$ \sigma (z)=\frac{1}{1+\infty }\approx 0 $$
        </ul>
        <li><strong>Logistic Regression Model: </strong></li>
        $$ \hat{y}=\sigma(w^{T}x+b) $$
    </ul>
    <p>So, when we implement Logistic Regression, our primary goal is to attempt computing the parameters \(w \) and \(b \), such that \(\hat{y} \) becomes a good estimate of the chance of \(y=1 \). To do this, cost functions are used.</p>
    

    <!----------------->
    <h4 id="cost-func1">Cost function for binary Logistic Regression</h4> 
    <p>Now, to train the parameters \(w \) and \(b \) of our Logistic Regression model, we need to define a Cost Function. For each data point \(x \), we start computing a series of operations to produce a predicted output. 
        Then, we compare this predicted output to the actual output and calculate a prediction error. This error is what we minimize during the learning process using an optimization strategy.
        The way we’re computing that error value is by using a Loss Function. Our ultimate goal is to minimize the Loss Function in order to identify the values of \(w \) and \(b \). For this, we use an algorithm called the Gradient Descent.
        The loss (error) function, to measure how well our algorithm is doing is defined as: \(\mathcal{L}(\hat{y}, y) = \frac{1}{2} (\hat{y} - y)^2\), (it is to be noted is that loss function is applied only to a single training sample). 
        In the case of Linear regression models, a loss function is a convex function. However in the case of Logistic regression, it is a non-convex function.
    </p>
    <figure>
        <img src="assets/img/machine-ln/lin-log.png" alt="" style="max-width: 90%; max-height: auto;">
        <figcaption style="text-align: center;">(<strong>Image credit:</strong> @ <a href="index.html">arunp77</a>)</figcaption>
    </figure>
    <p>So in the case of Liear regression, GD will take steps until it converges at the global minima. However in the case of non-convex functions, there are so many local minima and hence squared error cost function is not a good choice. 
        Instead, there will be different cost function which makes the cost function convex again and the GD method guaranteed to converge to the global minimum. A great pictorization of the non-convex in the 3D format can be seen at the top of this page.</p>

    <p>To ensure convergence to the global optimum, we utilize the Cross-Entropy Loss function. This metric evaluates the effectiveness of a classification model that produces probability values ranging from 0 to 1. We can write it as: 
        \(\mathcal{L}(\hat{y}, y) =-(y \text{log}\hat{y}+(1-y)\text{log}(1-\hat{y}))\) and hence, when \(y =1\), loss function becomes \(\mathcal{L}( \hat{y}, y) = – \text{log}\hat{y} \) should be large, so, we want \(\hat{y} \) large (as close as possible to 1 ).
        However, when \(y=0\), the loss function is given by \(\mathcal{L}(\hat{y}, y) =-\text{log}(1-\hat{y})\)  should be large, so, we want \(\hat{y} \) small (as close as possible to 0 ). Remember, \(\hat{y}\) is a Sigmoid Function such that it cannot be less than 0 and bigger than 1.
    </p>

    <p>Now, we can define our Cost Function which measures how well our parameters \(w \) and \(b \) are doing on the entire training set. Here, we will use \((i) \) superscript to index different training examples.</p>

    $$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] $$
    Here,
    <ul>
        <li>\(m\) is the number of training examples.</li>
        <li>\(y^{(i)}\) is the actual output for the i-th training example.</li>
        <li>\(h_\theta (x^{(i)})\) is the predicted output for the i-th training example using the logistic function.</li>
    </ul>
    <p>It is to be noted here that the Cost function \(J(\theta)\) is defined as the average of the sum of Loss functions and is a function of both weights \(\theta\) and indirectly on bias \(b\).</p>
    The cost function penalizes large errors, and the logarithmic terms ensure that the cost is higher when the prediction is far from the actual value.

    <figure>
        <img src="assets/img/machine-ln/log-cost-function.png" alt="" style="max-width: 90%; max-height: auto;">
        <figcaption style="text-align: center;">For code to plot these, see <a href="https://colab.research.google.com/drive/1uHyINYEHJtEA-ootSQ6MarzXB3rR1KKY#scrollTo=kkE1kG6Oz0h_" target="_blank">code</a> (<strong>Image credit:</strong> @ <a href="index.html">arunp77</a>)</figcaption>
    </figure>

    In training, the goal is to find the parameter vector \(\theta\) that minimizes the cost function. This is often done using optimization algorithms like gradient descent or Maximum Likelihood Estimation (MLE) method.

    <p></p>
    <!-------------------------->
    <h4 id="multi-class">Multi-Class Logistic Cost function</h4>
    <p>Let's delve into the mathematical formulations of both the One-vs-Rest (OvR) and Multinomial Logistic Regression approaches:</p>
    <ol>
        <li id="one-vs-rest"><strong>One-vs-Rest (OvR) Logistic Regression:</strong>
        Let us assume a scenario for each class \(k\) in a multi-class classification problem with \(K\) classes. 
        <ul>
            <li>Assume that we have a set of features \(x\) and parameters \(\theta^{(k)}\) for each class \(k\).</li> 
            <li>The hypothesis function \(h_\theta^{(k)}(x)\) represents the probability that example \(x\) belongs to class \(k\).</li>
            <li>The hypothesis function is given by the sigmoid (logistic) function:
                $$h_{\theta}^{(k)}(x) = \frac{1}{1+e^{-\theta^{(k)T}x}}.$$
            </li>
            <li>We train \(K\) separate logistic regression models, each predicting the probability of one class vs. all other classes.</li>
            <li>For class \(k\), the cost function is the binary cross-entropy loss:</li>
            $$J(\theta^{(k)})= -\frac{1}{m}\sum_{i=1}^{m} \left[ y^{(i)} \text{log}(h_{\theta}^{(k)}(x^{(i)}))+(1-y^{(i)})\text{log}(1-h_\theta^{(k)}(x^{(i)}))\right]$$
            where, \(y^{(i)}\) is the binary indicator (0 or 1) if example \(i\) belongs to class \(k\).
            <li>To predict the class for a new example \(x\), we compute the probabilities \(h_\theta^{(k)}(x)\) for each class \(k\), and select the class with the highest probability.</li>
        </ul>
        </li>
        <li id="multi-lr"><strong>Multinomial Logistic Regression (Softmax Regression): </strong>
        Consider a multi-class classification problem with \(K\) classes. 
        <ul>
            <li>We have a set of feature \(x\) and parameters \(\Theta\), where \(\Theta\) is a matrix of size \((n\times K)\) for \(n\) features and \(K\) classes.</li>
            <li>The hypothesis function \(h_\Theta(x)\) outputs a probability distribution over all \(K\) classes.</li>
            <li>The softmax function computes the probability of each class:</li>
            $$h_{\Theta}(x)_k = \frac{e^{\Theta^T_k x}}{\sum_{j=1}^K e^{\Theta^T_j x}}.$$
            <li>Here, \(h_\Theta(x)_k\) is the probability that example \(x\) belongs to class \(k\).</li>
            <li>The cost function is the categorical cross-entropy loss, also known as the softmax loss:</li>
            $$J(\Theta) = -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \text{log}(h_\Theta(x^{(i)})_k)$$
            where, \(y_k^{(i)}\) is the binary indicator (0 or 1) if example \(i\) belongs to class \(k\).
            <li>To predict the class for a new example \(x\), we compute the probabilities \(h_\Theta(x)\) for all \(K\) classes, and select the class with the highest probability. </li>
        </ul>
        </li>
    </ol>

    

    <!----------------->
    <div class="grey-box">
    <!---------------------------->
    <h4 id="gdr">Gradient Descent Rule:</h4> 
    The gradient descent algorithm for the logistic regression aims to find the optimal parameters \(\theta\) and \(w\)
    that minimize the cost function. The update rule is derived from the partial derivatives of the cost function with respect to each parameter. 

    <p>The update rule for the gradient descent algorithm is as follows:</p>
    $$\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}$$

    and 

    $$w_j := w_j - \alpha \frac{\partial J(\theta)}{\partial w_j}$$

    where:
    <ul>
        <li>\(\theta_j\) is the j-th parameter (weight or coefficient).</li>
        <li>\(\alpha\) is the learning rate, a positive scalar determining the size of each step in the parameter  space.</li>
        <li>\(\frac{\partial J(\theta)}{\partial \theta_j}\) is the partial derivative of the cost function \(J(\theta)\) with respect to \(\theta_j\).</li> 
    </ul>
    <!----------------------------->
    <h5 id="algorithm">Algorithm steps:</h5>
    <ol>
        <li>Initialize parameters: \(\theta_0, \theta_1, ..., \theta_n\).</li>
        <li>Repeat until convergence:
        <ul>
            <li>Update each parameter simultaneously using the update rule.</li>
            <li>Simultaneous update means calculating the new values of all parameters before applying them to the model.</li>
            <li>Repeat the update until the change in the cost function becomes small or a fixed number of iterations are reached.</li>
        </ul>
        </li>
    </ol>
    <h4 id="mle"><strong><a href="https://arunp77.github.io/Arun-Kumar-Pandey/Linear-reg.html#Maximum-likelihood-estimation" target="_blank">Maximum Likelihood Estimation (MLE)</a></strong></h4>
    Maximum Likelihood Estimation (MLE) is a statistical method used to calculate the parameters. In the case of logistic regression, MLE aims to find the set of parameters that maximizes the likelihood function, which measures how well the model explains the observed data.
    <p><strong>Mathematical Description:</strong>
    For logistic regression, let's denote the likelihood function as \(L(\theta)\), where \(\theta\) represents the parameters of the logistic regression model. The likelihood function is given by the product of the 
    probabilities of the observed outcomes under the current parameter values.</p>

    <p>For a binary classification problem (0 or 1), the likelihood function is often expressed as:</p>

    $$L(\theta) = \Pi_{i=1}^m P(y^{(i)}|x^{(i)}; \theta)^{y^{(i)}}\cdot \left(1-P(y^{(i)}|x^{(i)}; \theta)\right)^{1-y^{(i)}}.$$

    Here:

    <ul>
    <li>\(m\) is the number of training examples.</li>
    <li>\(y^{(i)}\) is the actual output for the i-th training example (0 or 1).</li>
    <li>\(x^{(i)}\) is the input feature vector for the i-th training example.</li>
    <li>\(P(y^{(i)|x^{(i); \theta}})\) is the predicted probabolity if the i-th example belonging to class 1.</li>
    </ul>

    In logistic regression, the predicted probability is given by the logistic function:

    $$P(y^{(i)} =1| x^{(i)}; \theta) = h_\theta(x^{(i)})$$

    $$P(y^{(i)} =0| x^{(i)}; \theta) = 1- h_\theta(x^{(i)})$$
    
    The goal of MLE is to find the values of \(\theta\) that maximize the likelihood function \(L(\theta)\). In practice, it 
    is often more convenient to maximize the log-likelihood function (logarithm of the likelihood function), denoted as \(l(\theta)\):

    $$l(\theta) = \sum_{i=1}^m \left[y^{(i)} \text{log}(h_\theta(x^{(i)}))- (1-y^{(i)})\text{log}(1-h_\theta(x^{(i)}))\right].$$

    Maximizing \(l(\theta)\) is equivalent to maximizing \(L(\theta)\), as the logrithm is a monotonically increasing function.

    <p>The logistic regression cost function \(J(\theta)\) that we discussed earlier is essentially the negative log-likelihood, with some scaling for convenience in optimization:</p>

    $$J(\theta) = -\frac{1}{m} l(\theta).$$

    So, in logistic regression, the optimization process, whether using gradient descent or another optimization algorithm, is essentially performing Maximum Likelihood Estimation to find the parameters that maximize the likelihood of observing the given set of training examples.
    </div>


    <!----------------->
    <h3 id="con-mat">Confusion matrix</h3>
    <p>The confusion matrix is a tool used in classification to evaluate the performance of a machine learning model. It is a matrix of actual classes vs. predicted classes. Here's the formula to compute it:</p>
    <p>Let's assume we have a binary classification problem with two classes: Positive (P) and Negative (N).</p>
    <ul>
        <li><strong>True Positive (TP):</strong> The model correctly predicts positive instances.</li>
        <li><strong>True Negative (TN):</strong> The model correctly predicts negative instances.</li>
        <li><strong>False Positive (FP):</strong> The model incorrectly predicts positive instances as negative.</li>
        <li><strong>False Negative (FN):</strong> The model incorrectly predicts negative instances as positive.</li>
    </ul>

    The confusion matrix is typically represented as:

    <figure>
        <img src="assets/img/machine-ln/classification-confusion.png" alt="" style="max-width: 40%; max-height: auto;">
        <figcaption style="text-align: center;">Confusion matrix (<strong>Image credit:</strong> @ <a href="index.html">arunp77</a>)</figcaption>
    </figure>

    <p>Lets learn about few metrics that help in understanding how well the model is performing in terms of correctly and incorrectly classifying instances.</p>

    <p><strong>Accuracy:</strong>Accuracy is a common metric used to evaluate the performance of a classification model. It represents the ratio of correctly predicted instances to the total instances in the dataset. Mathematically, accuracy is calculated as:</p> 
    $$\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total number of predictions}} = \frac{\text{TP+TN}}{\text{TP+TN+FP+FN}}.$$
    A higher accuracy value indicates better performance of the model in making correct predictions across all classes. However, accuracy alone might not provide a complete picture of the model's performance, especially in scenarios where classes are imbalanced or have different costs associated with misclassification. Therefore, it's essential to consider other evaluation metrics along with accuracy.
    
    <p><strong>Precision, recall & F-score: </strong>Precision and recall are two important metrics used to evaluate the performance of a classification model, especially in scenarios where class imbalance exists.</p>
    <ul>
        <li><strong>Precision: </strong>Precision measures the accuracy of positive predictions made by the model. It is the ratio of true positive predictions to the total number of positive predictions made by the model.
        Mathematically, it is defined as:
        $$\text{Precision} = \frac{\text{TP}}{\text{TP+FP}}$$
        Precision answers the question: "Of all the instances predicted as positive, how many are actually positive?" A high precision indicates that the model is making fewer false positive predictions.
        </li>
        <li><strong>Recall (Sensitivity):</strong>Recall, also known as sensitivity or true positive rate, measures the model's ability to correctly identify positive instances. It is the ratio of true positive predictions to the total number of actual positive instances in the dataset.
            Mathematically, recall is calculated as:
            $$\text{Recall} = \frac{\text{TP}}{\text{TP+FN}}.$$
            Recall answers the question: "Of all the actual positive instances, how many did the model correctly predict as positive?" A high recall indicates that the model is effectively capturing most of the positive instances.
        </li>
        Both precision and recall are important metrics, and there is often a trade-off between them. For example, increasing precision may lead to a decrease in recall and vice versa. The F1 score, which is the harmonic mean of precision and recall, is often used to balance these two metrics.
        <li><strong>F-score:</strong> The F-score, also known as the F1 score, is a single metric that combines precision and recall into a single value. It provides a balance between precision and recall, considering both false positives and false negatives.
            Mathematically, the F1 score is the harmonic mean of precision and recall and is calculated as:
            $$F_1 = 2\times \frac{\text{Precision} \times \text{Recall}}{\text{Precision}+\text{Recall}}.$$
            The F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It provides a single value that summarizes the performance of a classification model, especially in scenarios where class imbalance exists or when both precision and recall are equally important.
        </li>
    </ul>






    <!----------------->
    <h4>Training Objective</h4>
    The model is trained by finding the optimal values for \(\theta\) that minimize the prediction error. This is typically done using techniques like Maximum Likelihood Estimation (MLE).







    </section>


    <!-------------------------------->
    <section id="Example">
    <ul>
        <li>You can go to <a href="https://github.com/arunp77/Machine-Learning/tree/main/Projects-ML" target="_blank">following project</a> for a reference for linear regression analysis. </li>
    </ul>
    </section>
    
    <!----------- Reference ----------->
    <section id="reference">
    <h2>References</h2>
    <ul>
        <li>My github Repositories on Remote sensing <a href="https://github.com/arunp77/Machine-Learning/" target="_blank">Machine learning</a></li>
        <li><a href="https://mlu-explain.github.io/linear-regression/" target="_blank">A Visual Introduction To Linear regression</a> (Best reference for theory and visualization).</li>
        <li>Book on Regression model: <a href="https://avehtari.github.io/ROS-Examples/" target="_blank">Regression and Other Stories</a></li>
        <li>Book on Statistics: <a href="https://hastie.su.domains/Papers/ESLII.pdf" target="_blank">The Elements of Statistical Learning</a></li>
        <li><a href="https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf">https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf</a></li>
        <li><a href="https://datahacker.rs/002-machine-learning-linear-regression-model/" target="_blank">One of the best description on Linear regression</a>.</li>
    </ul>
    </section>

    <hr>
    
    <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

    <h3>Some other interesting things to know:</h3>
    <ul style="list-style-type: disc; margin-left: 30px;">
        <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
        <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
    </ul>
    </div>
    <p></p>

    <div class="navigation">
        <a href="index.html#portfolio" class="clickable-box">
            <span class="arrow-left">Portfolio section</span>
        </a>
        
        <a href="portfolio-details-1.html" class="clickable-box">
            <span class="arrow-right">Content</span>
        </a>
    </div>
</div>
</div>
</section><!-- End Portfolio Details Section -->
</main><!-- End #main --

<!-- ======= Footer ======= -->
<footer id="footer">
  <div class="container">
    <div class="copyright">
      &copy; Copyright <strong><span>Arun</span></strong>
    </div>
  </div>
</footer><!-- End  Footer -->

<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/typed.js/typed.umd.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    hljs.initHighlightingOnLoad();
  });
</script>

</body>

</html>