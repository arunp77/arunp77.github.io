<!DOCTYPE html>
<html lang="en">

<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1.0" name="viewport">

<title>Logistic Regression</title>
<meta content="" name="description">
<meta content="" name="keywords">

<!-- Favicons -->
<link href="assets/img/Favicon-1.png" rel="icon">
<link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

<!-- Google Fonts -->
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

<!-- Vendor CSS Files -->
<link href="assets/vendor/aos/aos.css" rel="stylesheet">
<link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
<link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
<link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
<link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
<link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
<!-- Creating a python code section-->
<link rel="stylesheet" href="assets/css/prism.css">
<script src="assets/js/prism.js"></script>

<!-- Template Main CSS File -->
<link href="assets/css/style.css" rel="stylesheet">

<!-- To set the icon, visit https://fontawesome.com/account-->
<script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
<!-- end of icon-->

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
======================================================== -->
</head>

<body>

<!-- ======= Mobile nav toggle button ======= -->
<i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

<!-- ======= Header ======= -->
<header id="header">
<div class="d-flex flex-column">

    <div class="profile">
    <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
    <h1 class="text-light"><a href="index.html">Arun</a></h1>
    <div class="social-links mt-3 text-center">
        <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
        <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
        <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
        <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
        <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
    </div>
    </div>

    <nav id="navbar" class="nav-menu navbar">
    <ul>
        <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
        <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
        <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
        <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
        <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
        <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
        <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
        <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
        <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
        <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
    </ul>
    </nav><!-- .nav-menu -->
</div>
</header><!-- End Header -->

<main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
    <div class="container">

    <div class="d-flex justify-content-between align-items-center">
        <h2>Machine Learning</h2>
        <ol>
        <li><a href="portfolio-details-1.html" class="clickable-box">Content section</a></li>
        <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
        </ol>
    </div>

    </div>
    </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
    <div class="dropdown">
        <button class="dropbtn"><strong>Shortcuts:</strong></button>
        <div class="dropdown-content">
            <ul>
                <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                <!-- Add more subsections as needed -->
            </ul>
        </div>
    </div>
    </div>

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
    <div class="container">
    <div class="row gy-4">
        <h1>Logistic Regression: Classification methods</h1>
        <div class="col-lg-8">
        <div class="portfolio-details-slider swiper">
            <div class="swiper-wrapper align-items-center">
            <div class="swiper-slide">
                <figure>
                    <img src="assets/img/data-engineering/classification.png" alt="" style="max-width: 50%; max-height: 50%;">
                    <figcaption style="text-align: center;"></figcaption>
                </figure>
            </div>
            </div>
        </div>
    </div>

    <div class="col-lg-4 grey-box">
        
        <div class="section-title">
        <h3>Content</h3>
        <ol>
            <li><a href="#introduction">Introduction</a></li>
            <ul>
            <li><a href="#importance">Importance of Logistic Regression</a></li>
            <li><a href="#need-of-LR">Need of Logistic Regression</a></li>
            <li><a href="#advantages">Advantages of Logistic Regression</a></li>
            <li><a href="#applications">Scope of Applications</a></li>
            </ul>
            <li><a href="#Binary">Binary Logistic Regression</a></li>
            <li><a href="#Types-of-Linear-Regression">Types of Linear Regression</a></li>
            <li><a href="#Mathematical-1">Mathematical Explanation</a></li>
            <li><a href="#Assumption-of-LR">Assumptions of Linear Regression</a></li>
            <li><a href="#evaluation-metrics-for-LR">Evaluation Metrics for Linear Regression</a></li>
            <li><a href="#overfit-goodfit-underfit">Overfitting, Good Fit, and Underfitting in Machine Learning</a></li>
            <li><a href="#reference">Reference</a></li>
        </ol>
        </div>
    </div>
    </div>

    <section>
    <h2 id="introdction">Introduction</h2>
    In the realm of machine learning, classification problems involve the task of categorizing input data into distinct classes or categories based on certain features or attributes. This fundamental problem finds widespread 
    applications across various domains, ranging from medical diagnosis and sentiment analysis to image recognition and spam detection.
    <p>Among the plethora of algorithms available for classification tasks, logistic regression stands out as a versatile and widely-used method. Despite its name, logistic regression is not used for regression problems but 
        rather for binary classification, where the goal is to predict the probability of an observation belonging to a particular class.</p>
    
    <!-------------------------->
    <h4 id="importance">Importance of Logistic Regression</h4>
    <p>Logistic regression holds significant importance in the field of machine learning for several reasons. Firstly, it provides a simple yet powerful framework for modeling the relationship between input features and the 
        likelihood of belonging to a particular class. This makes it particularly suitable for scenarios where interpretability and explanatory power are crucial.</p>
    <p>Secondly, logistic regression offers robustness and efficiency, making it well-suited for both small and large-scale classification tasks. Its computational simplicity and ability to handle high-dimensional data make 
        it a popular choice for real-world applications.</p>

    <!-------------------->
    <h4 id="need-of-LR">Need of Logistic Regression</h4>
    <p>Linear regression and logistic regression are both widely used techniques in machine learning, but they serve different purposes and are suited for different types of problems. </p>
    <ul>
        <li><strong>Linear Regression:</strong> Linear regression is used for predicting continuous outcomes. It models the relationship between a dependent variable and one or more independent variables by fitting a 
            linear equation to the observed data points. The output of linear regression is a continuous value, making it suitable for regression tasks where the target variable is numeric.</li>
        <li><strong>Logistic Regression:</strong> Logistic regression, on the other hand, is specifically designed for binary classification tasks, where the outcome variable is categorical and has two possible 
            classes (e.g., yes/no, spam/not spam). Unlike linear regression, logistic regression models the probability that a given input belongs to a particular class. It uses the logistic function (also known as the 
            sigmoid function) to map input features to a probability value between 0 and 1, making it suitable for classification problems.</li>
    </ul>

    <!------------------------>
    <h4 id="advantages">Advantages of Logistic Regression</h4>
    Logistic regression offers several advantages that make it a popular choice for binary classification tasks:
    <ul>
        <li><strong>Simple and Interpretable: </strong>Logistic regression models are relatively simple and easy to interpret compared to more complex algorithms like neural networks. The coefficients of logistic regression can 
            be directly interpreted in terms of the impact of each feature on the predicted probability of the target class.</li>
        <li><strong>Efficient Training and Inference: </strong>Logistic regression models are computationally efficient to train and make predictions with, especially for large datasets. They require less computational resources 
            compared to more complex models, making them suitable for real-time applications.</li>
        <li><strong>Probabilistic Interpretation: </strong>Logistic regression provides probabilistic outputs, allowing users to understand the uncertainty associated with each prediction. This probabilistic interpretation is useful 
            for decision-making and risk assessment in various domains.</li>
    </ul>

    <!------------------------------->
    <h4 id="applications">Scope of Applications</h4>
    Logistic regression finds applications across various fields due to its simplicity, interpretability, and effectiveness in binary classification tasks. Some common applications include:
    <ul>
        <li><strong>Medical Diagnosis:</strong> Predicting the likelihood of disease occurrence based on patient symptoms and medical history.</li>
        <li><strong>Credit Risk Assessment:</strong> Assessing the risk of default for loan applicants based on financial and demographic factors.</li>
        <li><strong>Marketing Analytics:</strong> Predicting customer churn or likelihood of response to marketing campaigns based on customer behavior and demographic information.</li>
        <li><strong>Fraud Detection:</strong> Identifying fraudulent transactions based on patterns and anomalies in financial data.</li>
        <li><strong>Sentiment Analysis:</strong> Classifying text data (e.g., customer reviews, social media posts) as positive or negative sentiment.</li>
    </ul>
    <hr>

    <!----------------------->
    <h3 id="Binary">Binary Logistic Regression</h3>
    Logistic regression is a statistical model in which the response variable takes a discrete value and the 
    explanatory variables can either be continuous or discrete. If the outcome variable takes only two values, 
    then the model is called binary logistic regression model. 

    <p>Logistic regression is statistical method used to model the probability of a outcome (i.e., outcome that can take on one of two values, such as 0 or 1, yes or no, etc.) based on one or more predictor variables. Mathematically, 
        logistic regression uses the logistic function, also known as the sigmoid function, to model the probability of the outcome.</p>
        
    <p>The logistic function is defined as:</p>

    $$P(Y=1 | X) =  \sigma(z) = \frac{1}{1+e^{-z}}$$

    Here \(z\) is defined as:

    $$z = \theta_0 + \theta_1 x_1 + \theta_2 x_2+ ... \theta_n x_n = \theta^T x$$

    and
    <ul>
        <li>\(P(Y=1 | X)\) is the probability of the outcome,</li>
        <li>\(X = (x_1, x_2, ..., x_n)\) represents the input features variables.</li>
        <li>\(\theta_0\) is the intercept, \(\theta_1, \theta_2, ..., \theta_n\) are the coefficients for the  input features variables.</li>
        <li>\(z\) is the linear combination of the input features.</li>
        <li>\(\theta^T\) represents the transpose of the parameter vector.</li>
        <li>'\(e\)' is the base of the natural logarithm (approximately equal to 2.71828).</li>
    </ul>

    <div class="box">
        Sometimes the logistic function is also represented in more compact form as \(h_\theta(x)\) and \(\sigma(z)\) and known as sigmoid function also known as 
        '<strong><em>activation function</em></strong>' (for more details, see various activation functions <a href="optimization.html">Optimization functions</a>):
        $$h_\theta(x) = \frac{1}{1+e^{-\theta^T x}}$$
        where:
        <ul>
        <li>\(h_\theta(x)\) is the predicted probability that \(y = 1\).</li>
        <li>\(\theta^T\) is the transpose of the parameter vectors</li>
        <li>\(x\) is the input feature vector</li>
        </ul>
    </div>
    
    <figure>
        <img src="assets/img/machine-ln/logistic-fun.png" alt="" style="max-width: 60%; max-height: auto;">
        <figcaption style="text-align: center;"></figcaption>
    </figure>
    
    <p>The logistic function has an S-shaped curve (thus also known as Sigmoid function).
    The logistic function maps any real-valued number to a value between 0 and 1, which can be interpreted as a probability. The goal of logistic regression is to find the values of the coefficients that maximize the likelihood of 
    observing the data, given the model. This is typically done using maximum likelihood estimation. Once the coefficients have been estimated, the logistic regression model can be used to predict the probability of the outcome 
    for new observations.</p>
    
    <div class="grey-box">
    <p><strong>Example: </strong>For example, if we have a new observation with predictor variables \(x_1=1, x_2=3\) and \(x_3=4\), and the estimated coefficients are \(\theta_0 = -1, \theta_1 =0.5, \theta_2 = 1\) and \(\theta_3 = 0.2\), 
        we can calculate the probability of the outcome as follows:</p>
    
    $$z = -1 +0.52 +13 +0.2 \times 4 = 3.3$$

    and hence,

    $$P(Y = 1 | X) = \frac{1}{1+e^{-3.3}} = 0.96$$

    Therefore, the probability of the outcome for this new observation is 0.96. It's important to note that logistic regression assumes a linear relationship between the predictor variables and the log odds of the outcome. This means that the logistic 
    regression model assumes that the relationship between the predictor variables and the probability of the outcome can be modeled using a linear equation.
    </div>
    <p>From above equation, we can re write following equations:</p>
    $$\text{ln}\left(\frac{P(Y=1 | X)}{1- P(Y=1 | X)}\right) = z = \theta_0 + \theta_1 x_1 + ... + \theta_n x_n . $$

    The right hand side of the equation is a linear function. Such models are called <strong>generalized linear models (GLM)</strong>. In GLM, the errors may not follow normal distribution and there exists a transformation function of the outcome variable that takes a linear 
    functional form.

    <br><br>
    <h4 id="how-sigma">How Sigmoid function works?</h4> 
    <p>Let's say we provide an image of a fruit to our model. The model analyzes the image's features and computes a raw output score. 
    This raw score is then passed through the sigmoid function, which converts it into a probability score. Based on the probability outcomes generated for a given image, we determine whether the fruit in the image is a mango or not.</p>
    <figure>
        <img src="assets/img/machine-ln/classification-binary.png" alt="" style="max-width: 90%; max-height: auto;">
        <figcaption style="text-align: center;">Here \(x\) represents out input image. \(y\) denotes the output and it can have two values, 1 or 0.if \(y=1 \), there is a mango in an image; else, if \(y=0 \), there is no mango in the image. (<strong>Image credit:</strong> Left image is credited to @ <a href="index.html">arunp77</a> and the
            right image is taken from <a href="https://datahacker.rs/004-machine-learning-logistic-regression-model/" target="_blank">datahacker.rs</a> (a nice description))</figcaption>
    </figure>
    <p>In this image, it is clear that we first calculate the output of a linear function \(z\). This output \(z\) will be the input to the Sigmoid Function. </p>
    <p>Next, for calculated \(z \) we will produce prediction \(\hat{y} \) which will be determined by the \(z \). Then, if \(z \) is large positive value, the \(\hat{y} \) will be close to 1. On the other hand, if \(z \) is a large negative value, the \(\hat{y} \) will be close to 0. Therefore, the \(\hat{y} \) will always be in the range between 0 to 1.</p>
    <p>One simple way to classify the prediction \(\hat{y} \) is to use a threshold value of 0.5. So, if our prediction is greater than 0.5, we assume that \(y \) is 1. Otherwise, we will assume that \(y \) is 0. As the \(\hat{y} \) gets closer to 1, the probability that there is a cat in the image becomes higher. On the other hand, as the \(\hat{y} \) comes closer to 0, the probability that there is a cat in the image also becomes lower.</p>
    <p>To calculate the \(\hat{y} \), we will use the following equations. It is a very simple calculation wherein we will just plug in the Sigmoid Function formula into the linear model.</p>
    <ul>
        <li><strong>Linear Model:</strong></li>
        $$\hat{y}=w^{T}x+b $$
        <li><strong>Sigmoid Function: </strong></li>
        $$ \sigma(z)=\frac{1}{1+e^{-z}} $$
        <ul>
            <li>If \(z \) is a large positive number, then:
                $$\sigma (z)=\frac{1}{1+0}\approx 1 $$
            </li>
            <li>If \(z \) is a large negative number, then:</li>
            $$ \sigma (z)=\frac{1}{1+\infty }\approx 0 $$
        </ul>
        <li><strong>Logistic Regression Model: </strong></li>
        $$ \hat{y}=\sigma(w^{T}x+b) $$
    </ul>
    <p>So, when we implement Logistic Regression, our primary goal is to attempt computing the parameters \(w \) and \(b \), such that \(\hat{y} \) becomes a good estimate of the chance of \(y=1 \). To do this, cost functions are used.</p>
    

    <!----------------->
    <h4 id="cost-func1">Cost function for binary Logistic Regression</h4> 
    <p>Now, to train the parameters \(w \) and \(b \) of our Logistic Regression model, we need to define a Cost Function.</p>

    <p>The cost function for the logistic regression is used to quantify the error between predicted and actual values.
    The objective is to minimize this cost. The cost function is defined as:</p>

    $$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] $$
    Here,
    <ul>
        <li>\(m\) is the number of training examples.</li>
        <li>\(y^{(i)}\) is the actual output for the i-th training example.</li>
        <li>\(h_\theta (x^{(i)})\) is the predicted output for the i-th training example using the logistic function.</li>
    </ul>
    The cost function penalizes large errors, and the logarithmic terms ensure that the cost is higher when the prediction is far from the actual value.

    In training, the goal is to find the parameter vector \(\theta\) that minimizes the cost function. This is often done using optimization algorithms like gradient descent.
    <br><br>



    <!----------------->
    <div class="box">
        <h5 id="mle"><strong><a href="https://arunp77.github.io/Arun-Kumar-Pandey/Linear-reg.html#Maximum-likelihood-estimation" target="_blank">Maximum Likelihood Estimation (MLE)</a></strong></h5>
        Maximum Likelihood Estimation (MLE) is a statistical method used to calculate the parameters. In the case of logistic regression, MLE aims to find the set of parameters that maximizes the likelihood function, which measures how well the model explains the observed data.
        <p><strong>Mathematical Description:</strong>
        For logistic regression, let's denote the likelihood function as \(L(\theta)\), where \(\theta\) represents the parameters of the logistic regression model. The likelihood function is given by the product of the 
        probabilities of the observed outcomes under the current parameter values.</p>

        <p>For a binary classification problem (0 or 1), the likelihood function is often expressed as:</p>

        $$L(\theta) = \Pi_{i=1}^m P(y^{(i)}|x^{(i)}; \theta)^{y^{(i)}}\cdot \left(1-P(y^{(i)}|x^{(i)}; \theta)\right)^{1-y^{(i)}}.$$

        Here:

        <ul>
        <li>\(m\) is the number of training examples.</li>
        <li>\(y^{(i)}\) is the actual output for the i-th training example (0 or 1).</li>
        <li>\(x^{(i)}\) is the input feature vector for the i-th training example.</li>
        <li>\(P(y^{(i)|x^{(i); \theta}})\) is the predicted probabolity if the i-th example belonging to class 1.</li>
        </ul>

        In logistic regression, the predicted probability is given by the logistic function:

        $$P(y^{(i)} =1| x^{(i)}; \theta) = h_\theta(x^{(i)})$$

        $$P(y^{(i)} =0| x^{(i)}; \theta) = 1- h_\theta(x^{(i)})$$
        
        The goal of MLE is to find the values of \(\theta\) that maximize the likelihood function \(L(\theta)\). In practice, it 
        is often more convenient to maximize the log-likelihood function (logarithm of the likelihood function), denoted as \(l(\theta)\):

        $$l(\theta) = \sum_{i=1}^m \left[y^{(i)} \text{log}(h_\theta(x^{(i)}))- (1-y^{(i)})\text{log}(1-h_\theta(x^{(i)}))\right].$$

        Maximizing \(l(\theta)\) is equivalent to maximizing \(L(\theta)\), as the logrithm is a monotonically increasing function.

        <p>The logistic regression cost function \(J(\theta)\) that we discussed earlier is essentially the negative log-likelihood, with some scaling for convenience in optimization:</p>

        $$J(\theta) = -\frac{1}{m} l(\theta).$$

        So, in logistic regression, the optimization process, whether using gradient descent or another optimization algorithm, is essentially performing Maximum Likelihood Estimation to find the parameters that maximize the likelihood of observing the given set of training examples.
    </div>


    <p></p>
    <!-------------------------->
    <h4 id="multi-class">Multi-Class Logistic Cost function</h4>
    <p>Let's delve into the mathematical formulations of both the One-vs-Rest (OvR) and Multinomial Logistic Regression approaches:</p>
    <ol>
        <li><strong>One-vs-Rest (OvR) Logistic Regression:</strong>
        Let us assume a scenario for each class \(k\) in a multi-class classification problem with \(K\) classes. 
        <ul>
            <li>Assume that we have a set of features \(x\) and parameters \(\theta^{(k)}\) for each class \(k\).</li> 
            <li>The hypothesis function \(h_\theta^{(k)}(x)\) represents the probability that example \(x\) belongs to class \(k\).</li>
            <li>The hypothesis function is given by the sigmoid (logistic) function:
                $$h_{\theta}^{(k)}(x) = \frac{1}{1+e^{-\theta^{(k)T}x}}.$$
            </li>
            <li>We train \(K\) separate logistic regression models, each predicting the probability of one class vs. all other classes.</li>
            <li>For class \(k\), the cost function is the binary cross-entropy loss:</li>
            $$J(\theta^{(k)})= -\frac{1}{m}\sum_{i=1}^{m} \left[ y^{(i)} \text{log}(h_{\theta}^{(k)}(x^{(i)}))+(1-y^{(i)})\text{log}(1-h_\theta^{(k)}(x^{(i)}))\right]$$
            where, \(y^{(i)}\) is the binary indicator (0 or 1) if example \(i\) belongs to class \(k\).
            <li>To predict the class for a new example \(x\), we compute the probabilities \(h_\theta^{(k)}(x)\) for each class \(k\), and select the class with the highest probability.</li>
        </ul>
        </li>
        <li><strong>Multinomial Logistic Regression (Softmax Regression): </strong>
        Consider a multi-class classification problem with \(K\) classes. 
        <ul>
            <li>We have a set of feature \(x\) and parameters \(\Theta\), where \(\Theta\) is a matrix of size \((n\times K)\) for \(n\) features and \(K\) classes.</li>
            <li>The hypothesis function \(h_\Theta(x)\) outputs a probability distribution over all \(K\) classes.</li>
            <li>The softmax function computes the probability of each class:</li>
            $$h_{\Theta}(x)_k = \frac{e^{\Theta^T_k x}}{\sum_{j=1}^K e^{\Theta^T_j x}}.$$
            <li>Here, \(h_\Theta(x)_k\) is the probability that example \(x\) belongs to class \(k\).</li>
            <li>The cost function is the categorical cross-entropy loss, also known as the softmax loss:</li>
            $$J(\Theta) = -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \text{log}(h_\Theta(x^{(i)})_k)$$
            where, \(y_k^{(i)}\) is the binary indicator (0 or 1) if example \(i\) belongs to class \(k\).
            <li>To predict the class for a new example \(x\), we compute the probabilities \(h_\Theta(x)\) for all \(K\) classes, and select the class with the highest probability. </li>
        </ul>
        </li>
    </ol>

    <!---------------------------->
    <h4 id="gdr">Gradient Descent Rule:</h4> 
    The gradient descent algorithm for the logistic regression aims to find the optimal parameters \(\theta\) and \(w\)
    that minimize the cost function. The update rule is derived from the partial derivatives of the cost function with respect to each parameter. 

    <p>The update rule for the gradient descent algorithm is as follows:</p>
    $$\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}$$

    and 

    $$w_j := w_j - \alpha \frac{\partial J(\theta)}{\partial w_j}$$

    where:
    <ul>
        <li>\(\theta_j\) is the j-th parameter (weight or coefficient).</li>
        <li>\(\alpha\) is the learning rate, a positive scalar determining the size of each step in the parameter  space.</li>
        <li>\(\frac{\partial J(\theta)}{\partial \theta_j}\) is the partial derivative of the cost function \(J(\theta)\) with respect to \(\theta_j\).</li>
    </ul>

    <!----------------------------->
    <h5 id="algorithm">Algorithm steps:</h5>
    <ol>
        <li>Initialize parameters: \(\theta_0, \theta_1, ..., \theta_n\).</li>
        <li>Repeat until convergence:
        <ul>
            <li>Update each parameter simultaneously using the update rule.</li>
            <li>Simultaneous update means calculating the new values of all parameters before applying them to the model.</li>
            <li>Repeat the update until the change in the cost function becomes small or a fixed number of iterations are reached.</li>
        </ul>
        </li>
    </ol>







    <!----------------->
    <h4>Training Objective</h4>
    The model is trained by finding the optimal values for \(\theta\) that minimize the prediction error. This is typically done using techniques like Maximum Likelihood Estimation (MLE).







    </section>


    <!-------------------------------->
    <section id="Example">
    <ul>
        <li>You can go to <a href="https://github.com/arunp77/Machine-Learning/tree/main/Projects-ML" target="_blank">following project</a> for a reference for linear regression analysis. </li>
    </ul>
    </section>
    
    <!----------- Reference ----------->
    <section id="reference">
    <h2>References</h2>
    <ul>
        <li>My github Repositories on Remote sensing <a href="https://github.com/arunp77/Machine-Learning/" target="_blank">Machine learning</a></li>
        <li><a href="https://mlu-explain.github.io/linear-regression/" target="_blank">A Visual Introduction To Linear regression</a> (Best reference for theory and visualization).</li>
        <li>Book on Regression model: <a href="https://avehtari.github.io/ROS-Examples/" target="_blank">Regression and Other Stories</a></li>
        <li>Book on Statistics: <a href="https://hastie.su.domains/Papers/ESLII.pdf" target="_blank">The Elements of Statistical Learning</a></li>
        <li><a href="https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf">https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf</a></li>
        <li><a href="https://datahacker.rs/002-machine-learning-linear-regression-model/" target="_blank">One of the best description on Linear regression</a>.</li>
    </ul>
    </section>

    <hr>
    
    <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

    <h3>Some other interesting things to know:</h3>
    <ul style="list-style-type: disc; margin-left: 30px;">
        <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
        <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
    </ul>
    </div>
    <p></p>

    <div class="navigation">
        <a href="index.html#portfolio" class="clickable-box">
            <span class="arrow-left">Portfolio section</span>
        </a>
        
        <a href="portfolio-details-1.html" class="clickable-box">
            <span class="arrow-right">Content</span>
        </a>
    </div>
</div>
</div>
</section><!-- End Portfolio Details Section -->
</main><!-- End #main --

<!-- ======= Footer ======= -->
<footer id="footer">
  <div class="container">
    <div class="copyright">
      &copy; Copyright <strong><span>Arun</span></strong>
    </div>
  </div>
</footer><!-- End  Footer -->

<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/typed.js/typed.umd.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    hljs.initHighlightingOnLoad();
  });
</script>

</body>

</html>