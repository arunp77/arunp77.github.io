<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>PCA</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

    <style>
    /* Add some basic styling for code */
    pre {
        background-color: #f4f4f4;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 5px;
        font-family: monospace;
        white-space: pre-wrap;
    }
    </style>

  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

    <!-- ======= Header ======= -->
    <header id="header">
    <div class="d-flex flex-column">
        <div class="profile">
            <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
            <h1 class="text-light"><a href="index.html">Arun</a></h1>
            <div class="social-links mt-3 text-center">
                <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
                <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
                <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
                <a href="https://arunp77.medium.com/" class="medium"><i class="bx bxl-medium"></i></a>
            </div>
        </div>

        <nav id="navbar" class="nav-menu navbar">
            <ul>
                <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
                <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
                <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
                <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
                <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
                <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
                <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
                <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
                <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
                <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
            </ul>
        </nav><!-- .nav-menu -->
    </div>
    </header><!-- End Header -->

    <main id="main">
        <!-- ======= Breadcrumbs ======= -->
        <section id="breadcrumbs" class="breadcrumbs"> 
          <div class="container">
    
            <div class="d-flex justify-content-between align-items-center">
              <h2>Machine Learning</h2>
              <ol>
                <li><a href="portfolio-details-1.html" class="clickable-box">Content section</a></li>
                <li><a href="index.html#portfolio" class="clickable-box">Portfolio section</a></li>
              </ol>
            </div>
    
          </div>
        </section><!-- End Breadcrumbs -->

        <!------  right dropdown menue ------->
        <div class="right-side-list">
            <div class="dropdown">
                <button class="dropbtn"><strong>Shortcuts:</strong></button>
                <div class="dropdown-content">
                    <ul>
                        <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                        <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                        <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                        <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                        <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(29, 27, 27);"></i> Docker</a></li>
                        <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(34, 32, 32);"></i> Jupyter-nifi</a></li>
                        <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                        <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                        <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                        <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                        <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                        <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                        <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                            <!-- Add more subsections as needed -->
                    </ul>
                </div>
            </div>
        </div>

        <!-- ======= Portfolio Details Section ======= -->
        <section id="portfolio-details" class="portfolio-details">
        <div class="container">
            <div class="row gy-4">
                <h1>Principal component analysis (PCA)</h1>
                <div class="col-lg-8">
                    <div class="portfolio-details-slider swiper">
                        <div class="swiper-wrapper align-items-center"> 
                            <figure>
                            <img src="assets/img/machine-ln/principal-component-analysis.png" alt="" style="max-width: 50%; max-height: auto;">
                            <figcaption></figcaption>
                            </figure>
                        </div>
                        <div class="swiper-pagination"></div>
                    </div>
                </div>

                <div class="col-lg-4 grey-box">
                    <div class="section-title">
                        <h3>Content</h3>
                        <ol>
                            <li><a href="#introduction">Introduction</a></li>
                            <li><a href="#reference">Reference</a></li>
                        </ol>
                    </div>
                </div>
            </div>

            <section>
            <h3 id="introduction">Introduction</h3>
            Principal component analysis (PCA) is a statistical procedure that is commonly used to reduce the dimensionality of large data sets. It does this by transforming the data into 
            a new coordinate system where the new variables are linear combinations of the original variables. The new variables are chosen so that they capture the most variance in the data.

            <br><br>
            <p>Imagine you're working on a big data project, and the data has too many features. Now, picture scenario where the dataset has too many variables (features).
                As you delve into the analysis, you may encounter few possible situations. For example, you find that most of the features are correlated on analysis, 
                and you become clueless about which feature should I choose for your analysis. One possibility to run a model on the whole data. This returns poor accuracy, 
                and you feel terrible and start thinking of some strategic method to find a few important variables such as using Ridge, lasso and Elasticnet methods. The <a href="Ridge-lasso-elasticnet.html">Ridge, Lasso, and Elasticnet</a> are regularization 
                techniques that help prevent overfitting in machine learning models by adding penalty terms to the loss function. They primarily work with the existing features. 
                On the other hand, PCA is a dimensionality reduction technique. It transforms the original features into a new set of uncorrelated features called principal components. 
                PCA can be useful when you have a large number of features and want to reduce them while retaining most of the variability in the data.</p>

            <p>In summary, while Ridge, Lasso, and Elastic Net focus on regularization, PCA focuses on reducing dimensionality, which can be beneficial in situations with a high number of features or multicollinearity.</p>
            <!--------------->
            <h3 id="how-pca-works">How PCA works</h3>
            <ul>
                <li>PCA is based on the idea that many real-world data sets are high-dimensional, but that most of the information in the data is contained in a relatively small number of dimensions. This means that we can often reduce the dimensionality of the data without losing much information.</li>
                <li>To do this, PCA first calculates the covariance matrix of the data. The covariance matrix is a square matrix that shows how each pair of variables are correlated with each other.</li>
                <li>Next, PCA calculates the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors are the directions in which the data varies the most, and the eigenvalues are the magnitudes of the variances along those directions.</li>
                <li>The principal components are then formed by taking linear combinations of the original variables, where the coefficients are the corresponding eigenvectors. The first principal component is the direction of greatest variance, the second principal component is the direction of second greatest variance that is orthogonal to the first principal component, and so on.</li>
            </ul>


            <!------------------>
            <h4 id="Process">Process of doing the PCA</h4>
            Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in data analysis and machine learning. It helps uncover the underlying structure in high-dimensional datasets by transforming the data into a new coordinate system, where the axes are aligned with the directions of maximum variance.
            Here's a more detailed explanation of the PCA process:

            <ul>
                <li><strong>Centering the Data:</strong>The first step in PCA is to standardize the data. This means subtracting the mean from each variable and then dividing by the standard deviation. This ensures that all of the variables are on the same scale and that they have a mean of zero and a standard deviation of one. This ensures that the data is centered around the origin.</li>
                $$X_c = X- \bar{X}$$
                where:
                <ul>
                    <li>\(\bar{X}\) is the mean of each feature from the corresponding values.</li>
                </ul>
                <li><strong>Computing the Covariance Matrix: </strong>The covariance matrix is a square matrix that measures the correlation between each pair of variables. The covariance between two variables is equal to the average of the product of the deviations from the mean for those two variables.</li>
                $$C(x, y) = \frac{X_c^T \cdot X_c}{m-1}$$
                where:
                <ul>
                    <li>\(X_c^T\): Transpose of the centered data matrix \(X_c\). This operation swaps the rows and columns of \(X_c\).</li>
                    <li>\(X_c^T\cdot X_c\): Matrix multiplication of the transposed centered data \(X_c^T\) with the original centered data \(X_c\). This results in a square matrix with dimensions \(n\times n\).</li>
                    <li>\(1/(m-1)\): This scaling factor is used to normalize the covariance matrix. It's common to divide by \(m-1\) instead of \(m\) to account for the degrees of freedom in the sample. This makes the covariance matrix an unbiased estimator of the population covariance matrix.</li>
                </ul>
                <li><strong>Eigenvalue Decomposition: </strong>The eigenvectors and eigenvalues of the covariance matrix are the directions and magnitudes of the data's variation. The eigenvectors are the columns of a matrix \(C\), and the eigenvalues are the diagonal entries of a diagonal matrix \(\Lambda\).</li>
                $$C\cdot v_i = \lambda_i \cdot v_i.$$
                The eigen vectors \(v_i\) is the \(i-\)th eigenvectors, \(\lambda_i\) is the \(i-\) the eigenvalues, and \(\cdot\) denotes matrix multiplication. Each eigen vectors are orthogonal to one another and form an orthogonal basis for the data. It also represent the directions of maximum varaince, and the corressponding eigenvalues indicate the magnitude of varaince along those directions.
                <li><strong>Sorting and Selecting Principal Components: </strong>The eigenvalues are sorted in descending order, and the corresponding eigenvectors are arranged accordingly. The principal components are selected based on the desired dimensionality reduction. If you want to reduce the data to k dimensions, you select the top k eigenvectors. More precisely,
                a common rule of thumb is to choose \(k\) principal components, where \(k\) is the maximum number of components that explain at least \(95\%\) of the varaince in the data.
                </li>
                <li><strong>Project the data onto the principal components: </strong>The original data is then projected onto the selected principal components. The transformed data matrix \(Y\) is given by:</li>
                $$Y = W^T \cdot X_c$$
                where \(W^T\) is the transpose of matrix of eigenvectors and \(X_c\) is the matrix of the standardized data.
                <li><strong>Normalize the principal components: </strong>The principal components may not be normalized, which means that they may not have unit variance. To normalize the principal components, we divide each component by its standard deviation:</li>
                $$Y = \frac{Y}{||Y||}$$
                where:
                <ul>
                    <li>\(Y\) is the matrix of normalized principle components,</li>
                    <li>\(||Y||\) is the frobenius norm of the mtrix \(Y\).</li>
                </ul>
                <li>The new coordinate in \(Y\) represent the data in the principle component space. The first principal component (PC1) captures the most varaince, followd by (PC2), and so on. By choosing a subset of the principal components, you can achieve dimensionality reduction while retaining most of the information present in the original data.</li>
            </ul>


            <!---------------------->
            <h3 id="why-pca">Why PCA is useful</h3>
            PCA is useful for several reasons:
            <ul>
                <li><strong>Dimensionality reduction: </strong>PCA can be used to reduce the dimensionality of large data sets, which can make them easier to analyze and visualize.</li>
                <li><strong>Feature extraction: </strong>PCA can be used to extract the most important features from a data set. This can be useful for tasks like classification and clustering.</li>
                <li><strong>Data visualization: </strong>PCA can be used to visualize high-dimensional data in a way that is easy to understand.</li>
            </ul>


            <!--------------------->
            <h3 id="application">Applications of PCA</h3>
            PCA has a wide range of applications in various fields, including:
            <ul>
                <li><strong>Machine learning: </strong>PCA is a common preprocessing step in machine learning algorithms. It can be used to reduce the dimensionality of training data, which can improve the performance of the algorithm.</li>
                <li><strong>Image analysis: </strong>PCA can be used to analyze and compress images. For example, it can be used to reduce the number of pixels in an image without losing much information.</li>
                <li><strong>Finance: </strong>PCA can be used to analyze financial data, such as stock prices and returns. It can be used to identify patterns in the data and to make predictions about future prices.</li>
                <li><strong>Chemistry: </strong>PCA can be used to analyze chemical data, such as spectra and molecular structures. It can be used to identify new compounds and to understand the relationships between different compounds.</li>
            </ul>


            <!---------------------->
            <h3 id="limitations">Limitations of PCA</h3>
            PCA is a powerful tool, but it also has some limitations:
            <ul>
                <li>PCA is based on the assumption that the data is Gaussian. This means that the data should be normally distributed. If the data is not normally distributed, PCA may not be able to accurately capture the most important features of the data.</li>
                <li>PCA is not scale-invariant. This means that the results of PCA can be sensitive to the scale of the variables. If the variables are measured on different scales, PCA may not be able to identify the most important features of the data.</li>
            </ul>


            <h3 id="example-1">Example:</h3>
            <h5>Example-1</h5>
            Let's walk through a simplified mathematical example of Principal Component Analysis (PCA) using a small dataset. Consider the following 2D dataset with three observations:
            \begin{pmatrix}
            1 & 2 \\
            2 & 3 \\
            3 & 4
            \end{pmatrix}
            <ol>
                <li><strong>Step 1: Centering the Data</strong>Calculate the mean of each feature and center the data by subtracting the means: </li>
                $$\bar{X} = \left[\frac{1+2+3}{3}   ~~~~ \frac{2+3+4}{3}\right] = [2 ~~~~ 3]$$
                $$X_c = X- \bar{X} = \begin{pmatrix}
                -1 & -1 \\
                0 & 0 \\
                1 & 1
                \end{pmatrix}$$ 
                <li><strong>Step 2: Covariance Matrix</strong></li>
                $$C = \frac{X_c^T\cdot X_c}{m-1} = \frac{1}{2} \begin{pmatrix}2 &2 \\ 2 & 2 \end{pmatrix} = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$$
                <li><strong>Step 3: Eigenvalue Decomposition </strong>Find the eigen values \(\lambda_i\) and eigenvectors \(v_i\) of C:</li>
                $$\text{det}(C - \lambda I) = 0$$
                $$\text{det} \left(\begin{pmatrix} 1 - \lambda & 1 \\ 1 & 1-\lambda \end{pmatrix}\right) = 0$$
                Solving for \(\lambda\) gives \(\lambda_1 =0\) and \(\lambda_2 =2\). 
                <ul>
                    <li>For \(\lambda = 0\), the corresponding eigenvectors is \([1, -1]^T\).</li>
                    <li>For \(\lambda = 2\), the corresponding eigenvectors is \([1, 1]^T\).</li>
                </ul>
                <li><strong>Step 4: Sorting and Selecting Principal Components</strong> Sort the eigenvectors by their corresponding eigenvalues in descending order:</li>
                $$W = \begin{pmatrix} 1 & 1 \\ -1 & 1 \end{pmatrix}$$
                <li><strong>Step 5: Transforming the Data</strong> Project the centered data onto the new basis:</li>
                $$Y = X_c \cdot W = \begin{pmatrix} -1 & -1 \\ 0 & 0 \\ 1 & 1 \end{pmatrix} \cdot \begin{pmatrix} 1 & 1 \\ -1 & 1 \end{pmatrix} = \begin{pmatrix} -2 & 0 \\ 0 & 0 \\ 2 & 0 \end{pmatrix}$$
                The transformed matrix \(Y\) representes the dataset in the principal component space.
            </ol>
            This is a simplified example, but it illustrates the key steps of PCA: centering the data, computing the covariance matrix, finding eigenvectors and eigenvalues, selecting principal components, and transforming the data. In practice, PCA is applied to high-dimensional datasets for efficient dimensionality reduction.

            <pre><code class="language-python">
                import numpy as np
                from sklearn.decomposition import PCA
                import matplotlib.pyplot as plt
                
                # Sample 2D dataset
                X = np.array([[1, 2], [2, 3], [3, 4]])
                
                # Step 1: Centering the Data
                mean_X = np.mean(X, axis=0)
                X_centered = X - mean_X
                
                # Step 2: Covariance Matrix
                cov_matrix = np.cov(X_centered, rowvar=False)
                
                # Step 3: Eigenvalue Decomposition
                eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
                
                # Step 4: Sorting and Selecting Principal Components
                sorted_indices = np.argsort(eigenvalues)[::-1]
                eigenvectors_sorted = eigenvectors[:, sorted_indices]
                principal_components = eigenvectors_sorted[:, :2]  # Selecting the top 2 principal components
                
                # Step 5: Transforming the Data
                X_pca = X_centered.dot(principal_components)
                
                # Plotting the original and transformed data
                plt.figure(figsize=(8, 4))
                
                plt.subplot(1, 2, 1)
                plt.scatter(X[:, 0], X[:, 1])
                plt.title('Original Data')
                
                plt.subplot(1, 2, 2)
                plt.scatter(X_pca[:, 0], X_pca[:, 1])
                plt.title('PCA Transformed Data')
                
                plt.tight_layout()
                plt.show()            
            </code></pre>
            <figure>
                <img src="assets/img/machine-ln/PCA-analysis.png" alt="" style="max-width: 70%; max-height: auto;">
                <figcaption></figcaption>
            </figure>

            <h5>Example-2</h5>
            In this example, PCA is applied to the Iris dataset, and the data is transformed to a 2D space for visualization. The reduced-dimensional representation still captures a significant amount of information, making it easier to analyze and interpret. This is just one use case; PCA is applied similarly in various machine learning scenarios for preprocessing and feature engineering.

            <pre><code class="language-python">
                import numpy as np
                from sklearn.decomposition import PCA
                from sklearn.datasets import load_iris
                import matplotlib.pyplot as plt
                
                # Load Iris dataset as an example
                iris = load_iris()
                X = iris.data
                y = iris.target
                
                # Apply PCA for visualization (reduce to 2 components for simplicity)
                pca = PCA(n_components=2)
                X_pca = pca.fit_transform(X)
                
                # Visualize the original and PCA-transformed data side by side
                plt.figure(figsize=(12, 6))
                
                # Plot original data
                plt.subplot(1, 2, 1)
                for i in range(len(np.unique(y))):
                    plt.scatter(X[y == i, 0], X[y == i, 1], label=f'Class {i}')
                
                plt.title('Original Data')
                plt.xlabel('Feature 1')
                plt.ylabel('Feature 2')
                plt.legend()
                
                # Plot PCA-transformed data
                plt.subplot(1, 2, 2)
                for i in range(len(np.unique(y))):
                    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=f'Class {i}')
                
                plt.title('PCA Transformed Data')
                plt.xlabel('Principal Component 1')
                plt.ylabel('Principal Component 2')
                plt.legend()
                
                plt.tight_layout()
                plt.show()
            </code></pre>

            <figure>
                <img src="assets/img/machine-ln/PCA-analysis2.png" alt="" style="max-width: 70%; max-height: auto;">
                <figcaption></figcaption>
            </figure>

            <h3 id="PCA-in-machine-learning">Application of PCA in machine learning</h3>
            PCA is a powerful tool that can be used for a variety of machine learning tasks, including dimensionality reduction, feature extraction, and data visualization. Here are some more complex examples of how PCA can be used in machine learning:

            <ul>
                <li><strong>Dimensionality reduction: </strong>One of the most common uses of PCA is for dimensionality reduction. This is when you want to reduce the number of features in a dataset while still preserving as much of the original information as possible. For example, you might be working with a dataset of images that has hundreds of features, such as the brightness, contrast, and color of each pixel. PCA can be used to reduce this number of features to a much smaller number, such as 10 or 20, while still capturing most of the important information in the images. This can make it easier to train a machine learning model on the data, and it can also make it easier to visualize the data.</li>
                <li><strong>Feature extraction: </strong>Another common use of PCA is for feature extraction. This is when you want to find a new set of features that are more informative than the original features. PCA can be used to do this by identifying the directions in which the data varies the most. These directions are called principal components, and they can be used to define a new set of features. For example, you might be working with a dataset of customer data that includes features such as age, income, and spending habits. PCA can be used to identify the principal components that are most correlated with spending habits, and these can be used to define a new set of features that are more informative than the original features. This can make it easier to train a machine learning model to predict customer spending habits.</li>
                <li><strong>Data visualization: </strong>PCA can also be used for data visualization. This is when you want to visualize a high-dimensional dataset in a way that is easy to understand. For example, you might be working with a dataset of financial data that includes features such as stock prices, interest rates, and economic indicators. PCA can be used to reduce the number of features in this dataset to a much smaller number, and then the data can be visualized using a scatter plot or other visualization technique. This can help you to identify patterns in the data that might not be obvious from the original dataset.</li>
            </ul>

            Here are some examples of how PCA has been used in machine learning:
            <ul>
                <li><strong>Image compression: </strong>PCA can be used to compress images by reducing the number of pixels in the image while still preserving as much of the original information as possible. This can make it easier to store and transmit images.</li>
                <li><strong>Face recognition: </strong>PCA can be used to identify faces in images. This is done by extracting the principal components of the faces in a training set, and then using these principal components to classify new faces.</li>
                <li><strong>Anomaly detection: </strong>PCA can be used to detect anomalies in data. This is done by projecting the data onto the principal components, and then looking for outliers in the projected data.</li>
                <li><strong>Recommendation systems: </strong>PCA can be used to recommend products or services to customers. This is done by analyzing the customer's purchase history and then using PCA to identify patterns in the data. These patterns can then be used to recommend products or services that the customer is likely to be interested in.</li>
            </ul>



            </section>

            


            <!-------Reference ------->
            <section id="reference">
                <h2>References</h2>
                <ul>
                <li><a href="https://neo4j.com/developer/get-started/" target="_blank">Getting Started with Neo4j</a></li>
                <li><a href="https://www.mongodb.com/docs/manual/reference/method/" target="_blank">MongoDB documentation</a>.</li>
                </ul>
            </section>

            <hr>
        
            <div style="background-color: #f0f0f0; padding: 15px; border-radius: 5px;">

                <h3>Some other interesting things to know:</h3>
                <ul style="list-style-type: disc; margin-left: 30px;">
                    <li>Visit my website on <a href="sql-project.html">For Data, Big Data, Data-modeling, Datawarehouse, SQL, cloud-compute.</a></li>
                    <li>Visit my website on <a href="Data-engineering.html">Data engineering</a></li>
                </ul>
            </div>
            <p></p>

            <div class="navigation">
                <a href="index.html#portfolio" class="clickable-box">
                    <span class="arrow-left">Portfolio section</span>
                </a>
                
                <a href="portfolio-details-1.html" class="clickable-box">
                    <span class="arrow-right">Content</span>
                </a>
            </div>
        </div>
        </section><!-- End Portfolio Details Section -->
    </main><!-- End #main --

    <!-- ======= Footer ======= -->
    <footer id="footer">
    <div class="container">
        <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
        </div>
    </div>
    </footer><!-- End  Footer -->

    <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="assets/vendor/typed.js/typed.umd.js"></script>
    <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function () {
        hljs.initHighlightingOnLoad();
    });
    </script>

</body>

</html>