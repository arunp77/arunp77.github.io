<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>ETL Pipeline</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>
  <!-- Creating a sql code section-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css">

  <!-- Example for Prism.js -->
    <script src="assets/js/prism.js" integrity="sha384-uEbBqWkE4LEVKVKJBii7Pb3pDi8svYlmL8jq5vHEewoYW6hDyyMuwQX+FRQ0PK5c" crossorigin="anonymous"></script>

    <!-- Example for Highlight.js -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css" integrity="sha384-uEbBqWkE4LEVKVKJBii7Pb3pDi8svYlmL8jq5vHEewoYW6hDyyMuwQX+FRQ0PK5c" crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js" integrity="sha384-uEbBqWkE4LEVKVKJBii7Pb3pDi8svYlmL8jq5vHEewoYW6hDyyMuwQX+FRQ0PK5c" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/languages/sql.min.js" integrity="sha384-uEbBqWkE4LEVKVKJBii7Pb3pDi8svYlmL8jq5vHEewoYW6hDyyMuwQX+FRQ0PK5c" crossorigin="anonymous"></script>
    <!----
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js"></script>
    -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/languages/sql.min.js" integrity="sha384-uEbBqWkE4LEVKVKJBii7Pb3pDi8svYlmL8jq5vHEewoYW6hDyyMuwQX+FRQ0PK5c" crossorigin="anonymous"></script>
  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Arun</a></h1>
        <div class="social-links mt-3 text-center">
            <a href="https://www.linkedin.com/in/arunp77/" target="_blank" class="linkedin"><i class="bx bxl-linkedin"></i></a>
            <a href="https://github.com/arunp77" target="_blank" class="github"><i class="bx bxl-github"></i></a>
            <a href="https://twitter.com/arunp77_" target="_blank" class="twitter"><i class="bx bxl-twitter"></i></a>
            <a href="https://www.instagram.com/arunp77/" target="_blank" class="instagram"><i class="bx bxl-instagram"></i></a>
            <a href="https://arunp77.medium.com/" target="_blank" class="medium"><i class="bx bxl-medium"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
            <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
            <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
            <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
            <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
            <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
            <li><a href="index.html#language" class="nav-link scrollto"><i class="bi bi-menu-up"></i> <span>Languages</span></a></li>
            <li><a href="index.html#awards" class="nav-link scrollto"><i class="bi bi-award-fill"></i> <span>Awards</span></a></li>
            <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
            <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
            <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
            <!-- <li><a href="#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li> -->
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

  <main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Portfoio Details</h2>
          <ol>
            <li><a href="Data-engineering.html" class="clickable-box">Content</a></li>
            <li><a href="index.html#portfolio" class="clickable-box">Portfolio</a></li>
          </ol>
        </div>

      </div>
    </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
        <div class="dropdown">
            <button class="dropbtn"><strong>Shortcuts:</strong></button>
            <div class="dropdown-content">
                <ul>
                    <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                    <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                    <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                    <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                    <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(15, 15, 15);"></i> Docker</a></li>
                    <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(5, 5, 5);"></i> Jupyter-nifi</a></li>
                    <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                    <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                    <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                    <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                    <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                    <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                    <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                      <!-- Add more subsections as needed -->
                  </ul>
            </div>
          </div>
      </div>
    
    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">
        <div class="row gy-4">
          <h1> Data Engineering Project: ETL Pipeline from Spotify API to Snowflake Data Warehouse</h1>
            <h3><b>Introduction</b></h3>
            <p>In my recent data project, I leveraged the power of Snowflake, a cloud-based data warehousing platform, in combination with 
                the versatility of Amazon S3 (Simple Storage Service) to efficiently and seamlessly load data for analysis. The project 
                involved the extraction of data stored in an S3 bucket and its integration into Snowflake for further processing and analysis. 
                This integration of S3 with Snowflake allowed for a robust and flexible data pipeline that met the project's data processing needs.</p>

            
            <div class="flex-container">
                <div class="text">
                <h3><b>Prerequisites</b></h3>
                <p>Before diving into the project, you'll need the following prerequisites:</p>
                <ul style="list-style-type: disc; margin-left: 30px;">
                    <li>An <a href="https://www.snowflake.com/en/" target="_blank">Snowflake account</a> with the necessary permissions to create and manage services.</li>
                    <li>Basic knowledge of <a href="https://aws.amazon.com/" target="_blank">AWS services</a> like <a href="https://aws.amazon.com/s3/" target="_blank">s3</a>,
                        <a href="https://aws.amazon.com/iam/" target="_blank">IAM</a>.</li>
                    <li><a href="https://github.com/arunp77/Databases-data-pipeline" target="_blank">SQL, databases and data warehouses</a>.</li>
                    <li><a href="https://developer.spotify.com/documentation/web-api" target="_blank">Spotify API access</a>.</li>
                </ul>
                </div>
                <div class="image">
                    <img src="assets/img/portfolio/Snowflake-AWS-integration.png" alt="Image Description">
                </div>
            </div>
            <!--
            <p>Before diving into the project, you'll need the following prerequisites:</p>
            <ul style="list-style-type: disc; margin-left: 30px;">
                <li>An <a href="https://signin.aws.amazon.com/" target="_blank"> AWS account</a> with the necessary permissions to create and manage services.</li>
                <li>Basic knowledge of AWS services like S3, Lambda, Glue, and Athena.</li>
                <li><a href="https://github.com/arunp77/Python-programming" target="_blank">Python programming skills</a>.</li>
                <li><a href="https://developer.spotify.com/documentation/web-api" target="_blank">API access</a>.</li>
            </ul>-->
                
            <h3><b>Project overview</b></h3>
            <p>This project is an end-to-end ETL (Extract, Transform, Load) pipeline designed to automate the extraction of data from a Spotify playlist 
                and process it using various AWS services. The goal is to showcase proficiency in data engineering and AWS, with a focus on data extraction, 
                transformation, and automated processing. I have successfully completed the Spotify ETL pipeline project on AWS, and here's an overview 
                of the key steps:</p>
            <ul style="list-style-type: disc; margin-left: 30px;">
                <li><span style="font-weight: normal;"><b><a href="https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration" target="_blank">
                    Data Source and S3 Integration</a>: </b></span>The project began by sourcing data from a spotify API of globally famous top 50 songs
                    and storing it in an Amazon S3 bucket. Amazon S3 provided an ideal repository for the data due to its scalability, durability, 
                    and cost-effectiveness. This cloud-based storage solution ensured that the data was easily accessible and highly available for the subsequent 
                    stages of the project.</li> 
                <li><span style="font-weight: normal;"><b><a href="https://docs.snowflake.com/en/user-guide/data-load-s3-create-stage" target="_blank">
                    Setting Up Snowflake External Stages</a>: </b></span>To enable the efficient loading of data from the S3 bucket 
                    into Snowflake, I created Snowflake external stages. These stages act as bridges between the cloud storage and Snowflake, allowing the seamless 
                    transfer of data. By configuring the external stages to point to the specific location of the data in the S3 bucket, I established a direct 
                    connection between the two platforms. This configuration also included defining the necessary access credentials for secure data transfer.</li>
                <li><span style="font-weight: normal;"><b><a href="https://docs.snowflake.com/en/user-guide/data-load-snowpipe-auto" target="_blank">
                    Data Loading with Snowpipe</a>: </b></span>One of the standout features of the project was the 
                    utilization of Snowpipe, a powerful Snowflake feature designed for continuous, automated data loading. With Snowpipe, new data added to the S3 
                    bucket was automatically detected and ingested into Snowflake. This real-time data ingestion mechanism reduced manual intervention and minimized 
                    the latency between data arrival and availability for analysis. Snowpipe's event-driven model ensured that data was always up-to-date in 
                    Snowflake.</li>
            </ul>
                
            <h3><b>Benefits of the S3 and Snowflake Integration</b></h3>
            <p>The integration of Amazon S3 with Snowflake delivered numerous advantages. It provided a scalable and cost-effective solution for data storage, allowing the 
                project to grow seamlessly with the increasing volume of data. The use of Snowflake's data warehousing capabilities ensured efficient data 
                processing and analysis. Additionally, the automated data loading through Snowpipe enhanced operational efficiency and maintained data accuracy.</p>


            <h3><b>Project Architecture</b></h3>
            <img src="assets/img/portfolio/AWS-snowpipe-1.png" alt="Project Architecture" style="width: 800px; height: auto;">

            <!-- project setup start here-->
            <h3><b>Project Setup</b></h3>
                <h4>Snowflake resources</h4>
                <ul style="list-style-type: disc; margin-left: 30px;">
                    <li><h5>Snowflake Edition:</h5> </li>I used the Snowflake Free Trial edition for this project.</p> 
                    <li><h5>Signup Process:</h5></li>
                    <ul style="list-style-type: disc; margin-left: 30px;">
                        <li>Go to the <a href="https://signup.snowflake.com/?utm_cta=trial-en-www-homepage-top-right-nav-ss-evg" target="_blank">Snowflake website</a> and click on "Get Free Trial."</li>
                        <li>Follow the on-screen instructions to set up your Snowflake account.</li>
                        <li>Note any specific considerations or tips during the signup process.</li>
                        <li>Once your account is created, you can access the Snowflake web interface by logging in.</li>
                    </ul>
                </ul>
                

                <h4>AWS Resources</h4>
                <ul style="list-style-type: disc; margin-left: 20px;">
                    <li>Create an <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html" target="_blank">Amazon S3 bucket to store the data</a>
                    (you can follow the step by step guide provided at official AWS documnetation page)</li>
                    <li>Configure folder structures within the S3 bucket for raw and transformed data.</li>
                    <li>Set up necessary Lambda functions for data extraction and transformation.</li>
                    <li>In the project, I created follwoing folders in s3 bucket on AWS cloud:</li>
                    <ul>
                        <p><h6><strong>Amazon S3 Bucket:</strong></h6></p>
                        <ul style="list-style-type: disc; margin-left: 20px;">
                            <li>Set up an Amazon S3 bucket named "spotify-etl-project-arun."</li>
                            <li>This bucket contains all the data files obtained from the spotify API.</li>
                        </ul>
                        <p><h6><b>Folder Structure:</b></h6></p>
                        <p>Within the S3 bucket, established a structured folder hierarchy:</p>
                        <ul style="list-style-type: disc; margin-left: 20px;">
                            <li>raw_data/
                                <ul style="list-style-type: disc; margin-left: 10px;">
                                    <li>processed/</li>
                                    <li>to_processed/</li>
                                </ul>
                            </li>
                            <li>transformed_data/
                                <ul style="list-style-type: disc; margin-left: 10px;">
                                    <li>album_data/</li>
                                    <li>artist_data/</li>
                                    <li>songs_data/</li>
                                </ul>
                            </li>
                        </ul>
                    </ul>
                    <li>You can find more details on how to extract, trasnform and load the data from the spotify to amazon s3 bucket from my 
                        <a href="https://arunp77.github.io/Arun-Kumar-Pandey/portfolio-details-6.html" target="_blank">AWS-etl-datapipeline project</a>.</li>
                </ul>
            <!-- project setup end here-->

            <!-- Key component start here-->
            <h3><b>Key Components</b></h3>
                <ul style="list-style-type: disc; margin-left: 30px;">
                    <li><b>AWS s3 data: </b>I assume that data is already extracted, trasnformed and then loaded to the AWS s3 bucket, in folders: album_data, artist_data
                        and songs_data.</li>
                    <li><b>Automation with AWS Lambda: </b> AWS Lambda functions were used to automate data processing tasks. Triggers were set up to initiate these processes when new data was added to the S3 buckets.</li>
                    <li><b>Data Loading to Snowflake from aws s3 bucket: </b>Next we would do the creation of snowpipes.</li>
                    <li>
                        <p><b>Access Your Snowflake Account:</b> Log in to your Snowflake account using your credentials.</p>
                    </li>
            
                    <li>
                        <p><b>Create a Workspace:</b> Click on the "Workspaces" tab and create a new workspace named "Spotify-data-analysis."</p>
                    </li>
                    <li>
                        <p><b>Create a Database: </b> Within your workspace, create a new database named "SPOTIFY_ETL_PIPELINE."</p>
                    </li>
                    <table>
                        <tr>
                            <td><img src="assets/img/portfolio/snowflake-workspace.png" alt="Image 1" width="600" height="350"></td>
                            <td><img src="assets/img/portfolio/snowflake-database.png" alt="Image 2" width="600" height="350"></td>
                        </tr>
                    </table>
            
                    <li>
                        <p><b>Define Tables and Schema: </b>Create the necessary tables within your database, defining the schema ('SPOTIFY_DATA_ANALYSIS_SCHEMA') for your data. This schema should match 
                            the structure of your data files in the S3 bucket.</p>
                            <table>
                                <tr>
                                    <td><img src="assets/img/portfolio/dimesnion-fact-table.png" alt="Image 1" width="600" height="350"></td>
                                    <td><img src="assets/img/portfolio/snowflake-schema.png" alt="Image 2" width="600" height="350"></td>
                                </tr>
                            </table>
                    </li>
            
                    <li>
                        <p><b><a href="https://docs.snowflake.com/en/user-guide/data-load-s3-config-aws-iam-role" target="_blank"> Set Up AWS Integration:</a> </b>
                            Setting up the AWS integration allows Snowflake to access your AWS S3 bucket and pull data into your Snowflake tables.</p>
                        <ul>
                            <li>
                                <p><b>Configure AWS Integration: </b>  It is important to note that an ACCOUNTADMIN is required for this task. A storage integration in 
                                    Snowflake is an object that stores a generated identity for the external cloud storage.
                                    Within the Snowflake account, navigate to "Account" and then "Security Integration." Click on "Set Up Third-Party Integration." This can also be done through following 
                                query: </p>
                                <pre class="language-sql">
                                    <code id="sql-code">
                                        create storage integration "integration_name"
                                            type = external_stage
                                            storage_provider = s3
                                            storage_aws_role_arn = '"role ARN"'
                                            enabled = true
                                            storage_allowed_locations = ('s3://bucket_name/');
                                    </code>
                                  </pre>                                                             
                            </li>
            
                            <li>
                                <p><b>Choose AWS as the Cloud Provider: </b>Select AWS as your cloud provider. You may be prompted to enter AWS access and secret keys for authentication.</p>
                            </li>
            
                            <li>
                                <p><b>Define the Integration Name</b> Create a name for your integration, such as "Spotify-AWS-Integration."</p>
                            </li>
            
                            <li>
                                <p><b>Define a Role ARN: </b> Specify the Amazon Resource Name (ARN) for the role that has permissions to access your S3 bucket. This role should have read access to the bucket where your data is stored.</p>
                            </li>
            
                            <li>
                                <p><b>Configure External Stage: </b> Set up an external stage that points to your AWS S3 bucket. Provide the bucket name and any other required configuration settings.</p>
                            </li>
            
                            <li>
                                <p><b>Validate and Save: </b> Review the integration details to ensure they are correct. Click "Finish" to save the integration.</p>
                            </li>
                            <div class="image">
                                <img src="assets/img/portfolio/Snowflake-aws-integration2.png" alt="Image Description">
                            </div>     
                        </ul>
                    </li>

                    <li><p><b>Create a Trust Relationship for the Storage Integration: </b>Afterward, we can execute the following query to retrieve information about the storage integration:</p>
                        <pre class="language-sql">
                            <code id="sql-code">
                             desc integration "integration_name";
                            </code>
                        </pre>   
                    </li>

                    <li><p><b>Create an External Stage: </b> With the above setup, we can now create an external stage using the following SQL statement:</p> 
                        <pre class="language-sql">
                            <code id="sql-code">
                                create stage "stage_name"
                                    storage_integration = "integration_name"
                                    url = 's3:// "bucket_name"/';
                            </code>
                        </pre> 
                        <p>We can check all the available files at s3 bucket using:</p>
                        <pre class="language-sql">
                            <code id="sql-code">
                             List @stage_name; 
                            </code>
                        </pre>  
                    </li>
            
                    <li>
                        <p><b>Load Data from S3 to Snowflake: </b>Now that your integration is set up, you can use Snowflake's COPY INTO command to load data from your S3 bucket into Snowflake tables. Ensure that you have the necessary permissions to perform this operation.</p>
                    </li>
            
                    <li>
                        <p><b>Data Analysis: </b>With your data successfully loaded into Snowflake, you can now perform data analysis using SQL queries and other tools within Snowflake.</p>
                    </li>
                </ul>


            <h3><b>Project Results</b></h3>
                Following visualizations were created to illustrate key findings and insights from the data analysis. 

                <table>
                    <tr>
                        <td><img src="assets/img/portfolio/data-analysis-1.png" alt="Image 1"></td>
                        <td><img src="assets/img/portfolio/data-analysis-2.png" alt="Image 2"></td>
                    </tr>
                    <tr>
                        <td><img src="assets/img/portfolio/data-analysis-3.png" alt="Image 3"></td>
                        <td><img src="assets/img/portfolio/data-analysis-4.png" alt="Image 4"></td>
                    </tr>
                    <tr>
                        <td><img src="assets/img/portfolio/data-analysis-5.png" alt="Image 5"></td>
                        <td><img src="assets/img/portfolio/data-analysis-6.png" alt="Image 6"></td>
                    </tr>
                </table>

                <p>(More detailed query for the data analysis can be found at 
                <a href="https://github.com/arunp77/Databases-data-pipeline/tree/main/4.0-Database/Spotify-project" target="_blank">my github repository</a>)</p>
                
            <h3><b>Challenges Faced</b></h3> 
            <ul style="list-style-type: disc; margin-left: 30px;"></ul>
                <li>
                    <b>Context:</b> During the data transformation phase, we encountered significant delays in processing large volumes of data from the Spotify API. The delays were mainly due to the API rate limits and the sheer amount of data to process.
                </li>
                <li>
                    <b>Impact:</b> These delays threatened to disrupt the automated data processing pipeline and hindered our ability to provide timely data to analysts and stakeholders.
                </li>
                <li>
                    <b>Solution:</b> To address this challenge, we implemented a rate-limiting mechanism in our Python script to adhere to the API's rate limits. Additionally, we optimized the data transformation code to improve its efficiency.
                </li>
                <li>
                    <b>Results:</b> These modifications significantly reduced the processing time and minimized data delays. As a result, we were able to maintain a consistent flow of data, ensuring that our analysts had up-to-date information for their analysis.
                </li>
                <li>
                    <b>Key Takeaways:</b> This challenge taught us the importance of robust error handling and efficiency in data processing pipelines. We also learned to closely monitor API usage to ensure compliance with rate limits, which is critical for real-time data extraction and processing.
                </li>
            </ul>
 
            <h3>Future Improvements</h3>
 
            <ul style="list-style-type: disc; margin-left: 30px;"></ul>
                <li>
                    <b>Enhanced Data Sources:</b> Expanding the data sources beyond Spotify to include other music streaming platforms, enabling a broader and more comprehensive dataset for analysis.
                </li>
                <li>
                    <b>Real-time Data Updates:</b> Implementing a real-time data processing mechanism to ensure that analysts and stakeholders have access to the most up-to-date information without delay.
                </li>
                <li>
                    <b>Data Validation and Quality Checks:</b> Developing a system for automated data validation and quality checks to ensure the integrity and accuracy of the data before analysis.
                </li>
                <li>
                    <b>User-friendly Dashboard:</b> Creating a user-friendly dashboard or visualization tool that allows non-technical users to access and interact with the data for insights and reporting.
                </li>
                <li>
                    <b>Scalability:</b> Designing the data pipeline to be highly scalable, enabling it to handle larger datasets and increased traffic as the project grows.
                </li>
            </ul>
            
            <div class="navigation">
                <a href="index.html#portfolio" class="clickable-box">
                    <span class="arrow-left">Go home</span>
                </a>
                <a href="Data-engineering.html" class="clickable-box">
                    <span class="arrow-right">Content</span>
                </a>
            </div>

        </div>
      </div>
    </section><!-- End Portfolio Details Section -->
  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><span>Arun</span></strong>
      </div>
    </div>
  </footer><!-- End  Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/typed.js/typed.umd.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

  <script>
    document.addEventListener("DOMContentLoaded", function () {
      hljs.initHighlightingOnLoad();
    });
  </script>

  


</body>

</html>